Session 1.1: Natural Language Content Analysis

This lecture is about Natural Language of Content Analysis.
As you see from this picture, this is really the first step to process any text data.
Text data are in natural languages.
So computers have to understand natural languages to some extent, in order to make use of the data.
So that's the topic of this lecture.
We're going to cover three things.
First, what is natural language processing, which is the main technique for processing natural language to obtain understanding.
The second is the state of the art of NLP which stands for natural language processing.
Finally we're going to cover the relation between natural language processing and text retrieval.
First, what is NLP?
Well the best way to explain it is to think about if you see a text in a foreign language that you can understand.
Now what do you have to do in order to understand that text?
This is basically what computers are facing.
So looking at the simple sentence like a dog is chasing a boy on the playground.
We don't have any problems understanding this sentence.
But imagine what the computer would have to do in order to understand it.
Well in general, it would have to do the following.
First, it would have to know dog is a noun, chasing's a verb, etc.
So this is called lexical analysis, or part-of-speech tagging, and we need to figure out the syntactic categories of those words.
So that's the first step.
After that, we're going to figure out the structure of the sentence.
So for example, here it shows that A and the dog would go together to form a noun phrase.
And we won't have dog and is to go first.
And there are some structures that are not just right.
But this structure shows what we might get if we look at the sentence and try to interpret the sentence.
Some words would go together first, and then they will go together with other words.
So here we show we have noun phrases as intermediate components, and then verbal phrases.
Finally we have a sentence.
And you get this structure.
We need to do something called a semantic analysis, or parsing.
And we may have a parser accompanying the program, and that would automatically created this structure.
At this point you would know the structure of this sentence, but still you don't know the meaning of the sentence.
So we have to go further to semantic analysis.
In our mind we usually can map such a sentence to what we already know in our knowledge base.
For example, you might imagine a dog that looks like that.
There's a boy and there's some activity here.
But for a computer would have to use symbols to denote that.
We'd use a symbol (d1) to denote a dog.
And (b)1 can denote a boy and then (p)1 can denote a playground.
Now there is also a chasing activity that's happening here so we have a relationship chasing that connects all these symbols.
So this is how a computer would obtain some understanding of this sentence.
Now from this representation we could also further infer some other things, and we might indeed naturally think of something else when we read a text and this is called inference.
So for example, if you believe that if someone's being chased and this person might be scared, but with this rule, you can see computers could also infer that this boy maybe scared.
So this is some extra knowledge that you'd infer based on some understanding of the text.
You can even go further to understand why the person say at this sentence.
So this has to do as a use of language.
This is called pragmatic analysis.
In order to understand the speak actor of a sentence, right?
We say something to basically achieve some goal.
There's some purpose there.
And this has to do with the use of language.
In this case the person who said this sentence might be reminding another person to bring back the dog.
That could be one possible intent.
To reach this level of understanding would require all of these steps and a computer would have to go through all these steps in order to completely understand this sentence.
Yet we humans have no trouble with understanding that, we instantly would get everything.
There is a reason for that.
That's because we have a large knowledge base in our brain and we can use common sense knowledge to help interpret the sentence.
Computers unfortunately are hard to obtain such understanding.
They don't have such a knowledge base.
They are still incapable of doing reasoning and uncertainties, so that makes natural language processing difficult for computers.
But the fundamental reason why natural language processing is difficult for computers is simply because natural language has not been designed for computers.
Natural languages are designed for us to communicate.
There are other languages designed for computers.
For example, programming languages.
Those are harder for us, right?
So natural languages is designed to make our communication efficient.
As a result, we omit a lot of common sense knowledge because we assume everyone knows about that.
We also keep a lot of ambiguities because we assume the receiver or the hearer could know how to decipher an ambiguous word based on the knowledge or the context.
There's no need to demand different words for different meanings.
We could overload the same word with different meanings without the problem.
Because of these reasons this makes every step in natural language of processing difficult for computers, ambiguity is the main difficulty.
And common sense and reasoning is often required, that's also hard.
So let me give you some examples of challenges here.
Consider the word level ambiguity.
The same word can have different syntactic categories.
For example design can be a noun or a verb.
The word of root may have multiple meanings.
So square root in math sense or the root of a plant.
You might be able to think about it's meanings.
There are also syntactical ambiguities.
For example, the main topic of this lecture, natural language processing, can actually be interpreted in two ways in terms of the structure.
Think for a moment and see if you can figure that out.
We usually think of this as processing of natural language, but you could also think of this as do say, language processing is natural.
So this is an example of synaptic ambiguity.
What we have different is structures that can be applied to the same sequence of words.
Another common example of an ambiguous sentence is the following.
A man saw a boy with a telescope.
Now in this case the question is, who had a telescope.
This is called a prepositional phrase attachment ambiguity or PP attachment ambiguity.
Now we generally don't have a problem with these ambiguities because we have a lot of background knowledge to help us disambiguate the ambiguity.
Another example of difficulty is anaphora resolution.
So think about the sentence John persuaded Bill to buy a TV for himself.
The question here is does himself refer to John or Bill?
So again this is something that you have to use some background or the context to figure out.
Finally, presupposition is another problem.
Consider the sentence, he has quit smoking.
Now this obviously implies that he smoked before.
So imagine a computer wants to understand all these subtle differences and meanings.
It would have to use a lot of knowledge to figure that out.
It also would have to maintain a large knowledge base of all the meanings of words and how they are connected to our common sense knowledge of the world.
So this is why it's very difficult.
So as a result, we are steep not perfect, in fact far from perfect in understanding natural language using computers.
So this slide sort of gains a simplified view of state of the art technologies.
We can do part of speech tagging pretty well, so I showed 97% accuracy here.
Now this number is obviously based on a certain dataset, so don't take this literally.
This just shows that we can do it pretty well.
But it's still not perfect.
In terms of parsing, we can do partial parsing pretty well.
That means we can get noun phrase structures, or verb phrase structure, or some segment of the sentence, and this dude correct them in terms of the structure.
And in some evaluation results, we have seen above 90% accuracy in terms of partial parsing of sentences.
Again, I have to say these numbers are relative to the dataset.
In some other datasets, the numbers might be lower.
Most of the existing work has been evaluated using news dataset.
And so a lot of these numbers are more or less biased toward news data.
Think about social media data, the accuracy likely is lower.
In terms of a semantical analysis, we are far from being able to do a complete understanding of a sentence.
But we have some techniques that would allow us to do partial understanding of the sentence.
So I could mention some of them.
For example, we have techniques that can allow us to extract the entities and relations mentioned in text articles.
For example, recognizing dimensions of people, locations, organizations, etc in text.
So this is called entity extraction.
We may be able to recognize the relations.
For example, this person visited that place or this person met that person or this company acquired another company.
Such relations can be extracted by using the computer current Natural Language Processing techniques.
They're not perfect but they can do well for some entities.
Some entities are harder than others.
We can also do word sense disintegration to some extend.
We have to figure out whether this word in this sentence would have certain meaning in another context the computer could figure out, it has a different meaning.
Again, it's not perfect, but you can do something in that direction.
We can also do sentiment analysis, meaning, to figure out whether a sentence is positive or negative.
This is especially useful for review analysis, for example.
So these are examples of semantic analysis.
And they help us to obtain partial understanding of the sentences.
It's not giving us a complete understanding, as I showed it before, for this sentence.
But it would still help us gain understanding of the content.
And these can be useful.
In terms of inference, we are not there yet, probably because of the general difficulty of inference and uncertainties.
This is a general challenge in artificial intelligence.
Now that's probably also because we don't have complete semantical representation for natural  text.
So this is hard.
Yet in some domains perhaps, in limited domains when you have a lot of restrictions on the word uses, you may be able to perform inference to some extent.
But in general we can not really do that reliably.
Speech act analysis is also far from being done and we can only do that analysis for very special cases.
So this roughly gives you some idea about the state of the art.
And then we also talk a little bit about what we can't do, and so we can't even do 100% part of speech tagging.
Now this looks like a simple task, but think about the example here, the two uses of off may have different syntactic categories if you try to make a fine grained distinctions.
It's not that easy to figure out such differences.
It's also hard to do general complete parsing.
And again, the same sentence that you saw before is example.
This ambiguity can be very hard to disambiguate and you can imagine example where you have to use a lot of knowledge in the context of the sentence or from the background, in order to figure out who actually had the telescope.
So although the sentence looks very simple, it actually is pretty hard.
And in cases when the sentence is very long, imagine it has four or five prepositional phrases, and there are even more possibilities to figure out.
It's also harder to do precise deep semantic analysis.
So here's an example.
In the sentence "John owns a restaurant." How do we define owns exactly?
The word own, it is something that we can understand but it's very hard to precisely describe the meaning of own for computers.
So as a result we have a robust and a general Natural Language Processing techniques that can process a lot of text data.
In a shallow way, meaning we only do superficial analysis.
For example, parts of speech tagging or a partial parsing or recognizing sentiment.
And those are not deep understanding, because we're not really understanding the exact meaning of the sentence.
On the other hand of the deep understanding techniques tend not to scale up well, meaning that they would fill only some restricted text.
And if you don't restrict the text domain or the use of words, then these techniques tend not to work well.
They may work well based on machine learning techniques on the data that are similar to the training data that the program has been trained on.
But they generally wouldn't work well on the data that are very different from the training data.
So this pretty much summarizes the state of the art of Natural Language Processing.
Of course, within such a short amount of time we can't really give you a complete view of NLP, which is a big field.
And I'd expect to see multiple courses on Natural Language Processing topic itself.
But because of its relevance to the topic that we talk about, it's useful for you to know the background in case you happen to be exposed to that.
So what does that mean for Text Retrieval?
Well, in Text Retrieval we are dealing with all kinds of text.
It's very hard to restrict text to a certain domain.
And we also are often dealing with a lot of text data.
So that means The NLP techniques must be general, robust, and efficient.
And that just implies today we can only use fairly shallow NLP techniques for text retrieval.
In fact, most search engines today use something called a bag of words representation.
Now, this is probably the simplest representation you can possibly think of.
That is to turn text data into simply a bag of words.
Meaning we'll keep individual words, but we'll ignore all the orders of words.
And we'll keep duplicated occurrences of words.
So this is called a bag of words representation.
When you represent text in this way, you ignore a lot of valid information.
That just makes it harder to understand the exact meaning of a sentence because we've lost the order.
But yet this representation tends to actually work pretty well for most search tasks.
And this was partly because the search task is not all that difficult.
If you see matching of some of the query words in a text document, chances are that that document is about the topic, although there are exceptions.
So in comparison of some other tasks, for example, machine translation would require you to understand the language accurately.
Otherwise the translation would be wrong.
So in comparison such tasks are all relatively easy.
Such a representation is often sufficient and that's also the representation that the major search engines today, like a Google or Bing are using.
Of course, I put in parentheses but not all, of course there are many queries that are not answered well by the current search engines, and they do require the replantation that would go beyond bag of words replantation.
That would require more natural language processing to be done.
There was another reason why we have not used the sophisticated NLP techniques in modern search engines.
And that's because some retrieval techniques actually, naturally solved the problem of NLP.
So one example is word sense disintegration.
Think about a word like Java.
It could mean coffee or it could mean program language.
If you look at the word anome, it would be ambiguous, but when the user uses the word in the query, usually there are other words.
For example, I'm looking for usage of Java applet.
When I have applet there, that implies Java means program language.
And that contest can help us naturally prefer documents which Java is referring to program languages.
Because those documents would probably match applet as well.
If Java occurs in that documents where it means coffee then you would never match applet or with very small probability.
So this is the case when some retrieval techniques naturally achieve the goal of word.
Another example is some technique called feedback which we will talk about later in some of the lectures.
This technique would allow us to add additional words to the query and those additional words could be related to the query words.
And these words can help matching documents where the original query words have not occurred.
So this achieves, to some extent, semantic matching of terms.
So those techniques also helped us bypass some of the difficulties in natural language processing.
However, in the long run we still need a deeper natural language processing techniques in order to improve the accuracy of the current search engines.
And it's particularly needed for complex search tasks.
Or for question and answering.
Google has recently launched a knowledge graph, and this is one step toward that goal, because knowledge graph would contain entities and their relations.
And this goes beyond the simple bag of words replantation.
And such technique should help us improve the search engine utility significantly, although this is the open topic for research and exploration.
In sum, in this lecture we talked about what is NLP and we've talked about the state of that techniques.
What we can do, what we cannot do.
And finally, we also explain why the bag of words replantation remains the dominant replantation used in modern search engines, even though deeper NLP would be needed for future search engines.
If you want to know more, you can take a look at some additional readings.
I only cited one here and that's a good starting point.
Thanks.




Session 1.2: Text Access

In this lecture, we're going to talk about the text access.
In the previous lecture, we talked about the natural language content, analysis.
We explained that the state of the are natural language processing techniques are still not good enough to process a lot of unrestricted text data in a robust manner.
As a result, bag of words remains very popular in applications like a search engine.
In this lecture, we're going to talk about some high-level strategies to help users get access to the text data.
This is also important step to convert raw big text data into small random data.
That are actually needed in a specific application.
So the main question we'll address here, is how can a text information system, help users get access to the relevant text data?
We're going to cover two complimentary strategies, push versus pull.
And then we're going to talk about two ways to implement the pull mode, querying versus browsing.
So first push versus pull.
These are two different ways connect the users with the right information at the right time.
The difference is which takes the initiative, which party takes the initiative.
In the pull mode, the users take the initiative to start the information access process.
And in this case, a user typically would use a search engine to fulfill the goal.
For example, the user may type in the query and then browse the results to find the relevant information.
So this is usually appropriate for satisfying a user's ad hoc information need.
An ad hoc information need is a temporary information need.
For example, you want to buy a product so you suddenly have a need to read reviews about related product.
But after you have cracked information, you have purchased in your product.
You generally no longer need such information, so it's a temporary information need.
In such a case, it's very hard for a system to predict your need, and it's more proper for the users to take the initiative, and that's why search engines are very useful.
Today because many people have many information needs all the time.
So as we're speaking Google is probably processing many queries from this.
And those are all, or mostly adequate.
Information needs.
So this is a pull mode.
In contrast in the push mode in the system would take the initiative to push the information to the user or to recommend information to the user.
So in this case this is usually supported by a recommender system.
Now this would be appropriate if.
The user has a stable information.
For example you may have a research interest in some topic and that interest tends to stay for a while.
So, it's rather stable.
Your hobby is another example of.
A stable information need is such a case the system can interact with you and can learn your interest, and then to monitor the information stream.
If the system hasn't seen any relevant items to your interest, the system could then take the initiative to recommend the information to you.
So, for example, a news filter or news recommended system could monitor the news stream and identify interesting news to you and simply push the news articles to you.
This mode of information access may be also a property that when this system has good knowledge about the users need and this happens in the search context.
So for example, when you search for information on the web a search engine might infer you might be also interested in something related.
Formation.
And they would recommend the information to you, so that just reminds you, for example, of an advertisement placed on the search page.
So this is about the two high level strategies or two modes of text access.
Now let's look at the pull mode in more detail.
In the pull mode, we can further distinguish it two ways to help users.
Querying versus browsing.
In querying, a user would just enter a query.
Typical the keyword query, and the search engine system would return relevant documents to use.
And this works well when the user knows what exactly are the keywords to be used.
So if you know exactly what you are looking for, you tend to know the right keywords.
And then query works very well, and we do that all of the time.
But we also know that sometimes it doesn't work so well.
When you don't know the right keywords to use in the query, or you want to browse information in some topic area.
You use because browsing would be more useful.
So in this case, in the case of browsing, the users would simply navigate it, into the relevant information by following the paths supported by the structures of documents.
So the system would maintain some kind of structures and then the user could follow these structures to navigate.
So this really works well when the user wants to explore the information space or the user doesn't know what are the keywords to using the query.
Or simply because the user finds it inconvenient to type in a query.
So even if a user knows what query to type in if the user is using a cellphone to search for information.
It's still harder to enter the query.
In such a case, again, browsing tends to be more convenient.
The relationship between browsing and querying is best understood by making and imagine you're site seeing.
Imagine if you're touring a city.
Now if you know the exact address of attraction.
Taking a taxi there is perhaps the fastest way.
You can go directly to the site.
But if you don't know the exact address, you may need to walk around.
Or you can take a taxi to a nearby place and then walk around.
It turns out that we do exactly the same in the information studies.
If you know exactly what you are looking for, then you can use the right keywords in your query to find the information you're after.
That's usually the fastest way to do, find information.
But what if you don't know the exact keywords to use?
Well, you clearly probably won't so well.
You will not related pages.
And then, you need to also walk around in the information space, meaning by following the links or by browsing.
You can then finally get into the relevant page.
If you want to learn about again.
You will likely do a lot of browsing so just like you are looking around in some area and you want to see some interesting attractions related in the same.
.
So this analogy also tells us that today we have very good support for query, but we don't really have good support for browsing.
And this is because in order to browse effectively, we need a map to guide us, just like you need a map to.
Of Chicago, through the city of Chicago, you need a topical map to tour the information space.
So how to construct such a topical map is in fact a very interesting research question that might bring us more interesting browsing experience on the web or in applications.
So, to summarize this lecture, we've talked about the two high level strategies for text access; push and pull.
Push tends to be supported by the Recommender System, and Pull tends to be supported by the Search Engine.
Of course, in the sophisticated  information system, we should combine the two.
In the pull mode, we can further this  Querying and Browsing.
Again we generally want to combine the two ways to help you assist, so that you can support the both querying nad browsing.
If you want to know more about the relationship between pull and push, you can read this article.
This give excellent discussion of the relationship between machine filtering and information retrieval.
Here informational filtering is similar to information recommendation or the push mode of information access.




Session 1.3: Text Retrieval Problem

This lecture is about the text retrieval problem.
This picture shows our overall plan for lectures.
In the last lecture, we talked about the high level strategies for text access.
We talked about push versus pull.
Such engines are the main tools for supporting the pull mode.
Starting from this lecture, we're going to talk about the how search engines work in detail.
So first it's about the text retrieval problem.
We're going to talk about the three things in this lecture.
First, we define Text Retrieval.
Second we're going to make a comparison between Text Retrieval and the related task Database Retrieval.
Finally, we're going to talk about the Document Selection versus Document Ranking as two strategies for responding to a user's query.
So what is Text Retrieval?
It should be a task that's familiar for the most of us because we're using web search engines all the time.
So text retrieval is basically a task where the system would respond to a user's query With relevant documents.
Basically, it's for supporting a query as one way to implement the poll mode of information access.
So the situation is the following.
You have a collection of text retrieval documents.
These documents could be all the webpages on the web, or all the literature articles in the digital library.
Or maybe all the text files in your computer.
A user will typically give a query to the system to express information need.
And then, the system would return relevant documents to users.
Relevant documents refer to those documents that are useful to the user who typed in the query.
All this task is a phone call that information retrieval.
But literally information retrieval would broadly include the retrieval of other non-textual information as well, for example audio, video, etc.
It's worth noting that Text Retrieval is at the core of information retrieval in the sense that other medias such as video can be retrieved by exploiting the companion text data.
So for example, current the image search engines actually match a user's query was the companion text data of the image.
This problem is also called search problem.
And the technology is often called the search technology industry.
If you ever take a course in databases it will be useful to pause the lecture at this point and think about the differences between text retrieval and database retrieval.
Now these two tasks are similar in many ways.
But, there are some important differences.
So, spend a moment to think about the differences between the two.
Think about the data, and the information managed by a search engine versus those that are managed by a database system.
Think about the different between the queries that you typically specify for database system versus queries that are typed in by users in a search engine.
And then finally think about the answers.
What's the difference between the two?
Okay, so if we think about the information or data managed by the two systems, we will see that in text retrieval.
The data is unstructured, it's free text.
But in databases, they are structured data where there is a clear defined schema to tell you this column is the names of people and that column is ages, etc.
The unstructured text is not obvious what are the names of people mentioned in the text.
Because of this difference, we also see that text information tends to be more ambiguous and we talk about that in the processing chapter, whereas in databases.
But they don't tend to have where to find the semantics.
The results important difference in the queries, and this is partly due to the difference in the information or data.
So test queries tend to be ambiguous.
Whereas in their research, the queries are typically well-defined.
Think about a SQL query that would clearly specify what records to be returned.
So it has very well-defined semantics.
Keyword queries or electronic queries tend to be incomplete, also in that it doesn't really specify what documents should be retrieved.
Whereas complete specification for what should be returned.
And because of these differences, the answers would be also different.
Being the case of text retrieval, we're looking for it rather than the documents.
In the database search, we are retrieving records or match records with the sequel query more precisely.
Now in the case of text retrieval, what should be the right answers to the query is not very well specified, as we just discussed.
So it's unclear what should be the right answers to a query.
And this has very important consequences, and that is, textual retrieval is an empirically defined problem.
So this is a problem because if it's empirically defined, then we can not mathematically prove one method is better than another method.
That also means we must rely on empirical evaluation involving users to know which method works better.
And that's why we have.
You need more than one lectures to cover the issue of evaluation.
Because this is very important topic for Sir Jennings.
Without knowing how to evaluate heroism properly, there's no way to tell whether we have got the better or whether one system is better than another.
So now let's look at the problem in a formal way.
So, this slide shows a formal formulation of the text retrieval problem.
First, we have our vocabulary set, which is just a set of words in a language.
Now here, we are considering only one language, but in reality, on the web, there might be multiple natural languages.
We have texts that are in all kinds of languages.
But here for simplicity, we just assume that is one kind of language.
As the techniques used for retrieving data from multiple languages Are more or less similar to the techniques used for retrieving documents in one end, which although there is important difference, the principle methods are very similar.
Next, we have the query, which is a sequence of words.
And so here, you can see the query is defined as a sequence of words.
Each q sub i is a word in the vocabulary.
A document is defined in the same way, so it's also a sequence of words.
And here, d sub ij is also a word in the vocabulary.
Now typically, the documents are much longer than queries.
But there are also cases where the documents may be very short.
So you can think about what might be a example of that case.
I hope you can think of Twitter search.
Tweets are very short.
But in general, documents are longer than the queries.
Now, then we have a collection of documents, and this collection can be very large.
So think about the web.
It could be very large.
And then the goal of text retrieval is you'll find the set of relevant in the documents, which we denote by R'(q), because it depends on the query.
And this in general, a subset of all the documents in the collection.
Unfortunately, this set of relevant documents is generally unknown, and user-dependent in the sense that, for the same query typed in by different users, they expect the relevant documents may be different.
The query given to us by the user is only a hint on which document should be in this set.
And indeed, the user is generally unable to specify what exactly should be in this set, especially in the case of web search, where the connection's so large, the user doesn't have complete knowledge about the whole production.
So the best search system can do is to compute an approximation of this relevant document set.
So we denote it by R'(q).
So formerly, we can see the task is to compute this R'(q) approximation of the relevant documents.
So how can we do that?
Now imagine if you are now asked to write a program to do this.
What would you do?
Now think for a moment.
Right, so these are your input.
The query, the documents.
And then you are to compute the answers to this query, which is a set of documents that would be useful to the user.
So, how would you solve the problem?
Now in general, there are two strategies that we can use.
The first strategy is we do a document selection, and that is, we're going to have a binary classification function, or binary classifier.
That's a function that would take a document and query as input, and then give a zero or one as output to indicate whether this document is relevant to the query or not.
So in this case, you can see the document.
The relevant document is set, is defined as follows.
It basically, all the documents that have a value of 1 by this function.
So in this case, you can see the system must have decide if the document is relevant or not.
Basically, it has to say whether it's one or zero.
And this is called absolute relevance.
Basically, it needs to know exactly whether it's going to be useful to the user.
Alternatively, there's another strategy called document ranking.
Now in this case, the system is not going to make a call whether a document is random or not.
But rather the system is going to use a real value function, f here.
That would simply give us a value that would indicate which document is more likely relevant.
So it's not going to make a call whether this document is relevant or not.
But rather it would say which document is more likely relevant.
So this function then can be used to random documents, and then we're going to let the user decide where to stop, when the user looks at the document.
So we have a threshold theta here to determine what documents should be in this approximation set.
And we're going to assume that all the documents that are ranked above the threshold are in this set, because in effect, these are the documents that we deliver to the user.
And theta is a cutoff determined by the user.
So here we've got some collaboration from the user in some sense, because we don't really make a cutoff.
And the user kind of helped the system make a cutoff.
So in this case, the system only needs to decide if one document is more likely relevant than another.
And that is, it only needs to determine relative relevance, as opposed to absolute relevance.
Now you can probably already sense that relative relevance would be easier to determine than absolute relevance.
Because in the first case, we have to say exactly whether a document is relevant or not.
And it turns out that ranking is indeed generally preferred to document selection.
So let's look at these two strategies in more detail.
So this picture shows how it works.
So on the left side, we see these documents, and we use the pluses to indicate the relevant documents.
So we can see the true relevant documents here consists this set of true relevant documents, consists of these process, these documents.
And with the document selection function, we're going to basically classify them into two groups, relevant documents, and non-relevant ones.
Of course, the classified will not be perfect so it will make mistakes.
So here we can see, in the approximation of the relevant documents, we have got some number in the documents.
And similarly, there is a relevant document that's misclassified as non-relevant.
In the case of document ranking, we can see the system seems like, simply ranks all the documents in the descending order of the scores.
And then, we're going to let the user stop wherever the user wants to stop.
If the user wants to examine more documents, then the user will scroll down some more and then stop .
But if the user only wants to read a few random documents, the user might stop at the top position.
So in this case, the user stops at d4.
So in fact, we have delivered these four documents to our user.
So as I said ranking is generally preferred, and one of the reasons is because the classifier in the case of document selection is unlikely accurate.
Why?
Because the only clue is usually the query.
But the query may not be accurate in the sense that it could be overly constrained.
For example, you might expect relevant documents to talk about all these topics by using specific vocabulary.
And as a result, you might match no relevant documents.
Because in the collection, no others have discussed the topic using these vocabularies, right?
So in this case, we'll see there is this problem of no relevant documents to return in the case of over-constrained query.
On the other hand, if the query is under-constrained, for example, if the query does not have sufficient descriptive words to find the random documents.
You may actually end up having of over delivery, and this when you thought these words my be sufficient to help you find the right documents.
But, it turns out they are not sufficient and there are many distractions, documents using similar words.
And so, this is a case of over delivery.
Unfortunately, it's very hard to find the right position between these two extremes.
Why?
Because whether users looking for the information in general the user does not have a good knowledge about the information to be found.
And in that case, the user does not have a good knowledge about what vocabularies will be used in those relevent documents.
So it's very hard for a user to pre-specify the right level of constraints.
Even if the classifier is accurate, we also still want to rend these relevant documents, because they are generally not equally relevant.
Relevance is often a matter of degree.
So we must prioritize these documents for a user to examine.
And note that this prioritization is very important because a user cannot digest all the content the user generally would have to look at each document sequentially.
And therefore, it would make sense to users with the most relevant documents.
And that's what ranking is doing.
So for these reasons, ranking is generally preferred.
Now this preference also has a theoretical justification and this is given by the probability ranking principle.
In the end of this lecture, there is reference for this.
This principle says, returning a ranked list of documents in descending order of probability that a document is relevant to the query is the optimal strategy under the following two assumptions.
First, the utility of a document (to a user) Is independent of the utility of any other document.
Second, a user would be assumed to browse the results sequentially.
Now it's easy to understand why these assumptions are needed in order to justify Site for the ranking strategy.
Because if the documents are independent, then we can evaluate the utility of each document that's separate.
And this would allow the computer score for each document independently.
And then, we are going to rank these documents based on the scrolls.
The second assumption is to say that the user would indeed follow the rank list.
If the user is not going to follow the ranked list, is not going to examine the documents sequentially, then obviously the ordering would not be optimal.
So under these two assumptions, we can theoretically justify the ranking strategy is, in fact, the best that you could do.
Now, I've put one question here.
Do these two assumptions hold?
I suggest you to pause the lecture, for a moment, to think about this.
Now, can you think of some examples that would suggest these assumptions aren't necessarily true.
Now, if you think for a moment, you may realize none of the assumptions Is actually true.
For example, in the case of independence assumption we might have documents that have similar or exactly the same content.
If we look at each of them alone, each is relevant.
But if the user has already seen one of them, we can assume it's generally not very useful for the user to see another similar or duplicated one.
So clearly the utility on the document that is dependent on other documents that the user has seen.
In some other cases you might see a scenario where one document that may not be useful to the user, but when three particular documents are put together.
They provide answers to the user's question.
So this is a collective relevance and that also suggests that the value of the document might depend on other documents.
Sequential browsing generally would make sense if you have a ranked list there.
But even if you have a rank list, there is evidence showing that users don't always just go strictly sequentially through the entire list.
They sometimes will look at the bottom for example, or skip some.
And if you think about the more complicated interfaces that we could possibly use like two dimensional in the phase.
Where you can put that additional information on the screen then sequential browsing is a very restricted assumption.
So the point here is that none of these assumptions is really true but less than that.
But probability ranking principle establishes some solid foundation for ranking as a primary pattern for search engines.
And this has actually been the basis for a lot of research work in information retrieval.
And many hours have been designed based on this assumption, despite that the assumptions aren't necessarily true.
And we can address this problem by doing post processing Of a ranked list, for example, to remove redundancy.
So to summarize this lecture, the main points that you can take away are the following.
First, text retrieval is an empirically defined Problem.
And that means which algorithm is better must be judged by the users.
Second, document ranking is generally preferred.
And this will help users prioritize examination of search results.
And this is also to bypass the difficulty in determining absolute relevance Because we can get some help from users in determining where to make the cut off, it's more flexible.
So, this further suggests that the main technical challenge in designing a search engine is the design effective ranking function.
In other words, we need to define what is the value of this function F on the query and document pair.
How we design such a function is the main topic in the following lectures.
There are two suggested additional readings.
The first is the classical paper on the probability ranking principle.
The second one is a must-read for anyone doing research on information retrieval.
It's a classic IR book, which has excellent coverage of the main research and results in early days up to the time when the book was written.
Chapter six of this book has an in-depth discussion of the Probability Ranking Principle and Probably for retrieval models in general.




Session 1.4: Overview Of Text Retrieval Methods

This lecture is a overview of text retrieval methods.
In the previous lecture, we introduced the problem of text retrieval.
We explained that the main problem is the design of ranking function to rank documents for a query.
In this lecture, we will give an overview of different ways of designing this ranking function.
So the problem is the following.
We have a query that has a sequence of words and the document that's also a sequence of words.
And we hope to define a function f that can compute a score based on the query and document.
So the main challenge you hear is with design a good ranking function that can rank all the relevant documents on top of all the non-relevant ones.
Clearly, this means our function must be able to measure the likelihood that a document d is relevant to a query q.
That also means we have to have some way to define relevance.
In particular, in order to implement the program to do that, we have to have a computational definition of relevance.
And we achieve this goal by designing a retrieval model, which gives us a formalization of relevance.
Now, over many decades, researchers have designed many different kinds of retrieval models.
And they fall into different categories.
First, one family of the models are based on the similarity idea.
Basically, we assume that if a document is more similar to the query than another document is, then we will say the first document is more relevant than the second one.
So in this case, the ranking function is defined as the similarity between the query and the document.
One well known example in this case is vector space model, which we will cover more in detail later in the lecture.
A second kind of models are called probabilistic models.
In this family of models, we follow a very different strategy, where we assume that queries and documents are all observations from random variables.
And we assume there is a binary random variable called R here to indicate whether a document is relevant to a query.
We then define the score of document with respect to a query as a probability that this random variable R is equal to 1, given a particular document query.
There are different cases of such a general idea.
One is classic probabilistic model, another is language model, yet another is divergence from randomness model.
In a later lecture, we will talk more about one case, which is language model.
A third kind of model are based on probabilistic inference.
So here the idea is to associate uncertainty to inference rules, and we can then quantify the probability that we can show that the query follows from the document.
Finally, there is also a family of models that are using axiomatic thinking.
Here, an idea is to define a set of constraints that we hope a good retrieval function to satisfy.
So in this case, the problem is to seek a good ranking function that can satisfy all the desired constraints.
Interestingly, although these different models are based on different thinking, in the end, the retrieval function tends to be very similar.
And these functions tend to also involve similar variables.
So now let's take a look at the common form of a state of the art retrieval model and to examine some of the common ideas used in all these models.
First, these models are all based on the assumption of using bag of words to represent text, and we explained this in the natural language processing lecture.
Bag of words representation remains the main representation used in all the search engines.
So with this assumption, the score of a query, like a presidential campaign news with respect to a document of d here, would be based on scores computed based on each individual word.
And that means the score would depend on the score of each word, such as presidential, campaign, and news.
Here, we can see there are three different components, each corresponding to how well the document matches each of the query words.
Inside of these functions, we see a number of heuristics used.
So for example, one factor that affects the function d here is how many times does the word presidential occur in the document?
This is called a term frequency, or TF.
We might also denote as c of presidential and d.
In general, if the word occurs more frequently in the document, then the value of this function would be larger.
Another factor is, how long is the document?
And this is to use the document length for scoring.
In general, if a term occurs in a long document many times, it's not as significant as if it occurred the same number of times in a short document.
Because in a long document, any term is expected to occur more frequently.
Finally, there is this factor called document frequency.
That is, we also want to look at how often presidential occurs in the entire collection, and we call this document frequency, or df of presidential.
And in some other models, we might also use a probability to characterize this information.
So here, I show the probability of presidential in the collection.
So all these are trying to characterize the popularity of the term in the collection.
In general, matching a rare term in the collection is contributing more to the overall score than matching up common term.
So this captures some of the main ideas used in pretty much older state of the art original models.
So now, a natural question is, which model works the best?
Now it turns out that many models work equally well.
So here are a list of the four major models that are generally regarded as a state of the art original models, pivoted length normalization, BM25, query likelihood, PL2.
When optimized, these models tend to perform similarly.
And this was discussed in detail in this reference at the end of this lecture.
Among all these, BM25 is probably the most popular.
It's most likely that this has been used in virtually all the search engines, and you will also often see this method discussed in research papers.
And we'll talk more about this method later in some other lectures.
So, to summarize, the main points made in this lecture are first the design of a good ranking function pre-requires a computational definition of relevance, and we achieve this goal by designing appropriate retrieval model.
Second, many models are equally effective, but we don't have a single winner yet.
Researchers are still active and working on this problem, trying to find a truly optimal retrieval model.
Finally, the state of the art ranking functions tend to rely on the following ideas.
First, bag of words representation.
Second, TF and document frequency of words.
Such information is used in the weighting function to determine the overall contribution of matching a word and document length.
These are often combined in interesting ways, and we'll discuss how exactly they are combined to rank documents in the lectures later.
There are two suggested additional readings if you have time.
The first is a paper where you can find the detailed discussion and comparison of multiple state of the art models.
The second is a book with a chapter that gives a broad review of different retrieval models.




Session 1.5: Vector Space Model Basic Idea

This lecture is about the vector space retrieval model.
We're going to give an introduction to its basic idea.
In the last lecture, we talked about the different ways of designing a retrieval model, which would give us a different arranging function.
In this lecture, we're going to talk about a specific way of designing a ramping function called a vector space retrieval model.
And we're going to give a brief introduction to the basic idea.
Vector space model is a special case of similarity based models as we discussed before.
Which means we assume relevance is roughly similarity, between the document and the query.
Now whether is this assumption is true is actually a question.
But in order to solve the search problem, we have to convert the vague notion of relevance into a more precise definition that can be implemented with the program analogy.
So in this process, we have to make a number of assumptions.
This is the first assumption that we make here.
Basically, we assume that if a document is more similar to a query than another document.
Then the first document will be assumed it will be more relevant than the second one.
And this is the basis for ranking documents in this approach.
Again, it's questionable whether this is really the best definition for randoms.
As we will see later there are other ways to model randoms.
The basic idea of vectors for base retrieval model is actually very easy to understand.
Imagine a high dimensional space where each dimension corresponds to a term.
So here I issue a three dimensional space with three words, programming, library and presidential.
So each term here defines one dimension.
Now we can consider vectors in this, three dimensional space.
And we're going to assume that all our documents and the query will be placed in this vector space.
So for example, on document might be represented by this vector, d1.
Now this means this document probably covers library and presidential, but it doesn't really talk about programming.
What does this mean in terms of representation of document?
That just means we're going to look at our document from the perspective of this vector.
We're going to ignore everything else.
Basically, what we see here is only the vector root condition of the document.
Of course, the document has all information.
For example, the orders of words are  model and that's because we assume that the  of words will .
So with this presentation you can really see d1 simply suggests a  library.
Now this is different from another document which might be recommended as a different vector, d2 here.
Now in this case, the document that covers programming and library, but it doesn't talk about presidential.
So what does this remind you?
Well you can probably guess the topic is likely about program language and the library is software lab library.
So this shows that by using this vector space reproduction, we can actually capture the differences between topics of documents.
Now you can also imagine there are other vectors.
For example, d3 is pointing into that direction, that might be a presidential program.
And in fact we can place all the documents in this vector space.
And they will be pointing to all kinds of directions.
And similarly, we're going to place our query also in this space, as another vector.
And then we're going to measure the similarity between the query vector and every document vector.
So in this case for example, we can easily see d2 seems to be the closest to this query vector.
And therefore, d2 will be rendered above others.
So this is basically the main idea of the vector space model.
So to be more precise, vector space model is a framework.
In this framework, we make the following assumptions.
First, we represent a document and query by a term vector.
So here a term can be any basic concept.
For example, a word or a phrase or even n gram of characters.
Those are just sequence of characters inside a word.
Each term is assumed that will be defined by one dimension.
Therefore n terms in our vocabulary, we define N-dimensional space.
A query vector would consist of a number of elements corresponding to the weights on different terms.
Each document vector is also similar.
It has a number of elements and each value of each element is indicating the weight of the corresponding term.
Here, you can see, we assume there are N dimensions.
Therefore, they are N elements each corresponding to the weight on the particular term.
So the relevance in this case will be assumed to be the similarity between the two vectors.
Therefore, our ranking function is also defined as the similarity between the query vector and document vector.
Now if I ask you to write a program to implement this approach in a search engine.
You would realize that this was far from clear.
We haven't said a lot of things in detail, therefore it's impossible to actually write the program to implement this.
That's why I said, this is a framework.
And this has to be refined in order to actually suggest a particular ranking function that you can implement on a computer.
So what does this framework not say?
Well, it actually hasn't said many things that would be required in order to implement this function.
First, it did not say how we should define or select the basic concepts exactly.
We clearly assume the concepts are orthogonal.
Otherwise, there will be redundancy.
For example, if two synonyms or somehow distinguish it as two different concepts.
Then they would be defining two different dimensions and that would clearly cause redundancy here.
Or all the emphasizing of matching this concept, because it would be as if you match the two dimensions when you actually matched one semantic concept.
Secondly, it did not say how we exactly should place documents and the query in this space.
Basically that show you some examples of query and document vectors.
But where exactly should the vector for a particular document point to?
So this is equivalent to how to define the term weights?
How do you compute the lose element values in those vectors?
This is a very important question, because term weight in the query vector indicates the importance of term.
So depending on how you assign the weight, you might prefer some terms to be matched over others.
Similarly, the total word in the document is also very meaningful.
It indicates how well the term characterizes the document.
If you got it wrong then you clearly don't represent this document accurately.
Finally, how to define the similarity measure is also not given.
So these questions must be addressed before we can have a operational function that we can actually implement using a program language.
So how do we solve these problems is the main topic of the next lecture.




Session 1.6: Vector Space Retrieval Model Simplest Instantiation

In this lecture we're going to talk about how to instantiate vector space model so that we can get very specific ranking function.
So this is to continue the discussion of the vector space model, which is one particular approach to design a ranking function.
And we're going to talk about how we use the general framework of the the vector space model as a guidance to instantiate the framework to derive a specific ranking function.
And we're going to cover the symbolist instantiation of the framework.
So as we discussed in the previous lecture, the vector space model is really a framework.
And this didn't say.
As we discussed in the previous lecture, vector space model is really a framework.
It does not say many things.
So, for example, here it shows that it did not say how we should define the dimension.
It also did not say how we place a document vector in this space.
It did not say how we place a query vector in this vector space.
And, finally, it did not say how we should measure the similarity between the query vector and the document vector.
So you can imagine, in order to implement this model, we have to say specifically how we compute these vectors.
What is exactly xi?
And what is exactly yi?
This will determine where we place a document vector, where we place a query vector.
And, of course, we also need to say exactly what should be the similarity function.
So if we can provide a definition of the concepts that would define the dimensions and these xi's or yi's and namely weights of terms for queries and document, then we will be able to place document vectors and query vectors in this well defined space.
And then, if we also specify similarity function, then we'll have a well defined ranking function.
So let's see how we can do that and think about the instantiation.
Actually, I would suggest you to pause the lecture at this point, spend a couple minutes to think about.
Suppose you are asked to implement this idea.
You have come up with the idea of vector space model, but you still haven't figured out how to compute these vectors exactly, how to define the similarity function.
What would you do?
So, think for a couple of minutes, and then proceed.
So, let's think about some simplest ways of instantiating this vector space model.
First, how do we define the dimension?
Well, the obvious choice is to use each word in our vocabulary to define the dimension.
And show that there are N words in our vocabulary.
Therefore, there are N dimensions.
Each word defines one dimension.
And this is basically the bag of words with Now let's look at how we place vectors in this space.
Again here, the simplest strategy is to use a Bit Vector to represent both the query and a document.
And that means each element, xi and yi will be taking a value of either zero or 1.
When it's 1, it means the corresponding word is present in the document or in the query.
When it's 0, it's going to mean that it's absent.
So you can imagine if the user types in a few words in the query, then the query vector will only have a few 1's, many, many zeros.
The document vector, generally we have more 1's, of course.
But it will also have many zeros since the vocabulary is generally very large.
Many words don't really occur in any document.
Many words will only occasionally occur in a document.
A lot of words will be absent in a particular document.
So now we have placed the documents and the query in the vector space.
Let's look at how we measure the similarity.
So, a commonly used similarity measure here is Dot Product.
The Dot Product of two vectors is simply defined as the sum of the products of the corresponding elements of the two vectors.
So, here we see that it's the product of x1 and y1.
So, here.
And then, x2 multiplied by y2.
And then, finally, xn multiplied by yn.
And then, we take a sum here.
So that's a Dot Product.
Now, we can represent this in a more general way using a sum here.
So this is only one of the many different ways of measuring the similarity.
So, now we see that we have defined the dimensions, we have defined the vectors, and we have also defined the similarity function.
So now we finally have the simplest vector space model, which is based on the bit vector  dot product similarity and bag of words .
And the formula looks like this.
So this is our formula.
And that's actually a particular retrieval function, a ranking function right?
Now we can finally implement this function using a program language, and then rank the documents for query.
Now, at this point you should again pause the lecture to think about how we can interpreted this score.
So, we have gone through the process of modeling the retrieval problem using a vector space model.
And then, we make assumptions about how we place vectors in the vector space, and how do we define the similarity.
So in the end, we've got a specific retrieval function shown here.
Now, the next step is to think about whether this retrieval function actually makes sense, right?
Can we expect this function to actually perform well when we used it to rank documents for user's queries?
So it's worth thinking about what is this value that we are calculating.
So, in the end, we'll get a number.
But what does this number mean?
Is it meaningful?
So, spend a couple minutes to sort of think about that.
And, of course, the general question here is do you believe this is a good ranking function?
Would it actually work well?
So, again, think about how to interpret this value.
Is it actually meaningful?
Does it mean something?
This is related to how well the document matched the query.
So, in order to assess whether this simplest vector space model actually works well, let's look at the example.
So, here I show some sample documents and a sample query.
The query is news about the presidential campaign.
And we have five documents here.
They cover different terms in the query.
And if you look at these documents for a moment, you may realize that some documents are probably relevant, and some others are probably not relevant.
Now, if I asked you to rank these documents, how would you rank them?
This is basically our ideal ranking.
When humans can examine the documents, and then try to rank them.
Now, so think for a moment, and take a look at this slide.
And perhaps by pausing the lecture.
So I think most of you would agree that d4 and d3 are probably better than others because they really cover the query well.
They match news, presidential and campaign.
So, it looks like these documents are probably better than the others.
They should be ranked on top.
And the other three d2, d1, and d5 are really not relevant.
So we can also say d4 and d3 are relevant documents, and d1, d2 and d5 are non-relevant.
So now let's see if our simplest vector space model could do the same, or could do something closer.
So, let's first think about how we actually use this model to score documents.
All right.
Here I show two documents, d1 and d3.
And we have the query also here.
In the vector space model, of course we want to first compute the vectors for these documents and the query.
Now, I showed the vocabulary here as well.
So these are the end dimensions that we'll be thinking about.
So what do you think is the vector for the query?
Note that we're assuming that we only use zero and 1 to indicate whether a term is absent or present in the query or in the document.
So these are zero,1 bit vectors.
So what do you think is the query vector?
Well, the query has four words here.
So for these four words, there will be a 1.
And for the rest, there will be zeros.
Now, what about the documents?
It's the same.
So d1 has two rows, news and about.
So, there are two 1's here, and the rest are zeroes.
Similarly, so now that we have the two vectors, let's compute the similarity.
And we're going to use Do Product.
So you can see when we use Dot Product, we just multiply the corresponding elements, right?
So these two will be formal product, and these two will generate another product, and these two will generate yet another product and so on, so forth.
Now you can easily see if we do that, we actually don't have to care about these zeroes because whenever we have a zero the product will be zero.
So when we take a sum over all these pairs, then the zero entries will be gone.
As long as you have one zero, then the product would be zero.
So, in the fact, we're just counting how many pairs of 1 and 1.
In this case, we have seen two, so the result will be 2.
So what does that mean?
Well, that means this number, or the value of this scoring function, is simply the count of how many unique query terms are matched in the document.
Because if a term is matched in the document, then there will be two one's.
If it's not, then there will be zero on the document side.
Similarly, if the document has a term but the term is not in the query, there will be a zero in the query vector.
So those don't count.
So, as a result, this scoring function basically measures how many unique query terms are matched in a document.
This is how we interpret this score.
Now, we can also take a look at d3.
In this case, you can see the result is 3 because d3 matched to the three distinctive query words news, presidential campaign, whereas d1 only matched the two.
Now in this case, this seems reasonable to rank d3 on top of d1.
And this simplest vector space model indeed does that.
So that looks pretty good.
However, if we examine this model in detail, we likely will find some problems.
So, here I'm going to show all the scores for these five documents.
And you can easily verify they're correct because we're basically counting the number of unique query terms matched in each document.
Now note that this measure actually makes sense, right?
It basically means if a document matches more unique query terms, then the document will be assumed to be more relevant.
And that seems to make sense.
The only problem is here we can note that there are three documents, d2, d3 and d4.
And they tied with a 3 as a score.
So, that's a problem because if you look at them carefully, it seems that the d4 should be ranked above d3 because d3 only mentions the presidential once, but d4 mentioned it multiple times.
In the case of d3, presidential could be an dimension.
But d4 is clearly above the presidential campaign.
Another problem is that d2 and d3 also have the same score.
But if you look at the three words that are matched, in the case of d2, it matched the news, about and campaign.
But in the case of d3, it matched news, presidential and campaign.
So intuitively this reads better because matching presidential is more important than matching about, even though about and the presidential are both in the query.
So intuitively, we would like d3 to be ranked above d2.
But this model doesn't do that.
So that means this model is still not good enough.
We have to solve these problems.
To summarize, in this lecture we talked about how to instantiate a vector space model.
We mainly need to do three things.
One is to define the dimension.
The second is to decide how to place documents as vectors in the vector space, and to also place a query in the vector space as a vector.
And third is to define the similarity between two vectors, particularly the query vector and the document vector.
We also talked about various simple way to instantiate the vector space model.
Indeed, that's probably the simplest vector space model that we can derive.
In this case, we use each word to define the dimension.
We use a zero, 1 bit vector to represent a document or a query.
In this case, we basically only care about word presence or absence.
We ignore the frequency.
And we use the Dot Product as the similarity function.
And with such a instantiation, we showed that the scoring function is basically to score a document based on the number of distinct query words matched in the document.
We also showed that such a simple vector space model still doesn't work well, and we need to improve it.
And this is a topic that we're going to cover in the next lecture.




Session 2.1: Vector Space Model Improved Instantiation

In this lecture, we are going to talk about how to improve the instantiation of the vector space model.
This is a continued discussion of the vector space model.
We're going to focus on how to improve the instantiation of this model.
In the previous lecture, you have seen that with simple instantiations of the vector space model, we can come up with a simple scoring function that would give us basically an account of how many unique query terms are matched in the document.
We also have seen that this function has a problem, as shown on this slide.
In particular, if you look at these three documents, they will all get the same score because they match the three unique query words.
But intuitively we would like d4 to be ranked above d3, and d2 is really not relevant.
So the problem here is that this function couldn't capture the following heuristics.
First, we would like to give more credit to d4 because it matched presidential more times than d3.
Second, intuitively, matching presidential should be more important than matching about, because about is a very common word that occurs everywhere.
It doesn't really carry that much content.
So in this lecture, let's see how we can improve the model to solve these two problems.
It's worth thinking at this point about why do we have these problems?
If we look back at assumptions we have made while instantiating the vector space model, we'll realize that the problem is really coming from some of the assumptions.
In particular, it has to do with how we placed the vectors in the vector space.
So then naturally, in order to fix these problems, we have to revisit those assumptions.
Perhaps we will have to use different ways to instantiate the vector space model.
In particular, we have to place the vectors in a different way.
So let's see how we can improve this.
One natural thought is in order to consider multiple times of a term in the document, we should consider the term frequency instead of just the absence or presence.
In order to consider the difference between a document where a query term occurred multiple times and one where the query term occurred just once, we have to consider the term frequency, the count of a term in the document.
In the simplest model, we only modeled the presence and absence of a term.
We ignored the actual number of times that a term occurs in a document.
So let's add this back.
So we're going to then represent a document by a vector with term frequency as element.
So that is to say, now the elements of both the query vector and the document vector will not be 0 or 1s, but instead they will be the counts of a word in the query or the document.
So this would bring in additional information about the document, so this can be seen as more accurate representation of our documents.
So now let's see what the formula would look like if we change this representation.
So as you'll see on this slide, we still use dot product.
And so the formula looks very similar in the form.
In fact, it looks identical.
But inside the sum, of course, x i and y i are now different.
They are now the counts of word i in the query and in the document.
Now at this point I also suggest you to pause the lecture for a moment and just to think about how we can interpret the score of this new function.
It's doing something very similar to what the simplest VSM is doing.
But because of the change of the vector, now the new score has a different interpretation.
Can you see the difference?
And it has to do with the consideration of multiple occurrences of the same term in a document.
More importantly, we would like to know whether this would fix the problems of the simplest vector space model.
So let's look at this example again.
So suppose we change the vector representation to term frequency vectors.
Now let's look at these three documents again.
The query vector is the same because all these words occurred exactly once in the query.
So the vector is still a 01 vector.
And in fact, d2 is also essentially representing the same way because none of these words has been repeated many times.
As a result, the score is also the same, still 3.
The same is true for d3, and we still have a 3.
But d4 would be different, because now presidential occurred twice here.
So the ending for presidential in the document vector would be 2 instead of 1.
As a result, now the score for d4 is higher.
It's a 4 now.
So this means by using term frequency, we can now rank d4 above d2 and d3, as we hoped to.
So this solved the problem with d4.
But we can also see that d2 and d3 are still filtering the same way.
They still have identical scores, so it did not fix the problem here.
So how can we fix this problem?
Intuitively, we would like to give more credit for matching presidential than matching about.
But how can we solve the problem in a general way?
Is there any way to determine which word should be treated more importantly and which word can be basically ignored?
About is such a word which does not really carry that much content.
We can essentially ignore that.
We sometimes call such a word a stock word.
Those are generally very frequent and they occur everywhere.
Matching it doesn't really mean anything.
But computationally how can we capture that?
So again, I encourage you to think a little bit about this.
Can you came up with any statistical approaches to somehow distinguish presidential from about?
Now if you think about it for a moment, you'll realize that one difference is that a word like above occurs everywhere.
So if you count the occurrence of the word in the whole collection, then we will see that about has much higher frequency than presidential, which tends to occur only in some documents.
So this idea suggests that we could somehow use the global statistics of terms or some other information to trying to down-weight the element of about in a vector representation of d2.
At the same time, we hope to somehow increase the weight of presidential in the vector of d3.
If we can do that, then we can expect that d2 will get the overall score to be less than 3 while d3 will get the score above 3.
Then we would be able to rank d3 on top of d2.
So how can we do this systematically?
Again, we can rely on some statistical count.
And in this case, the particular idea is called inverse document frequency.
Now we have seen document frequency as one signal used in the modern retrieval functions.
We discussed this in a previous lecture.
So here is the specific way of using it.
Document frequency is the count of documents that contain a particular term.
Here we say inverse document frequency because we actually want to reward a word that doesn't occur in many documents.
And so the way to incorporate this into our vector representation is then to modify the frequency count by multiplying it by the IDF of the corresponding word, as shown here.
If we can do that, then we can penalize common words, which generally have a lower IDF, and reward rare words, which will have a higher IDF.
So more specifically, the IDF can be defined as the logarithm of M+1 divided by k, where M is the total number of documents in the collection, k is the DF or document frequency, the total number of documents containing the word W.
Now if you plot this function by varying k, then you would see the curve would look like this.
In general, you can see it would give a higher value for a low DF word, a rare word.
You can also see the maximum value of this function is log of M+1.
It would be interesting for you to think about what's the minimum value for this function.
This could be an interesting exercise.
Now the specific function may not be as important as the heuristic to simply penalize popular terms.
But it turns out that this particular function form has also worked very well.
Now whether there's a better form of function here is the open research question.
But it's also clear that if we use a linear penalization, like what's shown here with this line, then it may not be as reasonable as the standard IDF.
In particular, you can see the difference in the standard IDF, and we somehow have a turning point of here.
After this point, we're going to say these terms are essentially not very useful.
They can be essentially ignored.
And this makes sense when the term occurs so frequently and let's say a term occurs in more than 50% of the documents, then the term is unlikely very important and it's basically a common term.
It's not very important to match this word.
So with the standard IDF you can see it's basically assumed that they all have low weights.
There's no difference.
But if you look at the linear penalization, at this point that there is still some difference.
So intuitively we'd want to focus more on the discrimination of low DF words rather than these common words.
Well, of course, which one works better still has to be validated by using the empirically correlated dataset.
And we have to use users to judge which results are better.
So now let's see how this can solve problem 2.
So now let's look at the two documents again.
Now without the IDF weighting before, we just have term frequency vectors.
But with IDF weighting we now can adjust the TF weight by multiplying with the IDF value.
For example, here we can see is adjustment and in particular for about there's adjustment by using the IDF value of about, which is smaller than the IDF value of presidential.
So if you look at these, the IDF will distinguish these two words.
As a result, adjustment here would be larger, would make this weight larger.
So if we score with these new vectors, then what would happen is that, of course, they share the same weights for news and campaign, but the matching of about will discriminate them.
So now as a result of IDF weighting, we will have d3 to be ranked above d2 because it matched a rare word, whereas d2 matched a common word.
So this shows that the IDF weighting can solve problem 2.
So how effective is this model in general when we used TF-IDF weighting?
Well, let's look at all these documents that we have seen before.
These are the new scores of the new documents.
But how effective is this new weighting method and new scoring function point?
So now let's see overall how effective is this new ranking function with TF-IDF weighting.
Here we show all the five documents that we have seen before, and these are their scores.
Now we can see the scores for the first four documents here seem to be quite reasonable.
They are as we expected.
However, we also see a new problem because now d5 here, which did not have a very high score with our simplest vector space model, now actually has a very high score.
In fact, it has the highest score here.
So this creates a new problem.
This is actually a common phenomenon in designing retrieval functions.
Basically, when you try to fix one problem, you tend to introduce other problems.
And that's why it's very tricky how to design effective ranking function.
And what's the best ranking function is their open research question.
Researchers are still working on that.
But in the next few lectures we're going to also talk about some additional ideas to further improve this model and try to fix this problem.
So to summarize this lecture, we've talked about how to improve the vector space model, and we've got to improve the instantiation of the vector space model based on TD-IDF weighting.
So the improvement is mostly on the placement of the vector where we give high weight to a term that occurred many times in a document but infrequently in the whole collection.
And we have seen that this improved model indeed looks better than the simplest vector space model.
But it also still has some problems.
In the next lecture we're going to look at how to address these additional problems.




Session 2.2: Tf Transformation

In this lecture, we continue the discussion of vector space model.
In particular, we're going to talk about the TF transformation.
In the previous lecture, we have derived a TF idea of weighting formula using the vector space model.
And we have assumed that this model actually works pretty well for these examples as shown on this slide, except for d5, which has received a very high score.
Indeed, it has received the highest score among all these documents.
But this document is intuitive and non-relevant, so this is not desirable.
In this lecture, we're going to talk about, how we're going to use TF transformation to solve this problem.
Before we discuss the details, let's take a look at the formula for this simple TF-IDF weighting ranking function.
And see why this document has received such a high score.
So this is the formula, and if you look at the formula carefully, then you will see it involves a sum over all the matched query terms.
And inside the sum, each matched query term has a particular weight.
And this weight is TF-IDF weighting.
So it has an idea of component, where we see two variables.
One is the total number of documents in the collection, and that is M.
The other is the document of frequency.
This is the number of documents that are contained.
This word w.
The other variables involved in the formula include the count of the query term.
W in the query, and the count of the word in the document.
If you look at this document again, now it's not hard to realize that the reason why it hasn't received a high score is because it has a very high count of campaign.
So the count of campaign in this document is a 4, which is much higher than the other documents, and has contributed to the high score of this document.
So in treating the amount to lower the score for this document, we need to somehow restrict the contribution of the matching of this term in the document.
And if you think about the matching of terms in the document carefully, you actually would realize, we probably shouldn't reward multiple occurrences so generously.
And by that I mean, the first occurrence of a term says a lot about the matching of this term, because it goes from zero count to a count of one.
And that increase means a lot.
Once we see a word in the document, it's very likely that the document is talking about this word.
If we see a extra occurrence on top of the first occurrence, that is to go from one to two, then we also can say that, well the second occurrence kind of confirmed that it's not a accidental managing of the word.
Now we are more sure that this document is talking about this word.
But imagine we have seen, let's say, 50 times of the word in the document.
Now, adding one extra occurrence is not going to test more about the evidence, because we're already sure that this document is about this word.
So if you're thinking this way, it seems that we should restrict the contribution of a high count of a term, and that is the idea of TF Transformation.
So this transformation function is going to turn the real count of word into a term frequency weight for the word in the document.
So here I show in x axis that we'll count, and y axis I show the term frequency weight.
So in the previous breaking functions, we actually have imprison rate use some kind of transformation.
So for example, in the 0/1 bit vector recantation, we actually use such a transformation function, as shown here.
Basically if the count is 0, then it has 0 weight, otherwise it would have a weight of 1.
It's flat.
Now, what about using term count as TF weight?
Well, that's a linear function, so it has just exactly the same weight as the count.
Now we have just seen that this is not desirable.
So what we want is something like this.
So for example, with an algorithm function, we can't have a sublinear transformation that looks like this.
And this will control the influence of really high weight, because it's going to lower its inference.
Yet, it will retain the inference of small counts.
Or we might want to even bend the curve more by applying logarithm twice.
Now people have tried all these methods.
And they are indeed working better than the linear form of the transformation.
But so far, what works the best seems to be this special transformation, called a BM25 transformation.
BM stands for best matching.
Now in this transformation, you can see there's a parameter k here.
And this k controls the upper bound of this function.
It's easy to see this function has a upper bound, because if you look at the x divided by x + k, where k is a non-active number, then the numerator will never be able to exceed the denominator, right?
So it's upper bounded by k+1.
Now, this is also difference between this transformation function and a logarithm transformation.
Which it doesn't have upper bound.
Furthermore, one interesting property of this function is that, as we vary k, we can actually simulate different transformation functions.
Including the two extremes that are shown here.
That is, the 0/1 bit transformation and the linear transformation.
So for example, if we set k to 0, now you can see the function value will be 1.
So we precisely recover the 0/1 bit transformation.
If you set k to very large number on the other hand, it's going to look more like the linear transformation function.
So in this sense, this transformation is very flexible.
It allows us to control the shape of the transformation.
It also has a nice property of the upper bound.
And this upper bound is useful to control the inference of a particular term.
And so that we can prevent a spammer from just increasing the count of one term to spam all queries that might match this term.
In other words, this upper bound might also ensure that all terms would be counted when we aggregate the weights to compute the score.
As I said, this transformation function has worked well so far.
So to summarize this lecture, the main point is that we need to do Sublinear TF Transformation, and this is needed to capture the intuition of diminishing return from higher term counts.
It's also to avoid the dominance by one single term over all others.
This BM25 transformation that we talked about is very interesting.
It's so far one of the best-performing TF Transformation formulas.
It has upper bound, and so it's also robust and effective.
Now if we're plugging this function into our TF-IDF weighting vector space model.
Then we'd end up having the following ranking function, which has a BM25 TF component.
Now, this is already very close to a state of the odd ranking function called BM25.
And we'll discuss how we can further improve this formula in the next lecture.




Session 2.3: Doc Length Normalization

This lecture is about Document Length Normalization in the Vector Space Model.
In this lecture, we will continue the discussion of the vector space model.
In particular, we're going to discuss the issue of document length normalization.
So far in the lectures about the vector space model, we have used the various signals from the document to assess the matching of the document with a query.
In particular, we have considered the tone frequency.
The count of a tone in a document.
We have also considered it's global statistics such as, IDF, Inverse Document Frequency.
But we have not considered document lengths.
So here I show two example documents, d4 is much shorter with only 100 words.
D6 on the other hand, has a 5000 words.
If you look at the matching of these query words, we see that in d6, there are more matchings of the query words.
But one might reason that, d6 may have matched these query words in a scattered manner.
So maybe the topic of d6, is not really about the topic of the query.
So, the discussion of the campaign at the beginning of the document, may have nothing to do with the managing of presidential at the end.
In general, if you think about the long documents, they would have a higher chance for matching any query.
In fact, if you generate a long document randomly by assembling words from a distribution of words, then eventually you probably will match an inquiry.
So in this sense, we should penalize on documents because they just naturally have better chance matching to any query, and this is idea of document normalization.
We also need to be careful in avoiding to over penalize long documents.
On the one hand, we want to penalize the long document.
But on the other hand, we also don't want to over-penalize them.
Now, the reasoning is because a document that may be long because of different reasons.
In one case, the document may be long because it uses more words.
So for example, think about the vortex article on the research paper.
It would use more words than the corresponding abstract.
So, this is a case where we probably should penalize the matching of long documents such as a full paper.
When we compare the matching of words in such a long document with matching of the words in the shop abstract.
Then long papers in general, have a higher chance of matching clearer words, therefore, we should penalize them.
However, there is another case when the document is long, and that is when the document simply has more content.
Now consider another case of long document, where we simply concatenate a lot of abstracts of different papers.
In such a case, obviously, we don't want to over-penalize such a long document.
Indeed, we probably don't want to penalize such a document because it's long.
So that's why, we need to be careful about using the right degree of penalization.
A method of that has been working well, based on recent results, is called a pivoted length normalization.
And in this case, the idea is to use the average document length as a pivot, as a reference point.
That means we'll assume that for the average length documents, the score is about right so the normalizer would be 1.
But if the document is longer than the average document length, then there will be some penalization.
Whereas if it's a shorter, then there is even some reward.
So this is illustrated at using this slide, on the axis, x-axis you can see the length of document.
On the y-axis, we show the normalizer.
In this case, the Pivoted Length Normalization formula for the normalizer, is seeing to be interpolation of 1 and the normalize the document in length controlled by a parameter B here.
So you can see here, when we first divide the length of the document by the average documents, this not only gives us some sense about how this document is compared with average documents, but also gives us a benefit of not worrying about the unit of length.
We can measure the length by words or by characters.
Anyway, this normalizer has interesting property.
First we see that, if we set the parameter b to 0 then the value would be 1.
So, there's no lens normalization at all.
So, b, in this sense, controls the lens normalization.
Whereas, if we set b to a nonzero value, then the normalizer would look like this.
All right, so the value would be higher for documents that are longer than the average document lens.
Whereas, the value of the normalizer would be shorter, would be smaller for shorter documents.
So in this sense, we see there is a penalization for long documents, and there's a reward for short documents.
The degree of penalization is controlled by b, because if we set b to a larger value, then the normalizer would look like this.
There's even more penalization for long documents and more reward for the short documents.
By adjusting b, which varies from 0 to 1, we can control the degree of length normalization.
So, if we plug in this length normalization fact that into the vector space model, ranking functions is that we have already examined them.
Then we will end up having the following formulas.
And these are in fact the state of the vector space model formulas.
Let's take a look at each of them.
The first one is called a pivoted length normalization vector space model, and a reference in  duration of this model.
And here we see that, it's basically a TFI model that we have discussed, the idea of component should be very familiar to you.
There is also a query term frequency component here.
And then, in the middle, there is the normalizer tf and in this case, we see we use the double logarithm as we discussed before and this is to achieve a sublinear transformation.
But we also put a document the length normalizer in the bottom.
Right, so this would cause penalization for long document, because the larger the denominator is, then the smaller the is.
And this is of course controlled by the parameter b here.
And you can see again, if b is set to 0 then there is no length normalization.
Okay, so this is one of the two most effective at these base model formulas.
The next one called a BM25 or Okapi, is also similar in that it also has a IDF component here, and query IDF component here.
But in the middle, the normal issue's a little bit different.
As we explained, there is our copy tf transformation here, and that does sublinear transformation with the upper bound.
In this case we have put the length normalization factor here.
We're adjusting k but it achieves a similar factor, because we put a normalizer in the denominator.
Therefore, again, if a document is longer then the term weight will be smaller.
So you can see after we have gone through all the n answers that we talked about, and we have in the end reached the basically the state of god functions.
So, So far, we have talked about mainly how to place the document vector in the vector space.
And, this has played an important role in determining the effectiveness of the simple function.
But there are also other dimensions, where we did not really examine details.
For example, can we further improve the instantiation of the dimension of the Vector Space Model?
Now, we've just assumed that the bag of words representation should issue dimension as a word but obviously, we can see there are many other choices.
For example, a stemmed word, those are the words that haven't transformed into the same root form, so that computation and computing were all become the same and they can be match.
We get those stop word removal.
This is to remove some very common words that don't carry any content like the off.
We get use of phrases to define dimensions.
We can even use later in the semantical analysis, it will find some clusters of words that represent the a late in the concept as one by an engine.
We can also use smaller unit, like a character end grams those are sequences of and the characters for dimensions.
However, in practice, people have found that the bag-of-words representation with phrases is still the most effective one and it's also efficient.
So, this is still so far the most popular dimension instantiation method.
And it's used in all major search engines.
I should also mention, that sometimes we need to do language specific and domain specific tokenization.
And this is actually very important, as we might have variations of terms that might prevent us from matching them with each other, even when they mean the same thing.
In some languages like Chinese, there is also the challenge in segmenting text to obtain word band rates because it's just a sequence of characters.
A word might correspond to one character or two characters or even three characters.
So, it's easier in English when we have a space to separate the words.
In some other languages, we may need to do some Americanize processing to figure a way out of what are the boundaries for words.
There is also the possibility to improve the similarity of the function.
And so far we have used as a top product, but one can imagine there are other measures.
For example, we can measure the cosine of the angle between two vectors.
Or we can use Euclidean distance measure.
And these are all possible, but dot product seems still the best and one reason is because it's very general.
In fact that it's sufficiently general, if you consider the possibilities of doing waiting in different ways.
So, for example, cosine measure can be thought of as the thought product of two normalized factors.
That means, we first normalize each factor and then we take the thought product.
That would be critical to the cosine measure.
I just mentioned that the BM25, seems to be one of the most effective formulas.
But there has been also further developments in improving BM25.
Although, none of these words have changed the BM25 fundamental.
So in one line work, people have divide the BM25 F.
Here, F stands for field, and this is use BM25 for documents with structures.
So for example, you might consider a title field, the abstract, or body of the research article.
Or even anchor text on the web page, those are the text fields that describe links to other pages and these can all be combined with a proper way of different fields to help improve scoring for different documents.
When we use BM25 for such a document and the obvious choice is to apply BM25 for each field and then combine the scores.
Basically, the idea of BM25F is to first combine the frequency counts of terms in all the fields, and then apply BM25.
Now, this has advantage of avoiding over counting the first occurrence of the term.
Remember in the sublinear transformation of TF, the first occurrence is very important and it contributes a large weight.
And if we do that for all the fields, then the same term might have gained a lot of advantage in every field.
But when we combine these word frequencies together, we just do the transformation one time.
At that time, then the extra occurrences will not be counted as fresh first recurrences.
And this method has been working very well for scoring structure with documents.
The other line of extension is called a BM25+.
In this line, risk is to have to address the problem of over penalization of long documents by BM25.
So to address this problem, the fix is actually quite simple.
We can simply add a small constant to the TF normalization formula.
But what's interesting is that, we can analytically prove that by doing such a small modification, we will fix the problem of over penalization of law documents by the original BM25.
So the new formula called BM25+, is empirically and analytically shown to be better than BM25.
So to summarize all what we have said about vector space model, here are the major take away points.
First, in such a model, we use the similarity of relevance.
Assuming that relevance of a document with respect to a query, is basically proportional to the similarity between the query and the document.
So naturally, that implies that the query and document must have been represented in the same way.
And in this case, we will present them as vectors in high-dimensional vector space.
Where the dimensions are defined by words, or concepts, or terms, in general.
And we generally, need to use a lot of heuristics to design the ranking function.
We use some examples, which show the needs for several heuristics, including Tf weighting and transformation.
And IDF weighting, and document length normalization.
These major heuristics are the most important of heuristics, to ensure such a general ranking function to work well for all kinds of test.
And finally, BM25 and pivoted normalization seem to be the most effective formulas out of the vector space model.
Now I have to say that, I put BM25 in the category of vector space model, but in fact, the BM25 has been derived using probabilistic model.
So the reason why I've put it in the vector space model is first, the ranking function actually has a nice interpretation in the vector space model.
We can easily see, it looks very much like a vector space model, with a special waiting function.
The second reason is because the original BM25, has somewhat different form of IDF.
And that form of IDF after the  doesn't work so well as the standard IDF that you have seen here.
So as effective retrieval function, BM25 should probably use a heuristic modification of the IDF.
To make them even more look like a vector space model There are some additional readings.
The first is, a paper about the pivoted length normalization.
It's an excellent example of using empirical data analysis to suggest the need for length normalization and then further derive the length normalization formula.
The second, is the original paper where the BM25 was proposed.
The third paper, has a thorough discussion of BM25 and its extensions, particularly BM25 F.
And finally, in the last paper has a discussion of improving BM25 to correct the over penalization of long documents.




Session 2.4: Implementation Of Tr Systems

This lecture is about the implementation of text retrieval systems.
In this lecture we will discuss how we can implement a text retrieval method to build a search engine.
The main challenge is to manage a lot of text data and to enable a query to be answered very quickly and to respond to many queries.
This is a typical text retrieval system architecture.
We can see the documents are first processed by a tokenizer to get tokenized units, for example, words.
And then, these words, or tokens, will be processed by a indexer that will create a index, which is a data structure for the search engine to use to quickly answer a query.
And the query would be going through a similar processing step.
So the Tokenizer would be apprised of the query as well, so that the text can be processed in the same way.
The same units would be matched with each other.
The query's representation would then be given to the Scorer, which would use the index to quickly answer user's query by scoring the documents and then ranking them.
The results will be given to the user.
And then the user can look at the results and provided us some feedback that can be explicit judgements of both which documents are good, which documents are bad.
Or implicit feedback such as so that user didn't have to do anything extra.
End user will just look at the results, and skip some, and click on some result to view.
So these interacting signals can be used by the system to improve the ranking accuracy by assuming that viewed documents are better than the skipped ones.
So a search engine system then can be divided into three parts.
The first part is the indexer, and the second part is a Scorer that responds to the users query, and the third part is a Feedback mechanism.
Now typically, the Indexer is done in the offline manner, so you can pre-process the correct data and to build the inventory index, which we will introduce in moment.
And this data structure can then be used by the online module which is a scorer to process a user's query dynamically and quickly generate search results.
The feedback mechanism can be done online or offline, depending on the method.
The implementation of the indexer and the scorer is very standard, and this is the main topic of this lecture and the next few lectures.
The feedback mechanism, on the other hand, has variations, it depends on which method is used.
So that is usually done in algorithms specific way.
Let's first talk about the tokenizer.
Tokernization is a normalized lexical units in through the same form, so that semantically similar words can be matched with each other.
Now, in the language like English, stemming is often used and this will map all the inflectional forms of words into the same root form.
So for example, computer, computation, and computing can all be matched to the root form compute.
This way all these different forms of computing can be matched with each other.
Now normally, this is a good idea, to increase the coverage of documents that are matched up with this query.
But it's also not always beneficial, because sometimes the subtlest difference between computer and computation might still suggest the difference in the coverage of the content.
But in most cases, stemming seems to be beneficial.
When we tokenize the text in some other languages, for example Chinese, we might face some special challenges in segmenting the text to find the word boundaries.
Because it's not obvious where the boundary is as there's no space to separate them.
So here of course, we have to use some language specific processing techniques.
Once we do tokenization, then we would index the text documents and than it'll convert the documents and do some data structure that can enable faster search.
The basic idea is to precompute as much as we can basically.
So the most commonly used index is call an Inverted index.
And this has been used in many search engines to support basic search algorithms.
Sometimes the other indices, for example, document index might be needed in order to support feedback, like I said.
And these kind of techniques are not really standard in that they vary a lot according to the feedback methods.
To understand why we want to use inverted index it will be useful for you to think about how you would respond to a single term query quickly.
So if you want to use more time to think about that, pause the video.
So think about how you can pre process the text data so that you can quickly respond to a query with just one word.
Where if you have thought about that question, you might realize that where the best is to simply create the list of documents that match every term in the vocabulary.
In this way, you can basically pre-construct the answers.
So when you see a term you can simply just to fetch the random list of documents for that term and return the list to the user.
So that's the fastest way to respond to a single term here.
Now the idea of the invert index is actually, basically, like that.
We're going to do pre-constructed search an index, that will allows us to quickly find all the documents that match a particular term.
So let's take a look at this example.
We have three documents here, and these are the documents that you have seen in some previous lectures.
Suppose that we want to create an inverted index for these documents.
Then we want to maintain a dictionary, in the dictionary we will have one entry for each term and we're going to store some basic statistics about the term.
For example, the number of documents that match the term, or the total number of code or frequency of the term, which means we would kind of duplicate the occurrences of the term.
And so, for example, news, this term occur in all the three documents, so the count of documents is three.
And you might also realize we needed this count of documents, or document frequency, for computing some statistics to be used in the vector space model.
Can you think of that?
So what weighting heuristic would need this count.
Well, that's the idea, right, inverse document frequency.
So, IDF is the property of a term, and we can compute it right here.
So, with the document that count here, it's easy to compute the idea of, either at this time, or with the old index, or.
At random time when we see a query.
Now in addition to these basic statistics, we'll also store all the documents that matched the news, and these entries are stored in the file called Postings.
So in this case it matched three documents and we store information about these three documents here.
This is the document id, document 1 and the frequency is 1.
The tf is one for news, in the second document it's also 1, et cetera.
So from this list, we can get all the documents that match the term news and we can also know the frequency of news in these documents.
So, if the query has just one word, news, and we have easily look up to this table to find the entry and go quicker into the postings to fetch all the documents that matching yours.
So, let's take a look at another term.
This time, let's take a look at the word presidential.
This would occur in only one document, document 3.
So the document frequency is 1 but it occurred twice in this document.
So the frequency count is two, and the frequency count is used for some other reachable method where we might use the frequency to assess the popularity of a term in the collection.
Similarly we'll have a pointer to the postings here, and in this case, there is only one entry here because the term occurred in just one document and that's here.
The document id is 3 and it occurred twice.
So this is the basic idea of inverted index.
It's actually pretty simple, right?
With this structure we can easily fetch all the documents that match a term.
And this will be the basis for scoring documents for a query.
Now sometimes we also want to store the positions of these terms.
So in many of these cases the term occurred just once in the document.
So there's only one position for example in this case.
But in this case, the term occurred twice so there's two positions.
Now the position information is very useful for the checking whether the matching of query terms is actually within a small window of, let's say, five words or ten words.
Or, whether the matching of the two query terms is, in fact, a phrase of two words.
That this can all be checked quickly by using the position from each.
So, why is inverted index good for fast search?
Well, we just talked about the possibility of using the two answer single-term query.
And that's very easy.
What about the multiple term queries?
Well let's first look at the some special cases of the Boolean query.
A Boolean query is basically a Boolean expression like this.
So I want the value in the document to match both term A and term B.
So that's one conjunctive query.
Or I want the web documents to match term A or term B.
That's a disjunctive query.
But how can we answer such a query by using inverted index?
Well if you think a bit about it, it would be obvious because we have simply fetch all the documents that match term A and also fetch all the documents that match term B.
And then just take the intersection to answer a query like A and B.
Or to take the union to answer the query A or B.
So this is all very easy to answer.
It's going to be very quick.
Now what about the multi-term keyword query?
We talked about the vector space model for example and we will do a match such query with document and generate the score.
And the score is based on aggregated term weights.
So in this case it's not the Boolean query but the scoring can be actually done in similar way.
Basically it's similar to disjunctive Boolean query.
Basically, it's like A or B.
We take the union of all the documents that match at least one query term and then we would aggregate the term weights.
So this is a basic idea of using inverted index for scoring documents in general.
And we're going to talk about this in more detail later.
But for now, let's just look at the question why is in both index, a good idea?
Basically why is more efficient than sequentially just scanning documents.
This is the obvious approach.
You can just compute a score for each document and then you can then sort them.
And this is a straightforward method but this is going to be very slow imagine the wealth, there's a lot of documents.
If you do this then it will take a long time to answer your query.
So the question now is why would the invert index be much faster?
Well it has to do is the word distribution in text.
So, here's some common phenomena of word distribution in the text.
There are some languages independent of patterns that seem to be stable.
And these patterns are basically characterized by the following pattern.
A few words like the common words like the, a, or we occur very, very frequently in text.
So they account for a large percent of occurrences of words.
But most words would occur just rarely.
There are many words that occur just once, let's say, in a document or once in the collection.
And there are many such.
It's also true that the most frequent the words in one corpus they have to be rare in another.
That means although the general phenomenon is applicable, was observed in many cases that exact words that are common may vary from context to context.
So this phenomena is characterized by what's called a Zipf's Law.
This law says that the rank of a word multiplied by the frequency of the word is roughly constant.
So formally if we use F(w) to denote the frequency, r(w) to denote the rank of a word.
Then this is the formula.
It basically says the same thing, just mathematical term.
Where C is basically a constant and so, and there is also a parameter, alpha, that might be adjusted to better fit any empirical observations.
So if I plot the word frequencies in sorted order, then you can see this more easily.
The x axis is basically the word rank.
This is r(w) and the y axis is word frequency or F(w).
Now this curve shows that the product of the two is roughly the constant.
Now if you look at these words, we can see They can be separated into three groups.
In the middle, it's the intermediary frequency words.
These words tend to occur quite in a few documents, but they are not like those most frequent words.
And they are also not very rare.
So they tend to be often used in queries and they also tend to have high TF-IDF weights.
These intermediate frequency words.
But if you look at the left part of the curve, these are the highest frequency words.
They are covered very frequently.
They are usually words, like the, we, of Etc.
Those words are very, very frequent and they are in fact the two frequent to be discriminated, and they are generally not very useful for retrieval.
So they are often removed and this is called the stop words removal.
So you can use pretty much just the kind of words in the collection to kind of infer what words might be stop words.
Those are basically the highest frequency words.
And they also occupy a lot of space in the inverted index.
You can imagine the posting entries for such a word would be very long.
And then therefore, if you can remove such words you can save a lot of space in the inverted index.
We also show the tail part, which has a lot of rare words.
Those words don't occur very frequently, and there are many such words.
Those words are actually very useful for search also, if a user happens to be interested in such a topic.
But because they're rare, it's often true that users aren't necessarily interested in those words.
But retain them would allow us to match such a document accurately.
They generally have very high IDF.
So what kind of data structures should we use to store inverted index?
Well, it has two parts, right.
If you recall, we have a dictionary and we also have postings.
The dictionary has modest size, although for the web it's still going to be very large but compare it with postings it's more distinct.
And we also need to have fast random access to the entries because we're going to look up on the query term very quickly.
So therefore, we'd prefer to keep such a dictionary in memory if it's possible.
If the collection is not very large, this is feasible, but if the collection is very large then it's in general not possible.
If the vocabulary size is very large, obviously we can't do that.
So, in general that's how it goes.
So the data structures that we often use for storing dictionary, it would be direct access.
There are structures like hash table, or b-tree if we can't store everything in memory or use disk.
And then try to build a structure that would allow it to quickly look up entries.
For postings they are huge.
And in general, we don't have to have direct access to a specific entry.
We generally would just look up a sequence of document IDs and frequencies for all the documents that matches the query term.
So would read those entries sequentially.
And therefore because it's large and we generally have store postings on disc, they have to stay on disc and they would contain information such as document IDs, term frequency or term positions, etcetera.
Now because they are very large, compression is often desirable.
Now this is not only to save disc space, and this is of course one benefit of compression, it It's not going to occupy that much space.
But it's also to help improving speed.
Can you see why?
Well, we know that input and output would cost a lot of time.
In comparison with the time taken by CPU.
So, CPU is much faster but IO takes time and so by compressing the inverter index, opposing files will become smaller, and the entries, that we have the readings, and memory to process a query term, would be smaller, and then, so we can reduce the amount of tracking IO and that can save a lot of time.
Of course, we have to then do more processing of the data when we uncompress the data in the memory.
But as I said CPU is fast.
So over all we can still save time.
So compression here is both to save disc space and to speed up the loading of the index.




Session 2.5: System Implementation Inverted Index Construction

This lecture is about the inverted index construction.
In this lecture, we will continue the discussion of system implementation.
In particular, we're going to discuss how to construct the inverted index.
The construction of the inverted index is actually very easy if the dataset is very small.
It's very easy to construct a dictionary and then store the postings in a file.
The problem is that when our data is not able to fit to the memory then we have to use some special method to deal with it.
And unfortunately, in most retrieval applications the dataset will be large.
And they generally cannot be loaded into memory at once.
And there are many approaches to solve that problem, and sorting-based method is quite common and works in four steps as shown here.
First, you collect the local termID, documentID and frequency tuples.
Basically you will locate the terms in a small set of documents.
And then once you collect those accounts you can sort those count based on terms.
So that you will be able to local a partial inverted index and these are called rounds.
And then you write them into a temporary file on the disk and then you merge in step 3.
Do pairwise merging of these runs, until you eventually merge all the runs and generate a single inverted index.
So this is an illustration of this method.
On the left you see some documents and on the right we have a term lexicon and a document ID lexicon.
These lexicons are to map string-based representations of document IDs or terms into integer representations or map back from integers to the stream representation.
The reason why we want our interest using integers to present these IDs is because integers are often easier to handle.
For example, integers can be used as index for array, and they are also easy to compress.
So this is one reason why we tend to map these strings into integers, so that we don't have to carry these strings around.
So how does this approach work?
Well, it's very simple.
We're going to scan these documents sequentially and then parse the documents and count the frequencies of terms.
And in this stage we generally sort the frequencies by document IDs, because we process each document sequentially.
So we'll first encounter all the terms in the first document.
Therefore the document IDs are all ones in this case.
And this will be followed by document IDs two and they are natural results in this only just because we process the data in a sequential order.
At some point, we will run out of memory and that would have to write them into the disc.
Before we do that we 're going to sort them, just use whatever memory we have.
We can sort them and then this time we're going to sort based on term IDs.
Note that here, we're using the term IDs as a key to sort.
So all the entries that share the same term would be grouped together.
In this case, we can see all the IDs of documents that match term 1 would be grouped together.
And we're going to write this into that this is a temporary file.
And would that allows you to use the memory to process and makes a batch of documents.
And we're going to do that for all the documents.
So we're going to write a lot of temporary files into the disc.
And then the next stage is we do merge sort basically.
We're going to merge them and then sort them.
Eventually, we will get a single inverted index, where the entries are sorted based on term IDs.
And on the top, we're going to see these are the older entries for the documents that match term ID 1.
So this is basically, how we can do the construction of inverted index.
Even though the data cannot be all loaded into the manner.
Now, we mention earlier that because of hostings are very large, it's desirable to compress them.
So let's now take a little bit how we compressed inverted index.
Well the idea of compression in general, is for leverage skewed distributions of values.
And we generally have to use variable-length encoding, instead of the fixed-length encoding as we use by default in a program manager like C++.
And so how can we leverage the skewed distributions of values to compress these values?
Well in general, we will use few bits to encode those frequent words at the cost of using longer bit string code those rare values.
So in our case, let's think about how we can compress the TF, tone frequency.
Now, if you can picture what the inverted index look like, and you will see in post things, there are a lot of tone frequencies.
Those are the frequencies of terms in all those documents.
Now, if you think about it, what kind of values are most frequent there?
You probably will be able to guess that small numbers tend to occur far more frequently than large numbers.
Why?
Well, think about the distribution of words and this is to do the sip of slopes, and many words occur just rarely so we see a lot of small numbers.
Therefore, we can use fewer bits for the small, but highly frequent integers and that's cost of using more bits for larger integers.
This is a trade off of course.
If the values are distributed to uniform, then this won't save us any space, but because we tend to see many small values, they are very frequent.
We can save on average even though sometimes when we see a large number we have to use a lot of bits.
What about the document IDs that we also saw in postings?
Well they are not distributed in the skewed way.
So how can we deal with that?
Well it turns out that we can use a trick called a d-gap and that is to store the difference of these term IDs.
And we can imagine if a term has matched that many documents then there will be longest of document IDs.
So when we take the gap, and we take the difference between adjacent document IDs, those gaps will be small.
So again, see a lot of small numbers.
Whereas if a term occurred in only a few documents, then the gap would be large, the large numbers would not be frequent.
So this creates some skewed distribution, that would allow us to compress these values.
This is also possible because in order to uncover or uncompress these document IDs, we have to sequentially process the data.
Because we stored the difference and in order to recover the exact document ID we have to first recover the previous document ID.
And then we can add the difference to the previous document ID to restore the current document ID.
Now this was possible because we only needed to have sequential access to those document IDs.
Once we look up the term, we look up all the document IDs that match the term, then we sequentially process them.
So it's very natural, that's why this trick actually works.
And there are many different methods for encoding.
So binary code is a commonly used code in just any program language.
We use basically fixed glance in coding.
Unary code, gamma code, and delta code are all possibilities and there are many other possibilities.
So let's look at some of them in more detail.
Binary coding is really equal length coding, and that's a property for randomly distributed values.
The unary coding is a variable length in coding method.
In this case, integer this 1 will be encoded as x -1, 1 bit followed by 0.
So for example, 3 will be encoded as 2, 1s followed by 0, whereas 5 will be encoded as 4, 1s, followed by 0, etc.
So now you can imagine how many bits do we have to use for a large number like 100?
So how many bits do you have to use exactly for a number like 100?
Well exactly, we have to use 100 bits.
So it's the same number of bits as the value of this number.
So this is very inefficient if you were likely to see some large numbers.
Imagine if you occasionally see a number like 1,000, you have to use 1,000 bits.
So this only works well if you are absolutely sure that there will be no large numbers, mostly very often you see very small numbers.
Now, how do you decode this code?
Now since these are variable length encoding methods, you can't just count how many bits and then just stop.
You can't say 8-bits or 32-bits, then you will start another code.
They are variable length, so you will have to rely on some mechanism.
In this case for unary, you can see it's very easy to see the boundary.
Now you can easily see 0 would signal the end of encoding.
So you just count up how many 1s you have seen and at the end you hit 0.
You have finished one number, you will start another number.
Now we just saw that unary coding is too aggressive.
In rewarding small numbers, and if you occasionally can see a very big number, it would be a disaster.
So what about some other less aggressive method?
Well gamma coding's one of them and in this method we can use unary coding for a transform form of that.
So it's 1 plus the floor of log of x.
So the magnitude of this value is much lower than the original x.
So that's why we can afford using unary code for that.
And so first I have the unary code for coding this log of x.
And this would be followed by a uniform code or binary code.
And this basically the same uniform code, and binary code are the same.
And we're going to use this coder to code the remaining part of the value of x.
And this is basically precisely x-1 to the floor of log of x So the unary code are basically called the flow of log of x, well add one there and here.
But the remaining part we'll be using uniform code through actually code the difference between the x and this 2 to the log of x.
And it's easy to show that for this difference we only need to use up to this many bits and the floor of log of x bits.
And this is easy to understand, if the difference is too large, then we would have a higher floor of log of x.
So here are some examples for example, 3 is is encoded as 101.
The first two digits are the unary code.
So this isn't for the value 2, 10 encodes 2 in unary coding.
And so that means the floor of log of x is 1, because we won't actually use unary codes.
In code 1 plus the flow of log of x, since this is two then we know that the flow of log of x is actually 1.
So that 3 is still larger than 2 to the 1.
So the difference is 1, and the 1 is encoded here at the end.
So that's why we have 101 for 3.
Now similarly 5 is encoded as 110, followed by 01.
And in this case the unary code in code 3.
And so this is a unary code 110 and so the flow of log of x is 2.
And that means we're going to compute a difference between 5 and the 2 to the 2 and that's 1.
And so we now have again 1 at the end.
But this time we're going to use 2 bits, because with this level of flow of log of x.
We could have more numbers a 5, 6, 7 they would all share the same prefix here, 110.
So in order to differentiate them, we have to use 2 bits in the end to differentiate them.
So you can imagine 6 would be 10 here in the end instead of 01 after 10.
It's also true that the form of a gamma code is always the first odd number of bits, and in the center there is a 0.
That's the end of the unary code.
And before that or on the left side of this 0, there will be all 1s.
And on the right side of this 0, it's binary coding or uniform coding.
So how can you decode such code?
Well you again first do unary coding.
Once you hit 0, you have got the unary code and this also tell you how many bits you have to read further to decode the uniform code.
So this is how you can decode a gamma code.
There is also a delta code that's basically the same as a gamma code except that you replace the unary prefix with the gamma code.
So that's even less conservative than gamma code in terms of wording the small integers.
So that means, it's okay if you occasionally see a large number.
It's okay with delta code.
It's also fine with the gamma code, it's really a big loss for unary code.
And they are all operating of course, at different degrees of favoring short or favoring small integers.
And that also means they would be appropriate for a sorting distribution.
But none of them is perfect for all distributions.
And which method works the best would have to depend on the actual distribution in your dataset.
For inverted index compression, people have found that gamma coding seems to work well.
So how to uncompress inverted index?
I will just talk about this.
Firstly, you decode those encoded integers.
And we just I think discussed the how we decode unary coding and gamma coding.
What about the document IDs that might be compressed using d-gap?
Well, we're going to do sequential decoding so supposed the encoded I list is x1, x2, x3 etc.
We first decode x1 to obtain the first document ID, ID1.
Then we can decode x2, which is actually the difference between the second ID and the first one.
So we have to add the decoder value of x2 to ID1 to recover the value of the ID at this secondary position.
So this is where you can see the advantages of converting document IDs to integers.
And that allows us to do this kind of compression.
And we just repeat until we decode all the documents.
Every time we use the document ID in the previous position to help to recover the document ID in the next position.




Session 2.6: System Implementation Fast Search

This lecture is about how to do faster search by using invert index.
In this lecture, we're going to continue the discussion of system implementation.
In particular, we're going to talk about how to support a faster search by using invert index.
So let's think about what a general scoring function might look like.
Now of course, the vector space model is a special case of this, but we can imagine many other retrieval functions of the same form.
So the form of this function is as follows.
We see this scoring function of a document D and a query Q is defined as first a function of fa that adjustment a function that would consider two factors.
That I'll assume here at the end, f sub d of d and f sub q of q.
These are adjustment factors of a document and a query, so they are at the level of a document and the query.
So and then inside of this function, we also see there's another function called h.
So this is the main part of the scoring function and these as I just said of the scoring factors at the level of the whole document and the query.
For example, document  and this aggregate punching would then combine all these.
Now inside this h function, there are functions that would compute the weights of the contribution of a matched query term ti.
So this g, the function g gives us the weight of a matched query term ti in document d.
And this h function would then aggregate all these weights.
So for example, take a sum of all the matched query terms, but it can also be a product or it could be another way of aggregating them.
And then finally, this adjustment the functioning would then consider the document level or query level factors to further adjust this score, for example, document .
So, this general form would cover many state of  functions.
Let's look at how we can score documents with such a function using virtual index.
So, here's a general algorithm that works as follows.
First this query level and document level factors can be pre-computed in the indexing time.
Of course, for the query we have to compute it at the query time but for document, for example, document  can be pre-computed.
And then, we maintain a score accumulator for each document d to computer h.
An h is an aggregation function over all the matching query terms.
So how do we do that?
For each period term we're going to do fetch the inverted list from the invert index.
This will give us all the documents that match this query term and that includes d1, f1 and so dn fn.
So each pair is a document ID and the frequency of the term in the document.
Then for each entry d sub j and f sub j are particular match of the term in this particular document d sub j.
We'll going to compute the function g that would give us something like weight of this term, so we're computing the weight completion of matching this query term in this document.
And then, we're going to update the score accumulator for this document and this would allow us to add this to our accumulator that would incrementally compute function h.
So this is basically a general way to allow pseudo computer or functions of this form by using the inbound index.
Note that we don't have to attach any of document and that didn't match any query term.
Well, this is why it's fast, we only need to process the documents that matched at least one query term.
In the end, then we're going to adjust the score the computer this function f sub a and then we can sort.
So let's take a look at a specific example.
In this case, let's assume the scoring function is a very simple one, it just takes the sum of t f, the role of t f, the count of a term in the document.
This simplification would help shield the algorithm clearly.
It's very easy to extend the computation to include other weights like the transformation of tf, or  or IDF .
So let's take a look at specific example, where the queries information security and it show some entries of invert index on the right side.
Information occurred in four documents and their frequencies are also there, security occurred in three documents.
So let's see how the arrows works, so first we iterate overall query terms and we fetch the first query then, what is that?
That's information, right?
And imagine we have all these score accumulators who score the, scores for these documents.
We can imagine there will be other but then they will only be allocated as needed.
So before we do any waiting of terms, we don't even need a score of.
That comes actually we have these score accumulators eventually allocating.
So lets fetch the interest from the entity  for information, that the first one.
So these four accumulators obviously would be initialize as zeros.
So, the first entry is d1 and 3, 3 is occurrences of information in this document.
Since our scoring function assume that the score is just a sum of these raw counts.
We just need to add a 3 to the score accumulator to account for the increase of score due to matching this term information, a document d1.
And then, we go to the next entry, that's d2 and 4 and then we add a 4 to the score accumulator of d2.
Of course, at this point, that we will allocate the score accumulator as needed.
And so at this point, we allocated the d1 and d2, and the next one is d3, and we add one, we allocate another score  d3 and add one to it.
And then finally, the d4 gets a 5, because the term information occurred five times in this document.
Okay, so this completes the processing of all the entries in the invert index for information.
It processed all the contributions of matching information in this four documents.
So now, our error will go to the next that's security.
So, we're going to fetch all the inverted index entries for security.
So, in this case, there are three entries, and we're going to go through each of them.
The first is d2 and 3 and that means security occur three humps in d2 and what do we do?
Well, we do exactly the same, as what we did for information.
So, this time we're going to change the score  d2 since it's already allocated and what we do is we'll add 3 to the existing value which is a 4, so we now get a 7 for d2.
D2 score is increased because the match that falls the information and the security.
Go to the next entry, that's d4 and 1, so we would the score for d4 and again, we add 1 to d4 so d4 goes from 5 to 6.
Finally, we process d5 and a 3.
Since we have not yet allocated a score accumulated for d5, at this point, we're going to allocate 1 for d5, and we're going to add a 3 to it.
So, those scores, of the last rule, are the final scores for these documents.
If our scoring function is just a simple some of TF values.
Now, what if we, actually, would like to do form addition?
Well, we going to do the  at this point, for each document.
So, to summarize this, all right, so you can see, we first process the information determine query term information and we processed all the entries in what index for this term.
Then we process the security, all right, its worst think about what should be the order of processing here when we can see the query terms?
It might make a difference especially if we don't want to keep all the score accumulators.
Let's say, we only want to keep the most promising score accumulators.
What do you think would be a good order to go through?
Would you process a common term first or would you process a rare term first?
The answers is we just go to who should process the rare term first.
A rare term would match a few documents, and then the score contribution would be higher, because the ideal value would be higher.
And then, it allows us to attach the most diplomacy documents first.
So, it helps pruning some non-promising ones, if we don't need so many documents to be returned to the user.
So those are all heuristics for further improving the accuracy.
Here you can also see how we can incorporate the idea of waiting, right?
So they can  when we incorporate  when we process each query time.
When we fetch the inverted index we can fetch the document frequency and then we can compute IDF.
Or maybe perhaps the IDF value has already been precomputed when we indexed the documents.
At that time, we already computed the IDF value that we can just fetch it, so all these can be done at this time.
So that would mean when we process all the entries for information, these words would be adjusted by the same IDF, which is IDF for information.
So this is the basic idea of using inverted index for fast research and it works well for all kinds of formulas that are of the general form.
And this generally, the general form covers actually most state of art retrieval functions.
So there are some tricks to further improve the efficiency, some general techniques to encode the caching.
This is we just store some results of popular queries, so that next time when you see the same query, you simply return the stored results.
Similarly, you can also slow the list of inverted index in the memory for a popular term.
And if the query term is popular likely, you will soon need to factor the inverted index for the same term again.
So keeping it in the memory would help, and these are general techniques for improving efficiency.
We can also keep only the most promising accumulators because a user generally doesn't want to examine so many documents.
We only need to return high qualities subset of documents that likely are ranked on the top.
For that purpose, we can then prune the accumulators.
We don't have to store all the accumulators.
At some point, we just keep the highest value accumulators.
Another technique is to do parallel processing and that's needed for really process in such a large data set like the web data set.
And you scale up to the Web-scale really to have the special techniques you do parallel processing and to distribute the storage of files on machines.
So here is a list of some text retrieval toolkits, it's not a complete list.
You can find more information at this URL on the bottom.
And here, I listed your four here, Lucene's one of the most popular toolkits that can support a lot of applications and it has very nice support for applications.
You can use it to build a search engine application very quickly.
The downside is that it's not that easy to extend it, and the algorithms implemented they are also not the most advanced algorithms.
Lemur or Indri is another toolkit that does not have such a nice support web application as Lucene but it has many advanced search algorithms and it's also easy to extend.
Terrier is yet another toolkit that also has good support for application capability and some advanced algorithms.
So that's maybe in between Lemur or Lucene, or maybe rather combining the strands of both, so that's also useful tool kit.
MeTA is a toolkit that we will use for the problem assignment and this is a new toolkit that has a combination of both text retrieval algorithms and text mining algorithms.
And so talking models are implement they are a number of text analysis algorithms implemented in the toolkit as well as basic search algorithms.
So to summarize all the discussion about the System Implementation, here are the major takeaway points.
Inverted index is the primary data structure for supporting a search engine and that's the key to enable faster response to a user's query.
And the basic idea is to preprocess the data as much as we can, and we want to do compression when appropriate.
So that we can save disk space and we can speed up IO and processing of inverted index in general.
We talked about how to construct the invert index when the data can't fit into the memory.
And then we talk about faster search using that index basically, what's we exploit the invective index to accumulate a scores for documents  algorithm.
And we exploit the Zipf's law to avoid the touching many documents that don't match any query term and this algorithm can actually support a wide range of ranking algorithms.
So these basic techniques have great potential for further scaling up using distributed file system, parallel processing, and caching.
Here are two additional readings you can take a look, if you have time and you are interested in learning more about this.
The first one is a classical textbook on the efficiency o inverted index and the compression techniques.
And how to, in general feel that the efficient any inputs of the space, overhead and speed.
The second one is a newer textbook that has a nice discussion of implementing and evaluating search engines.




Session 3.1: Evaluation Of Tr Systems

This lecture is about Evaluation of Text Retrieval Systems In the previous lectures, we have talked about the a number of Text Retrieval Methods, different kinds of ranking functions.
But how do we know which one works the best?
In order to answer this question, we have to compare them and that means we have to evaluate these retrieval methods.
So this is the main topic of this lecture.
First, lets think about why do we have to do evaluation?
I already give one reason.
That is, we have to use evaluation to figure out which retrieval method works better.
Now this is very important for advancing our knowledge.
Otherwise, we wouldn't know whether a new idea works better than an old idea.
In the beginning of this course, we talked about the problem of text retrieval.
We compare it with data base retrieval.
There we mentioned that text retrieval is an empirically defined problem.
So evaluation must rely on users.
Which system works better, would have to be judged by our users.
So, this becomes a very challenging problem because how can we get users involved in the evaluation?
How can we do a fair comparison of different method?
So just go back to the reasons for evaluation.
I listed two reasons here.
The second reason, is basically what I just said, but there is also another reason which is to assess the actual utility of a Text Regional system.
Imagine you're building your own such annual applications, it would be interesting knowing how well your search engine works for your users.
So in this case, matches must reflect the utility to the actual users in real occasion.
And typically, this has to be done by using user starters and using the real search engine.
In the second case, or the second reason, the measures actually all need to collated with the utility to actually use this.
Thus, they don't have to accurately reflect the exact utility to users.
So the measure only needs to be good enough to tell which method works better.
And this is usually done through a test collection.
And this is the main idea that we'll be talking about in this course.
This has been very important for comparing different algorithms and for improving search engine system in general.
So let's talk about what to measure.
There are many aspects of searching that we can measure, we can evaluate.
And here, I listed the three major aspects.
One, is effectiveness or accuracy.
How accurate are the search results?
In this case, we're measuring a system's capability of ranking relevant documents on top of non relevant ones.
The second, is efficiency.
How quickly can you get the results?
How much computing resources are needed to answer a query?
In this case, we need to measure the space and time overhead of the system.
The third aspect is usability.
Basically the question is, how useful is a system for new user tasks.
Here, obviously, interfaces and many other things also important and would typically have to do user studies.
Now in this course, we're going to talk mostly about effectiveness and accuracy measures.
Because the efficiency and usability dimensions are not really unique to search engines.
And so they are needed for without any other software systems.
And there is also good coverage of such and other causes.
But how to evaluate search engine's quality or accuracy is something unique to text retrieval and we're going to talk a lot about this.
The main idea that people have proposed before using a test set to evaluate the text retrieval algorithm is called the Cranfield Evaluation Methodology.
This one actually was developed a long time ago, developed in 1960s.
It's a methodology for laboratory test of system components.
Its sampling methodology that has been very useful, not just for search engine evaluation.
But also for evaluating virtually all kinds of empirical tasks, and for example in natural language processing or in other fields where the problem is empirical to find, we typically would need to use such a methodology.
And today with the big data challenging with the use of machine learning everywhere.
This methodology has been very popular, but it was first developed for a search engine application in the 1960s.
So the basic idea of this approach is to build a reusable test collection and define measures.
Once such a test collection is built, it can be used again and again to test different algorithms.
And we're going to define measures that allow you to quantify performance of a system and algorithm.
So how exactly will this work?
Well we can do have a sample collection of documents and this is adjusted to simulate the real document collection in the search application.
We're going to also have a sample set of queries, or topics.
This is a little simulator that uses queries.
Then, we'll have to have those relevance judgments.
These are judgments of which documents should be returned for which queries.
Ideally, they have to be made by users who formulated the queries.
Because those are the people that know exactly what documents would be used for.
And finally, we have to have matches for quantify how well our system's result matches the ideal ranked list.
That would be constructed base on user's relevance judgements.
So this methodology is very useful for starting retrieval algorithms, because the test can be reused many times.
And it will also provide a fair comparison for all the methods.
We have the same criteria or same dataset to be used to compare different algorithms.
This allows us to compare a new algorithm with an old algorithm that was divided many years ago, by using the same standard.
So this is the illustration of this works, so as I said, we need our queries that are showing here.
We have Q1, Q2 etc.
We also need the documents and that's called the document caching and on the right side you will see we need relevance judgments.
These are basically the binary judgments of documents with respect to a query.
So for example, D1 is judged as being relevant to Q1, D2 is judged as being relevant as well, and D3 is judged as not relevant.
And the Q1 etc.
These will be created by users.
Once we have these, and we basically have a test collection.
And then if you have two systems, you want to compare them, then you can just run each system on these queries and the documents and each system would then return results.
Let's say if the queries Q1 and then we would have the results here.
Here I show R sub A as the results from system A.
So this is, remember we talked about task of computing approximation of the relevant document set.
R sub A is system A's approximation here.
And R sub B is system B's approximation of relevant documents.
Now, let's take a look at these results.
So which is better?
Now imagine if a user, which one would you like?
Now let's take a look at the both results.
And there are some differences and there are some documents that are returned by both systems.
But if you look at the results, you will feel that maybe A is better in the sense that we don't have many number element documents.
And among the three documents returned, the two of them are relevant.
So that's good, it's precise.
On the other hand one council say maybe B is better, because we've got all of them in the documents.
We've got three instead of two.
So which one is better and how do we quantify this?
Well, obviously this question highly depends on a user's task.
It depends on users as well.
You might even imagine for some users may be system A is better.
If the user is not interested in getting all the random documents.
Right, in this case the user doesn't have to read a million users will see most of the relevant documents.
On the other hand, one can also imagine the user might need to have as many random documents as possible.
For example, if you're doing a literature survey you might be in the sigma category, and you might find that system B is better.
So in the case, we will have to also define measures that will quantify them.
And we might need it to define multiple measures because users have different perspectives of looking at the results.




Session 3.2: Evaluation Of Tr Systems Basic Measures

This lecture is about the basic measures for evaluation of text retrieval systems.
In this lecture, we're going to discuss how we design basic measures to quantitatively compare two retrieval systems.
This is a slide that you have seen earlier in the lecture where we talked about the Granville evaluation methodology.
We can have a test faction that consists of queries, documents, and .
We can then run two systems on these data sets to contradict the evaluator.
Their performance.
And we raise the question, about which set of results is better.
Is system A better or is system B better?
So let's now talk about how to accurately quantify their performance.
Suppose we have a total of 10 relevant documents in the collection for this query.
Now, the relevant judgments show on the right in  obviously.
And we have only seen 3  there,  documents there.
But, we can imagine there are other Random documents in judging for this query.
So now, intuitively, we thought that system A is better because it did not have much noise.
And in particular we have seen that among the three results, two of them are relevant but in system B, we have five results and only three of them are relevant.
So intuitively it looks like system A is more accurate.
And this infusion can be captured by a matching holder position, where we simply compute to what extent all the retrieval results are relevant.
If you have 100% position, that would mean that all the retrieval documents are relevant.
So in this case system A has a position of two out of three System B has some sweet hold of 5 and this shows that system A is better frequency.
But we also talked about System B might be prefered by some other units would like to retrieve as many random documents as possible.
So in that case we'll have to compare the number of relevant documents that they retrieve and there's another method called recall.
This method uses the completeness of coverage of random documents In your retrieval result.
So we just assume that there are ten relevant documents in the collection.
And here we've got two of them, in system A.
So the recall is 2 out of 10.
Whereas System B has called a 3, so it's a 3 out of 10.
Now we can see by recall system B is better.
And these two measures turn out to be the very basic of measures for evaluating search engine.
And they are very important because they are also widely used in many other test evaluation problems.
For example, if you look at the applications of machine learning, you tend to see precision recall numbers being reported and for all kinds of tasks.
Okay so, now let's define these two measures more precisely.
And these measures are to evaluate a set of retrieved documents, so that means we are considering that approximation of the set of relevant documents.
We can distinguish 4 cases depending on the situation of the documents.
A document can be retrieved or not retrieved, right?
Because we are talking about a set of results.
A document can be also relevant or not relevant depending on whether the user thinks this is a useful document.
So we can now have counts of documents in.
Each of the four categories again have a represent the number of documents that have been retrieved and relevant.
B for documents that are not retrieved but rather etc.
No with this table then we can define precision.
As the ratio of the relevant retrieved documents A to the total of relevant retrieved documents.
So, this is just A divided by The sum of a and c.
The sum of this column.
Singularly recall is defined by dividing a by the sum of a and b.
So that's again to divide a by.
The sum of the row instead of the column.
All right, so we can see precision and recall is all focused on looking at the a, that's the number of retrieved relevant documents.
But we're going to use different denominators.
Okay, so what would be an ideal result.
Well, you can easily see being the ideal case would have precision and recall oil to be 1.0.
That means We have got 1% of all the Relevant documents in our results, and all of the results that we returned all Relevant.
At least there's no single Not Relevant document returned.
In reality, however, high recall tends to be associated with low precision.
And you can imagine why that's the case.
As you go down the to try to get as many random documents as possible, you tend to encounter a lot of documents, so the precision has to go down.
Note that this set can also be defined by a cut off.
In the rest of this, that's why although these two measures are defined for retrieve the documents, they are actually very useful for evaluating a rank list.
They are the fundamental measures in task retrieval and many other tasks.
We often are interested in The precision at ten documents for web search.
This means we look at how many documents among the top ten results are actually relevant.
Now, this is a very meaningful measure, because it tells us how many relevant documents a user can expect to see On the first page of where they typically show ten results.
So precision and recall are the basic matches and we need to use them to further evaluate a search engine, but they are the Building blocks.
We just said that there tends to be a trailoff between precision and recall, so naturally it would be interesting to combine them.
And here's one method that's often used, called F-measure And it's a  mean of precision and recall as defined on this slide.
So, you can see at first, compute the.
Inverse of R and P here, and then it would interpret the 2 by using coefficients depending on parameter beta.
And after some transformation you can easily see it would be of this form.
And in any case it just becomes an agent of precision and recall, and beta is a parameter, that's often set to 1.
It can control the emphasis on precision or recall always set beta to 1 We end up having a special case of F-Measure, often called F1.
This is a popular measure that's often used as a combined precision and recall.
And the formula looks very simple.
It's just this, here.
Now it's easy to see that if you have a Larger precision, or larger recall than f measure would be high.
But, what's interesting is that the trade off between precision and recall is captured an interesting way in f1.
So, in order to understand that, we can first look at the natural Why not just the combining and using the symbol arithmetically as efficient here?
That would be likely the most natural way of combining them So what do you think?
If you want to think more, you can pause the video.
So why is this not as good as F1?
Or what's the problem with this?
Now, if you think about the arithmetic mean, you can see this is the sum of multiple terms.
In this case, it's the sum of precision and recall.
In the case of a sum, the total value tends to be dominated by the large values.
that means if you have a very high P or very high R then you really don't care about whether the other value is low so the whole sum would be high.
Now this is not desirable because one can easily have a perfect recall.
We have perfect recall easily.
Can we imagine how?
It's probably very easy to imagine that we simply retrieve all the documents in the collection and then we have a perfect recall.
And this will give us 0.5 as the average.
But such results are clearly not very useful for the users even though the average using this formula would be relevantly high.
In contrast you can see F 1 would reward a case where precision and recall are roughly That seminar, so it would a case where you had extremely high value for one of them.
So this means f one encodes a different trade off between that.
Now this example shows actually a very important.
Methodology here.
But when you try to solve a problem you might naturally think of one solution, let's say in this it's this error mechanism.
But it's important not to settle on this source.
It's important to think whether you have other ways to combine that.
And once you think about the multiple variance It's important to analyze their difference, and then think about which one makes more sense.
In this case, if you think more carefully, you will think that F1 probably makes more sense.
Than the simple.
Although in other cases there may be different results.
But in this case the seems not reasonable.
But if you don't pay attention to these subtle differences you might just take a easy way to combine them and then go ahead with it.
And here later, you will find that, the measure doesn't seem to work well.
All right.
So this methodology is actually very important in general, in solving problems.
Try to think about the best solution.
Try to understand the problem very well, and then know why you needed this measure, and why you need to combine precision and recall.
And then use that to guide you in finding a good way to solve the problem.
To summarize, we talked about precision which addresses the question are there retrievable results all relevant?
We also talk about the Recall.
Which addresses the question, have all of the relevant documents been retrieved.
These two, are the two, basic matches in text and retrieval in.
They are used for many other tasks, as well.
We talk about F measure as a way to combine Precision Precision and recall.
We also talked about the tradeoff between precision and recall.
And this turns out to depend on the user's search tasks and we'll discuss this point more in a later lecture.




Session 3.3: Evaluation Of Tr Systems Evaluating Ranked Lists Part 1

This lecture is about, how we can evaluate a ranked list?
In this lecture, we will continue the discussion of evaluation.
In particular, we are going to look at, how we can evaluate a ranked list of results.
In the previous lecture, we talked about, precision-recall.
These are the two basic measures for, quantitatively measuring the performance of a search result.
But, as we talked about, ranking, before, we framed that the text of retrieval problem, as a ranking problem.
So, we also need to evaluate the, the quality of a ranked list.
How can we use precision-recall to evaluate, a ranked list?
Well, naturally, we have to look after the precision-recall at different, cut-offs.
Because in the end, the approximation of relevant documents, set, given by a ranked list, is determined by where the user stops browsing.
Right?
If we assume the user, securely browses, the list of results, the user would, stop at some point, and that point would determine the set.
And then, that's the most important, cut-off, that we have to consider, when we compute the precision-recall.
Without knowing where exactly user would stop, then we have to consider, all the positions where the user could stop.
So, let's look at these positions.
Look at this slide, and then, let's look at the, what if the user stops at the, the first document?
What's the precision-recall at this point?
What do you think?
Well, it's easy to see, that this document is So, the precision is one out of one.
We have, got one document, and that's relevent.
What about the recall?
Well, note that, we're assuming that, there are ten relevant documents, for this query in the collection, so, it's one out of ten.
What if the user stops at the second position?
Top two.
Well, the precision is the same, 100%, two out of two.
And, the record is two out of ten.
What if the user stops at the third position?
Well, this is interesting, because in this case, we have not got any, additional relevant document, so, the record does not change.
But the precision is lower, because we've got number  so, what's exactly the precision?
Well, it's two out of three, right?
And, recall is the same, two out of ten.
So, when would see another point, where the recall would be different?
Now, if you look down the list, well, it won't happen until, we have, seeing another relevant document.
In this case D5, at that point, the, the recall is increased through three out of ten, and, the precision is three out of five.
So, you can see, if we keep doing this, we can also get to D8.
And then, we will have a precision of four out of eight, because there are eight documents, and four of them are relevant.
And, the recall is a four out of ten.
Now, when can we get, a recall of five out of ten?
Well, in this list, we don't have it, so, we have to go down on the list.
We don't know, where it is?
But, as convenience, we often assume that, the precision is zero, at all the, the othe, the precision are zero at all the other levels of recall, that are beyond the search results.
So, of course, this is a pessimistic assumption, the actual position would be higher, but we make, make this assumption, in order to, have an easy way to, compute another measure called Average Precision, that we will discuss later.
Now, I should also say, now, here you see, we make these assumptions that are clearly not, accurate.
But, this is okay, for the purpose of comparing to, text methods.
And, this is for the relative comparison, so, it's okay, if the actual measure, or actual, actual number deviates a little bit, from the true number.
As long as the deviation, is not biased toward any particular retrieval method, we are okay.
We can still, accurately tell which method works better.
And, this is important point, to keep in mind.
When you compare different algorithms, the key's to avoid any bias toward each method.
And, as long as, you can avoid that.
It's okay, for you to do transformation of these measures anyway, so, you can preserve the order.
Okay, so, we'll just talk about, we can get a lot of precision-recall numbers at different positions.
So, now, you can imagine, we can plot a curve.
And, this just shows on the, x-axis, we show the recalls.
And, on the y-axis, we show the precision.
So, the precision line was marked as .1, .2, .3, and, 1.0.
Right?
So, this is, the different, levels of recall.
And,, the y-axis also has, different amounts, that's for precision.
So, we plot the, these, precision-recall numbers, that we have got, as points on this picture.
Now, we can further, and link these points to form a curve.
As you'll see, we assumed all the other, precision as the high-level recalls, be zero.
And, that's why, they are down here, so, they are all zero.
And this, the actual curve probably will be something like this, but, as we just discussed, it, it doesn't matter that much, for comparing two methods.
because this would be, underestimated, for all the method.
Okay, so, now that we, have this precision-recall curve, how can we compare ranked to back list?
All right, so, that means, we have to compare two PR curves.
And here, we show, two cases.
Where system A is showing red, system B is showing blue, there's crosses.
All right, so, which one is better?
I hope you can see, where system A is clearly better.
Why?
Because, for the same level of recall, see same level of recall here, and you can see, the precision point by system A is better, system B.
So, there's no question.
In here, you can imagine, what does the code look like, for ideal search system?
Well, it has to have perfect, precision at all the recall points, so, it has to be this line.
That would be the ideal system.
In general, the higher the curve is, the better, right?
The problem is that, we might see a case like this.
This actually happens often.
Like, the two curves cross each other.
Now, in this case, which one is better?
What do you think?
Now, this is a real problem, that you actually, might have face.
Suppose, you build a search engine, and you have a old algorithm, that's shown here in blue, or system B.
And, you have come up with a new idea.
And, you test it.
And, the results are shown in red, curve A.
Now, your question is, is your new method better than the old method?
Or more, practically, do you have to replace the algorithm that you're already using, your, in your search engine, with another, new algorithm?
So, should we use system, method A, to replace method B?
This is going to be a real decision, that you to have to make.
If you make the replacement, the search engine would behave like system A here, whereas, if you don't do that, it will be like a system B.
So, what do you do?
Now, if you want to spend more time to think about this, pause the video.
And, it's actually very useful to think about that.
As I said, it's a real decision that you have to make, if you are building your own search engine, or if you're working, for a company that, cares about the search.
Now, if you have thought about this for a moment, you might realize that, well, in this case, it's hard to say.
Now, some users might like a system A, some users might like, like system B.
So, what's the difference here?
Well, the difference is just that, you know, in the, low level of recall, in this region, system B is better.
There's a higher precision.
But in high recall region, system A is better.
Now, so, that also means, it depends on whether the user cares about the high recall, or low recall, but high precision.
You can imagine, if someone is just going to check out, what's happening today, and want to find out something relevant in the news.
Well, which one is better?
What do you think?
In this case, clearly, system B is better, because the user is unlikely examining a lot of results.
The user doesn't care about high recall.
On the other hand, if you think about a case, where a user is doing you are, starting a problem.
You want to find, whether your idea ha, has been started before.
In that case, you emphasize high recall.
So, you want to see, as many relevant documents as possible.
Therefore, you might, favor, system A.
So, that means, which one is better?
That actually depends on users, and more precisely, users task.
So, this means, you may not necessarily be able to come up with one number, that would accurately depict the performance.
You have to look at the overall picture.
Yet, as I said, when you have a practical decision to make, whether you replace ours with another, then you may have to actually come up with a single number, to quantify each, method.
Or, when we compare many different methods in research, ideally, we have one number to compare, them with, so, that we can easily make a lot of comparisons.
So, for all these reasons, it is desirable to have one, single number to match it up.
So, how do we do that?
And, that, needs a number to summarize the range.
So, here again it's the precision-recall curve, right?
And, one way to summarize this whole ranked, list, for this whole curve, is look at the area underneath the curve.
Right?
So, this is one way to measure that.
There are other ways to measure that, but, it just turns out that,, this particular way of matching it has been very, popular, and has been used, since a long time ago for text And, this is, basically, in this way, and it's called the average precision.
Basically, we're going to take a, a look at the, every different, recall point.
And then, look out for the precision.
So, we know, you know, this is one precision.
And, this is another, with, different recall.
Now, this, we don't count to this one, because the recall level is the same, and we're going to, look at the, this number, and that's precision at a different recall level et cetera.
So, we have all these, you know, added up.
These are the precisions at the different points, corresponding to retrieving the first relevant document, the second, and then, the third, that follows, et cetera.
Now, we missed the many relevant documents, so, in all of those cases, we just, assume, that they have zero precisions.
And then, finally, we take the average.
So, we divide it by ten, and which is the total number of relevant documents in the collection.
Note that here, we're not dividing this sum by four.
Which is a number retrieved relevant documents.
Now, imagine, if I divide by four, what would happen?
Now, think about this, for a moment.
It's a common mistake that people, sometimes, overlook.
Right, so, if we, we divide this by four, it's actually not very good.
In fact, that you are favoring a system, that would retrieve very few random documents, as in that case, the denominator would be very small.
So, this would be, not a good matching.
So, note that this denomina, denominator is ten, the total number of relevant documents.
And, this will basically ,compute the area, and the needs occur.
And, this is the standard method, used for evaluating a ranked list.
Note that, it actually combines recall and, precision.
But first, you know, we have precision numbers here, but secondly, we also consider recall, because if missed many, there would be many zeros here.
All right, so, it combines precision and recall.
And furthermore, you can see this measure is sensitive to a small change of a position of a relevant document.
Let's say, if I move this relevant document up a little bit, now, it would increase this means, this average precision.
Whereas, if I move any relevant document, down, let's say, I move this relevant document down, then it would decrease, uh,the average precision.
So, this is a very good, because it's a very sensitive to the ranking of every relevant document.
It can tell, small differences between two ranked lists.
And, that is what we want, sometimes one algorithm only works slightly better than another.
And, we want to see this difference.
In contrast, if we look at the precision at the ten documents.
If we look at this, this whole set, well, what, what's the precision, what do you think?
Well, it's easy to see, that's a four out of ten, right?
So, that precision is very meaningful, because it tells us, what user would see?
So, that's pretty useful, right?
So, it's a meaningful measure, from a users perspective.
But, if we use this measure to compare systems, it wouldn't be good, because it wouldn't be sensitive to where these four relevant documents are ranked.
If I move them around the precision at ten, still, the same.
Right.
So, this is not a good measure for comparing different algorithms.
In contrast, the average precision is a much better measure.
It can tell the difference of, different, a difference in ranked list in, subtle ways.




Session 3.4: Evaluation Of Tr Systems Evaluating Ranked Lists Part 2

So average precision is computer for just one.
one query.
But we generally experiment with many different queries and this is to avoid the variance across queries.
Depending on the queries you use you might make different conclusions.
Right, so it's better then using more queries.
If you use more queries then, you will also have to take the average of the average precision over all these queries.
So how can we do that?
Well, you can naturally.
Think of just doing arithmetic mean as we always tend to, to think in, in this way.
So, this would give us what's called a "Mean Average Position", or MAP.
In this case, we take arithmetic mean of all the average precisions over several queries or topics.
But as I just mentioned in another lecture, is this good?
We call that.
We talked about the different ways of combining precision and recall.
And we conclude that the arithmetic mean is not as good as the MAP measure.
But here it's the same.
We can also think about the alternative ways of aggregating the numbers.
Don't just automatically assume that, though.
Let's just also take the arithmetic mean of the average position over these queries.
Let's think about what's the best way of aggregating them.
If you think about the different ways, naturally you will, probably be able to think about another way, which is geometric mean.
And we call this kind of average a gMAP.
This is another way.
So now, once you think about the two different ways.
Of doing the same thing.
The natural question to ask is, which one is better?
So.
So, do you use MAP or gMAP?
Again, that's important question.
Imagine you are again testing a new algorithm in, by comparing the ways your old algorithms made the search engine.
Now you tested multiple topics.
Now you've got the average precision for these topics.
Now you are thinking of looking at the overall performance.
You have to take the average.
But which, which strategy would you use?
Now first, you should also think about the question, well did it make a difference?
Can you think of scenarios where using one of them would make a difference?
That is they would give different rankings of those methods.
And that also means depending on the way you average or detect the.
Average of these average positions.
You will get different conclusions.
This makes the question becoming even more important.
Right?
So, which one would you use?
Well again, if you look at the difference between these.
Different ways of aggregating the average position.
You'll realize in arithmetic mean, the sum is dominating by large values.
So what does large value here mean?
It means the query is relatively easy.
You can have a high pres, average position.
Whereas gMAP tends to be affected more by low values.
And those are the queries that don't have good performance.
The average precision is low.
So if you think about the, improving the search engine for those difficult queries, then gMAP would be preferred, right?
On the other hand, if you just want to.
Have improved a lot.
Over all the kinds of queries or particular popular queries that might be easy and you want to make the perfect and maybe MAP would be then preferred.
So again, the answer depends on your users, your users tasks and their pref, their preferences.
So the point that here is to think about the multiple ways to solve the same problem, and then compare them, and think carefully about the differences.
And which one makes more sense.
Often, when one of them might make sense in one situation and another might make more sense in a different situation.
So it's important to pick out under what situations one is preferred.
As a special case of the mean average position, we can also think about the case where there was precisely one rank in the document.
And this happens often, for example, in what's called a known item search.
Where you know a target page, let's say you have to find Amazon, homepage.
You have one relevant document there, and you hope to find it.
That's call a "known item search".
In that case, there's precisely one relevant document.
Or in another application, like a question and answering, maybe there's only one answer.
Are there.
So if you rank the answers, then your goal is to rank that one particular answer on top, right?
So in this case, you can easily verify the average position, will basically boil down to reciprocal rank.
That is, 1 over r where r is the rank position of that single relevant document.
So if that document is ranked on the very top or is 1, and then it's 1 for reciprocal rank.
If it's ranked at the, the second, then it's 1 over 2.
Et cetera.
And then we can also take a, a average of all these average precision or reciprocal rank over a set of topics, and that would give us something called a mean reciprocal rank.
It's a very popular measure.
For no item search or, you know, an problem where you have just one relevant item.
Now again here, you can see this r actually is meaningful here.
And this r is basically indicating how much effort a user would have to make in order to find that relevant document.
If it's ranked on the top it's low effort that you have to make, or little effort.
But if it's ranked at 100 then you actually have to, read presumably 100 documents in order to find it.
So, in this sense r is also a meaningful measure and the reciprocal rank will take the reciprocal of r, instead of using r directly.
So my natural question here is why not simply using r?
I imagine if you were to design a ratio to, measure the performance of a random system, when there is only one relevant item.
You might have thought about using r directly as the measure.
After all, that measures the user's effort, right?
But, think about if you take a average of this over a large number of topics.
Again it would make a difference.
Right, for one single topic, using r or using 1 over r wouldn't make any difference.
It's the same.
Larger r with corresponds to a small 1 over r, right?
But the difference would only show when, show up when you have many topics.
So again, think about the average of Mean Reciprocal Rank versus average of just r.
What's the difference?
Do you see any difference?
And would, would this difference change the oath of systems.
In our conclusion.
And this, it turns out that, there is actually a big difference, and if you think about it, if you want to think about it and then, yourself, then pause the video.
Basically, the difference is, if you take some of our directory, then.
Again it will be dominated by large values of r.
So what are those values?
Those are basically large values that indicate that lower ranked results.
That means the relevant items rank very low down on the list.
And the sum that's also the average that would then be dominated by.
Where those relevant documents are ranked in, in ,in, in the lower portion of the ranked.
But from a users perspective we care more about the highly ranked documents.
So by taking this transformation by using reciprocal rank.
Here we emphasize more on the difference on the top.
You know, think about the difference between 1 and the 2, it would make a big difference, in 1 over r, but think about the 100, and 1, and where and when won't make much difference if you use this.
But if you use this there will be a big difference in 100 and let's say 1,000, right.
So this is not the desirable.
On the other hand, a 1 and 2 won't make much difference.
So this is yet another case where there may be multiple choices of doing the same thing and then you need to figure out which one makes more sense.
So to summarize, we showed that the precision-recall curve.
Can characterize the overall accuracy of a ranked list.
And we emphasized that the actual utility of a ranked list depends on how many top ranked results a user would actually examine.
Some users will examine more.
Than others.
An average person uses a standard measure for comparing two ranking methods.
It combines precision and recall and it's sensitive to the rank of every random document.




Session 3.5: Evaluation Of Tr Systems Multi Level Judgements

This lecture is about how to evaluate the text retrieval system when we have multiple levels of judgements.
In this lecture, we will continue the discussion of evaluation.
We're going to look at how to evaluate a text retrieval system, when we have multiple levels of judgements.
So far we have talked about the binary judgements, that means a document is judged as being relevant or not relevant.
But earlier, we also talk about the relevance as a medal of degrees.
So we often can distinguish very high relevant documents, those are very useful documents, from moderately relevant documents.
They are okay, they are useful perhaps.
And further from now, we're adding the documents, those are not useful.
So imagine you can have ratings for these pages.
Then, you would have multiple levels of ratings.
For example, here I show example of three levels, 3 for relevant, sorry 3 for very relevant, 2 for marginally relevant, and 1 for non-relevant.
Now, how do we evaluate the search engine system using these judgements?
Obvious that the map doesn't work, average of precision doesn't work, precision, and recall doesn't work, because they rely on binary judgements.
So let's look at some top ranked results when using these judgements.
Imagine the user would be mostly care about the top ten results here.
And we marked the rating levels, or relevance levels, for these documents as shown here, 3, 2, 1, 1, 3, etcetera.
And we call these gain.
And the reason why we call it the gain is because the measure that we are infusing is called the NDCG normalized or accumulated gain.
So this gain, basically, can measure how much a gain of random information a user can obtain by looking at each document, right?
So looking at the first document, the user can gain 3 points.
Looking at the non-relevant document user would only gain 1 point.
Looking at the moderator or marginally relevant, document the user would get 2 points, etcetera.
So, this gain to each of the measures is a utility of the document from a user's perspective.
Of course, if we assume the user stops at the 10 documents and we're looking at the cutoff at 10, we can look at the total gain of the user.
And what's that?
Well, that's simply the sum of these, and we call it the Cumulative Gain.
So if the user stops after the position 1, that's just a 3.
If the user looks at another document, that's a 3+2.
If the user looks at the more documents, then the cumulative gain is more.
Of course this is at the cost of spending more time to examine the list.
So cumulative gain gives us some idea about how much total gain the user would have if the user examines all these documents.
Now, in NDCG, we also have another letter here, D, discounted cumulative gain.
So, why do we want to do discounting?
Well, if you look at this cumulative gain, there is one deficiency, which is it did not consider the rank position of these documents.
So for example, looking at this sum here, and we only know there is 1 highly relevant document, 1 marginally relevant document, 2 non-relevant documents.
We don't really care where they are ranked.
Ideally, we want these two to be ranked on the top which is the case here.
But how can we capture that intuition?
Well we have to say, well this is 3 here is not as good as this 3 on the top.
And that means the contribution of the gain from different positions has to be weighted by their position.
And this is the idea of discounting, basically.
So we're going to to say, well, the first one does not need to be discounted because the user can be assumed that will always see this document.
But the second one, this one will be discounted a little bit because there's a small possibility that the user wouldn't notice it.
So we divide this gain by a weight based on the position.
So log of 2, 2 is the rank position of this document.
And when we go to the third position, we discounted even more, because the normalizer is log of 3, and so on and so forth.
So when we take such a sum that a lower ranked document would not contribute that much as a highly ranked document.
So that means if you, for example, switch the position of this, let's say this position, and this one, and then you would get more discount if you put, for example very relevant document here as opposed to here.
Imagine if you put the 3 here, then it would have to be discounted.
So it's not as good as if you we would put the 3 here.
So this is the idea of discounting.
Okay, so now at this point that we have got a discounted cumulative gain for measuring the utility of this ranked list with multiple levels of judgements.
So are we happy with this?
Well, we can use this to rank systems.
Now, we still need to do a little bit more in order to make this measure comparable across different topics.
And this is the last step, and by the way, here we just show the DCG at 10, so this is the total sum of DCG, all these 10 documents.
So the last step is called N, normalization.
And if we do that, then we'll get the normalized DCG.
So how do we do that?
Well, the idea here is we're going to normalize DCG by the ideal DCG at the same cutoff.
What is the ideal DCG?
Well, this is the DCG of an ideal ranking.
So imagine if we have 9 documents in the whole collection rated 3 here.
And that means in total we have 9 documents rated 3.
Then our ideal rank lister would have put all these 9 documents on the very top.
So all these would have to be 3 and then this would be followed by a 2 here.
Because that's the best we could do after we have run out of the 3.
But all these positions would be 3.
Right?
So this would our ideal ranked list.
And then we had computed the DCG for this ideal rank list.
So this would be given by this formula that you see here.
And so this ideal DCG would then be used as the normalizer DCG.
So here.
And this idea of DCG would be used as a normalizer.
So you can imagine now, normalization essentially is to compare the actual DCG with the best DCG you can possibly get for this topic.
Now why do we want to do this?
Well, by doing this we'll map the DCG values into a range of 0 through 1.
So the best value, or the highest value, for every query would be 1.
That's when your rank list is, in fact, the ideal list but otherwise, in general, you will be lower than one.
Now, what if we don't do that?
Well, you can see, this transformation, or this normalization, doesn't really affect the relative comparison of systems for just one topic, because this ideal DCG is the same for all the systems, so the ranking of systems based on only DCG would be exactly the same as if you rank them based on the normalized DCG.
The difference however is when we have multiple topics.
Because if we don't do normalization, different topics will have different scales of DCG.
For a topic like this one, we have 9 highly relevant documents, the DCG can get really high, but imagine in another case, there are only two very relevant documents in total in the whole collection.
Then the highest DCG that any system could achieve for such a topic would not be very high.
So again, we face the problem of different scales of DCG values.
When we take an average, we don't want the average to be dominated by those high values.
Those are, again, easy queries.
So, by doing the normalization, we can have avoided the problem, making all the queries contribute to equal to the average.
So, this is a idea of NDCG, it's used for measuring a rank list based on multiple level of relevance judgements.
In a more general way this is basically a measure that can be applied to any ranked task with multiple level of judgements.
And the scale of the judgements can be multiple, can be more than binary not only more than binary they can be much multiple levels like 1, 0, 5 or even more depending on your application.
And the main idea of this measure, just to summarize, is to measure the total utility of the top k documents.
So you always choose a cutoff and then you measure the total utility.
And it would discount the contribution from a lowly ranked document.
And then finally, it would do normalization to ensure comparability across queries.




Session 3.6: Evaluation Of Tr Systems Practical Issues

This lecture is about some practical issues that you would have to address in evaluation of text retrieval systems.
In this lecture, we will continue the discussion of evaluation.
We'll cover some practical issues that you have to solve in actual evaluation of text retrieval systems.
So, in order to create the test collection, we have to create a set of queries.
A set of documents and a set of relevance judgments.
It turns out that each is actually challenging to create.
First, the documents and queries must be representative.
They must represent the real queries and real documents that the users handle.
And we also have to use many queries and many documents in order to avoid a bias of conclusions.
For the matching of relevant documents with the queries.
We also need to ensure that there exists a lot of relevant documents for each query.
If a query has only one, that's a relevant option we can actually then.
It's not very informative to compare different methods using such a query because there's not that much room for us to see difference.
So ideally, there should be more relevant documents in the clatch but yet the queries also should represent the real queries that we care about.
In terms of relevance judgments, the challenge is to ensure complete judgments of all the documents for all the queries.
Yet, minimizing human and fault, because we have to use human labor to label these documents.
It's very labor intensive.
And as a result, it's impossible to actually label all the documents for all the queries, especially considering a giant data set like the web.
So this is actually a major challenge, it's a very difficult challenge.
For measures, it's also challenging, because we want measures that would accurately reflect the perceived utility of users.
We have to consider carefully what the users care about.
And then design measures to measure that.
If your measure is not measuring the right thing, then your conclusion would be misled.
So it's very important.
So we're going to talk about a couple of issues here.
One is the statistical significance test.
And this also is a reason why we have to use a lot of queries.
And the question here is how sure can you be that observe the difference doesn't simply result from the particular queries you choose.
So here are some sample results of average position for System A and System B into different experiments.
And you can see in the bottom, we have mean average of position.
So the mean, if you look at the mean average of position, the mean average of positions are exactly the same in both experiments, right?
So you can see this is 0.20, this is 0.40 for System B.
And again here it's also 0.20 and 0.40, so they are identical.
Yet, if you look at these exact average positions for different queries.
If you look at these numbers in detail, you would realize that in one case, you would feel that you can trust the conclusion here given by the average.
In the another case, in the other case, you will feel that, well, I'm not sure.
So, why don't you take a look at all these numbers for a moment, pause the media.
So, if you look at the average, the mean average of position, we can easily, say that well, System B is better, right?
So, after all it's 0.40 and this is twice as much as 0.20, so that's a better performance.
But if you look at these two experiments, look at the detailed results.
You will see that, we've been more confident to say that, in the case one, in experiment one.
In this case.
Because these numbers seem to be consistently better for System B.
Whereas in Experiment 2, we're not sure because looking at some results like this, after System A is better and this is another case System A is better.
But yet if we look at only average, System B is better.
So, what do you think?
How reliable is our conclusion, if we only look at the average?
Now in this case, intuitively, we feel Experiment 1 is more reliable.
But how can we quantitate the answer to this question?
And this is why we need to do statistical significance test.
So, the idea of the statistical significance test is basically to assess the variants across these different queries.
If there is a big variance, that means the results could fluctuate a lot according to different queries.
Then we should believe that, unless you have used a lot of queries, the results might change if we use another set of queries.
Right, so this is then not so if you have c high variance then it's not very reliable.
So let's look at these results again in the second case.
So, here we show two different ways to compare them.
One is a sign test where we just look at the sign.
If System B is better than System A, we have a plus sign.
When System A is better we have a minus sign, etc.
Using this case, if you see this, well, there are seven cases.
We actually have four cases where System B is better.
But three cases of System A is better, intuitively, this is almost like a random results, right?
So if you just take a random sample of you flip seven coins and if you use plus to denote the head and minus to denote the tail and that could easily be the results of just randomly flipping these seven coins.
So, the fact that the average is larger doesn't tell us anything.
We can't reliably conclude that.
And this can be quantitatively measured by a p value.
And that basically means the probability that this result is in fact from a random fluctuation.
In this case, probability is 1.0.
It means it surely is a random fluctuation.
Now in Willcoxan test, it's a non-parametric test, and we would be not only looking at the signs, we'll be also looking at the magnitude of the difference.
But we can draw a similar conclusion, where you say it's very likely to be from random.
To illustrate this, let's think about that such a distribution.
And this is called a now distribution.
We assume that the mean is zero here.
Lets say we started with assumption that there's no difference between the two systems.
But we assume that because of random fluctuations depending on the queries, we might observe a difference.
So the actual difference might be on the left side here or on the right side here, right?
So, and this curve kind of shows the probability that we will actually observe values that are deviating from zero here.
Now, so if we look at this picture then, we see that if a difference is observed here, then the chance is very high that this is in fact a random observation, right?
We can define a region of likely observation because of random fluctuation and this is that 95% of all the outcomes.
And in this then the observed may still be from random fluctuation.
But if you observe a value in this region or a difference on this side, then the difference is unlikely from random fluctuation.
All right, so there's a very small probability that you are observe such a difference just because of random fluctuation.
So in that case, we can then conclude the difference must be real.
So System B is indeed better.
So this is the idea of Statical Significance Test.
The takeaway message here is that you have to use many queries to avoid jumping into a conclusion.
As in this case, to say System B is better.
There are many different ways of doing this Statistical Significance Test.
So now, let's talk about the other problem of making judgments and, as we said earlier, it's very hard to judge all the documents completely unless it's a very small data set.
So the question is, if we can afford judging all the documents in the collection, which is subset should we judge?
And the solution here is Pooling.
And this is a strategy that has been used in many cases to solve this problem.
So the idea of Pooling is the following.
We would first choose a diverse set of ranking methods.
These are Text Retrieval systems.
And we hope these methods can help us nominate like the relevant documents.
So the goal is to pick out the relevant documents.
We want to make judgements on relevant documents because those are the most useful documents from users perspectives.
So then we're going to have each to return top-K documents.
The K can vary from systems.
But the point is to ask them to suggest the most likely relevant documents.
And then we simply combine all these top-K sets to form a pool of documents for human assessors.
To judge, so imagine you have many systems each were ten k documents.
We take the top-K documents, and we form a union.
Now, of course, there are many documents that are duplicated because many systems might have retrieved the same random documents.
So there will be some duplicate documents.
And there are also unique documents that are only returned by one system.
So the idea of having diverse set of ranking methods is to ensure the pool is broad.
And can include as many possible relevant documents as possible.
And then, the users would, human assessors would make complete the judgments on this data set, this pool.
And the other unjudged the documents are usually just assumed to be non relevant.
Now if the pool is large enough, this assumption is okay.
But if the pool is not very large, this actually has to be reconsidered.
And we might use other strategies to deal with them and there are indeed other methods to handle such cases.
And such a strategy is generally okay for comparing systems that contribute to the pool.
That means if you participate in contributing to the pool, then it's unlikely that it would penalize your system because the problematic documents have all been judged.
However, this is problematic for evaluating a new system that may have not contributed to the pool.
In this case, a new system might be penalized because it might have nominated some read only documents that have not been judged.
So those documents might be assumed to be non relevant.
That's unfair.
So to summarize the whole part of textual evaluation, it's extremely important.
Because the problem is the empirically defined problem, if we don't rely on users, there's no way to tell whether one method works better.
If we have in the property experiment design, we might misguide our research or applications.
And we might just draw wrong conclusions.
And we have seen this is in some of our discussions.
So make sure to get it right for your research or application.
The main methodology is the Cranfield evaluation methodology.
And they are the main paradigm used in all kinds of empirical evaluation tasks, not just a search engine variation.
Map and nDCG are the two main measures that you should definitely know about and they are appropriate for comparing ranking algorithms.
You will see them often in research papers.
Precision at 10 documents is easier to interpret from user's perspective.
So that's also often useful.
What's not covered is some other evaluation strategy like A-B Test.
Where the system would mix two, the results of two methods, randomly.
And then would show the mixed results to users.
Of course, the users don't see which result, from which method.
The users would judge those results or click on those documents in a search engine application.
In this case then, the search engine can check or click the documents and see if one method has contributed more through the click the documents.
If the user tends to click on one, the results from one method, then it suggests that message may be better.
So this is what leverages the real users of a search engine to do evaluation.
It's called A-B Test and it's a strategy that is often used by the modern search engines or commercial search engines.
Another way to evaluate IR or textual retrieval is user studies and we haven't covered that.
I've put some references here that you can look at if you want to know more about that.
So, there are three additional readings here.
These are three mini books about evaluation and they are all excellent in covering a broad review of Information Retrieval Evaluation.
And it covers some of the things that we discussed, but they also have a lot of others to offer.




Session 4.1: Probabilistic Retrieval Model Basic Idea

This lecture is about the Probabilistic Retrieval Model.
In this lecture, we're going to continue the discussion of the Text Retrieval Methods.
We're going to look at another kind of very different way to design ranking functions than the Vector Space Model that we discussed before.
In probabilistic models, we define the ranking function, based on the probability that this document is relevant to this query.
In other words, we introduce a binary random variable here.
This is the variable R here.
And we also assume that the query and the documents are all observations from random variables.
Note that in the vector-based models, we assume they are vectors, but here we assume they are the data observed from random variables.
And so, the problem of retrieval becomes to estimate the probability of relevance.
In this category of models, there are different variants.
The classic probabilistic model has led to the BM25 retrieval function, which we discussed in in the vectors-based model because its a form is actually similar to a backwards space model.
In this lecture, we will discuss another sub class in this P class called a language modeling approaches to retrieval.
In particular, we're going to discuss the query likelihood retrieval model, which is one of the most effective models in probabilistic models.
There was also another line called the divergence from randomness model which has led to the PL2 function, it's also one of the most effective state of the art retrieval functions.
In query likelihood, our assumption is that this probability of relevance can be approximated by the probability of query given a document and relevance.
So intuitively, this probability just captures the following probability.
And that is if a user likes document d, how likely would the user enter query q ,in order to retrieve document d?
So we assume that the user likes d, because we have a relevance value here.
And then we ask the question about how likely we'll see this particular query from this user?
So this is the basic idea.
Now, to understand this idea, let's take a look at the general idea or the basic idea of Probabilistic Retrieval Models.
So here, I listed some imagined relevance status values or relevance judgments of queries and documents.
For example, in this line, it shows that q1 is a query that the user typed in.
And d1 is a document that the user has seen.
And 1 means the user thinks d1 is relevant to q1.
So this R here can be also approximated by the click-through data that a search engine can collect by watching how you interacted with the search results.
So in this case, let's say the user clicked on this document.
So there's a 1 here.
Similarly, the user clicked on d2 also, so there is a 1 here.
In other words, d2 is assumed to be relevant to q1.
On the other hand, d3 is non-relevant, there's a 0 here.
And d4 is non-relevant and then d5 is again, relevant, and so on and so forth.
And this part, maybe, data collected from a different user.
So this user typed in q1 and then found that the d1 is actually not useful, so d1 is actually non-relevant.
In contrast, here we see it's relevant.
Or this could be the same query typed in by the same user at different times.
But d2 is also relevant, etc.
And then here, we can see more data about other queries.
Now, we can imagine we have a lot of such data.
Now we can ask the question, how can we then estimate the probability of relevance?
So how can we compute this probability of relevance?
Well, intuitively that just means if we look at all the entries where we see this particular d and this particular q, how likely we'll see a one on this other column.
So basically that just means that we can just collect the counts.
We can first count how many times we have seen q and d as a pair in this table and then count how many times we actually have also seen 1 in the third column.
And then, we just compute the ratio.
So let's take a look at some specific examples.
Suppose we are trying to compute this probability for d1, d2 and d3 for q1.
What is the estimated probability?
Now, think about that.
You can pause the video if needed.
Try to take a look at the table.
And try to give your estimate of the probability.
Have you seen that, if we are interested in q1 and d1, we'll be looking at these two pairs?
And in both cases, well, actually, in one of the cases, the user has said this is 1, this is relevant.
So R = 1 in only one of the two cases.
In the other case, it's 0.
So that's one out of two.
What about the d1 and the d2?
Well, they are here, d1 and d2, d1 and d2, in both cases, in this case, R = 1.
So it's a two out of two and so on and so forth.
So you can see with this approach, we can actually score these documents for the query, right?
We now have a score for d1, d2 and d3 for this query.
And we can simply rank them based on these probabilities and so that's the basic idea probabilistic retrieval model.
And you can see it makes a lot of sense, in this case, it's going to rank d2 above all the other documents.
Because in all the cases, when you have c and q1 and d2, R = 1.
The user clicked on this document.
So this also should show that with a lot of click-through data, a search engine can learn a lot from the data to improve their search engine.
This is a simple example that shows that with even with small amount of entries here we can already estimate some probabilities.
These probabilities would give us some sense about which document might be more relevant or more useful to a user for typing this query.
Now, of course, the problems that we don't observe all the queries and all the documents and all the relevance values, right?
There would be a lot of unseen documents, in general, we have only collected the data from the documents that we have shown to the users.
And there are even more unseen queries because you cannot predict what queries will be typed in by users.
So obviously, this approach won't work if we apply it to unseen queries or unseen documents.
Nevertheless, this shows the basic idea of probabilistic retrieval model and it makes sense intuitively.
So what do we do in such a case when we have a lot of unseen documents and unseen queries?
Well, the solutions that we have to approximate in some way.
So in this particular case called a query likelihood retrieval model, we just approximate this by another conditional probability.
p(q given d, R=1).
So in the condition part, we assume that the user likes the document because we have seen that the user clicked on this document.
And this part shows that we're interested in how likely the user would actually enter this query.
How likely we will see this query in the same row.
So note that here, we have made an interesting assumption here.
Basically, we're going to do, assume that whether the user types in this query has something to do with whether user likes the document.
In other words, we actually make the following assumption.
And that is a user formulates a query based on an imaginary relevant document.
Where if you just look at this as conditional probability, it's not obvious we are making this assumption.
So what I really meant is that to use this new conditional probability to help us score, then this new conditional probability will have to somehow be able to estimate this conditional probability without relying on this big table.
Otherwise we would be having similar problems as before, and by making this assumption, we have some way to bypass this big table, and try to just model how the user formulates the query, okay?
So this is how you can simplify the general model so that we can derive a specific relevant function later.
So let's look at how this model work for our example.
And basically, what we are going to do in this case is to ask the following question.
Which of these documents is most likely the imaginary relevant document in the user's mind when the user formulates this query?
So we ask this question and we quantify the probability and this probability is a conditional probability of observing this query if a particular document is in fact the imaginary relevant document in the user's mind.
Here you can see we've computed all these query likelihood probabilities.
The likelihood of queries given each document.
Once we have these values, we can then rank these documents based on these values.
So to summarize, the general idea of modern relevance in the proper risk model is to assume the we introduce a binary random variable R, here.
And then, let the scoring function be defined based on this conditional probability.
We also talked about approximating this by using the query likelihood.
And in this case we have a ranking function that's basically based on the probability of a query given the document.
And this probability should be interpreted as the probability that a user who likes document d, would pose query q.
Now, the question of course is, how do we compute this conditional probability?
At this in general has to do with how you compute the probability of text, because q is a text.
And this has to do with a model called a Language Model.
And these kind of models are proposed to model text.
So more specifically, we will be very interested in the following conditional probability as is shown in this here.
If the user liked this document, how likely the user would pose this query.
And in the next lecture we're going to do, giving introduction to language models that we can see how we can model text that was a probable risk model, in general.




Session 4.2: Statistical Language Model

This lecture is about the statistical language model.
In this lecture, we're going to give an introduction to statistical language model.
This has to do with how do you model text data with probabilistic models.
So it's related to how we model query based on a document.
We're going to talk about what is a language model.
And then we're going to talk about the simplest language model called the unigram language model, which also happens to be the most useful model for text retrieval.
And finally, what this class will use is a language model.
What is a language model?
Well, it's just a probability distribution over word sequences.
So here, I'll show one.
This model gives the sequence Today is Wednesday a probability of 0.001.
It give Today Wednesday is a very, very small probability because it's non-grammatical.
You can see the probabilities given to these sentences or sequences of words can vary a lot depending on the model.
Therefore, it's clearly context dependent.
In ordinary conversation, probably Today is Wednesday is most popular among these sentences.
Imagine in the context of discussing apply the math, maybe the eigenvalue is positive, would have a higher probability.
This means it can be used to represent the topic of a text.
The model can also be regarded as a probabilistic mechanism for generating text.
And this is why it's also often called a generating model.
So what does that mean?
We can imagine this is a mechanism that's visualised here as a stochastic system that can generate sequences of words.
So, we can ask for a sequence, and it's to send for a sequence from the device if you want, and it might generate, for example, Today is Wednesday, but it could have generated any other sequences.
So for example, there are many possibilities, right?
So in this sense, we can view our data as basically a sample observed from such a generating model.
So, why is such a model useful?
Well, it's mainly because it can quantify the uncertainties in natural language.
Where do uncertainties come from?
Well, one source is simply the ambiguity in natural language that we discussed earlier in the lecture.
Another source is because we don't have complete understanding, we lack all the knowledge to understand the language.
In that case, there will be uncertainties as well.
So let me show some examples of questions that we can answer with a language model that would have interesting applications in different ways.
Given that we see John and feels, how likely will we see happy as opposed to habit as the next word in a sequence of words?
Now, obviously, this would be very useful for speech recognition because happy and habit would have similar acoustic sound, acoustic signals.
But, if we look at the language model, we know that John feels happy would be far more likely than John feels habit.
Another example, given that we observe baseball three times and game once in a news article, how likely is it about sports?
This obviously is related to text categorization and information retrieval.
Also, given that a user is interested in sports news, how likely would the user use baseball in a query?
Now, this is clearly related to the query likelihood that we discussed in the previous lecture.
So now, let's look at the simplest language model, called a unigram language model.
In such a case, we assume that we generate a text by generating each word independently.
So this means the probability of a sequence of words would be then the product of the probability of each word.
Now normally, they're not independent, right?
So if you have single word in like a language, that would make it far more likely to observe model than if you haven't seen the language.
So this assumption is not necessarily true, but we make this assumption to simplify the model.
So now the model has precisely N parameters, where N is vocabulary size.
We have one probability for each word, and all these probabilities must sum to 1.
So strictly speaking, we actually have N-1 parameters.
As I said, text can then be assumed to be assembled, drawn from this word distribution.
So for example, now we can ask the device or the model to stochastically generate the words for us, instead of sequences.
So instead of giving a whole sequence, like Today is Wednesday, it now gives us just one word.
And we can get all kinds of words.
And we can assemble these words in a sequence.
So that will still allow you to compute the probability of Today is Wednesday as the product of the three probabilities.
As you can see, even though we have not asked the model to generate the sequences, it actually allows us to compute the probability for all the sequences, but this model now only needs N parameters to characterize.
That means if we specify all the probabilities for all the words, then the model's behavior is completely specified.
Whereas if we don't make this assumption, we would have to specify probabilities for all kinds of combinations of words in sequences.
So by making this assumption, it makes it much easier to estimate these parameters.
So let's see a specific example here.
Here I show two unigram language models with some probabilities.
And these are high probability words that are shown on top.
The first one clearly suggests a topic of text mining, because the high probability was all related to this topic.
The second one is more related to health.
Now we can ask the question, how likely were observe a particular text from each of these two models?
Now suppose we sample words to form a document.
Let's say we take the first distribution, would you like to sample words?
What words do you think would be generated while making a text or maybe mining maybe another word?
Even food, which has a very small probability, might still be able to show up.
But in general, high probability words will likely show up more often.
So we can imagine what general text of that looks like in text mining.
In fact, with small probability, you might be able to actually generate the actual text mining paper.
Now, it will actually be meaningful, although the probability will be very, very small.
In an extreme case, you might imagine we might be able to generate a text mining paper that would be accepted by a major conference.
And in that case, the probability would be even smaller.
But it's a non-zero probability, if we assume none of the words have non-zero probability.
Similarly from the second topic, we can imagine we can generate a food nutrition paper.
That doesn't mean we cannot generate this paper from text mining distribution.
We can, but the probability would be very, very small, maybe smaller than even generating a paper that can be accepted by a major conference on text mining.
So the point is that the keeping distribution, we can talk about the probability of observing a certain kind of text.
Some texts will have higher probabilities than others.
Now let's look at the problem in a different way.
Suppose we now have available a particular document.
In this case, many of the abstract or the text mining table, and we see these word counts here.
The total number of words is 100.
Now the question you ask here is an estimation question.
We can ask the question which model, which one of these distribution has been used to generate this text, assuming that the text has been generated by assembling words from the distribution.
So what would be your guess?
What we have to decide are what probabilities text mining, etc., would have.
Suppose the view for a second, and try to think about your best guess.
If you're like a lot of people, you would have guessed that well, my best guess is text has a probability of 10 out of 100 because I've seen text 10 times, and there are in total 100 words.
So we simply normalize these counts.
And that's in fact the word justified, and your intuition is consistent with mathematical derivation.
And this is called the maximum likelihood estimator.
In this estimator, we assume that the parameter settings of those that would give our observe the data the maximum probability.
That means if we change these probabilities, then the probability of observing the particular text data would be somewhat smaller.
So you can see, this has a very simple formula.
Basically, we just need to look at the count of a word in a document, and then divide it by the total number of words in the document or document lens.
Normalize the frequency.
A consequence of this is, of course, we're going to assign zero probabilities to unseen words.
If we have an observed word, there will be no incentive to assign a non-zero probability using this approach.
Why?
Because that would take away probability mass for these observed words.
And that obviously wouldn't maximize the probability of this particular observed text data.
But one has still question whether this is our best estimate.
Well, the answer depends on what kind of model you want to find, right?
This estimator gives a best model based on this particular data.
But if you are interested in a model that can explain the content of the full paper for this abstract, then you might have a second thought, right?
So for thing, there should be other words in the body of that article, so they should not have zero probabilities, even though they're not observed in the abstract.
So we're going to cover this a little bit more later in this class in the query likelihood model.
So let's take a look at some possible uses of these language models.
One use is simply to use it to represent the topics.
So here I show some general English background texts.
We can use this text to estimate a language model, and the model might look like this.
Right, so on the top, we have those all common words, the, a, is, we, etc., and then we'll see some common words like these, and then some very, very rare words in the bottom.
This is a background language model.
It represents the frequency of words in English in general.
This is the background model.
Now let's look at another text, maybe this time, we'll look at the computer science research papers.
So we have a collection of computer science research papers, we do as mentioned again, we can just use the maximum likelihood estimator, where we simply normalize the frequencies.
Now in this case, we'll get the distribution that looks like this.
On the top, it looks similar because these words occur everywhere, they are very common.
But as we go down, we'll see words that are more related to computer science, computer software, text, etc.
And so although here, we might also see these words, for example, computer, but we can imagine the probability here is much smaller than the probability here.
And we will see many other words here that would be more common in general English.
So you can see this distribution characterizes a topic of the corresponding text.
We can look at even the smaller text.
So in this case, let's look at the text mining paper.
Now if we do the same, we have another distribution, again the can be expected to occur in the top.
The sooner we see text, mining, association, clustering, these words have relatively high probabilities.
In contrast, in this distribution, the text has a relatively small probability.
So this means, again, based on different text data, we can have a different model, and the model captures the topic.
So we call this document the language model, and we call this collection language model.
And later, you will see how they're used in the retrieval function.
But now, let's look at another use of this model.
Can we statistically find what words are semantically related to computer?
Now how do we find such words?
Well, our first thought is that let's take a look at the text that match computer.
So we can take a look at all the documents that contain the word computer.
Let's build a language model.
We can see what words we see there.
Well, not surprisingly, we see these common words on top as we always do.
So in this case, this language model gives us the conditional probability of seeing the word in the context of computer.
And these common words will naturally have high probabilities.
But we also see the computer itself and software will have relatively high probabilities.
But if we just use this model, we cannot just say all these words are semantically related to computer.
So ultimately, what we'd like to get rid of is these common words.
How can we do that?
It turns out that it's possible to use language model to do that.
But I suggest you think about that.
So how can we know what words are very common, so that we want to kind of get rid of them?
What model will tell us that?
Well, maybe you can think about that.
So the background language model precisely tells us this information.
It tells us what was our common in general.
So if we use this background model, we would know that these words are common words in general.
So it's not surprising to observe them in the context of computer.
Whereas computer has a very small probability in general, so it's very surprising that we have seen computer with this probability, and the same is true for software.
So then we can use these two models to somehow figure out the words that are related to computer.
For example, we can simply take the ratio of these group probabilities and normalize the topic of language model by the probability of the word in the background language model.
So if we do that, we take the ratio, we'll see that then on the top, computer is ranked, and then followed by software, program, all these words related to computer.
Because they occur very frequently in the context of computer, but not frequently in the whole collection, whereas these common words will not have a high probability.
In fact, they have a ratio about 1 down there because they are not really related to computer.
By taking the sample of text that contains the computer, we don't really see more occurrences of that than in general.
So this shows that even with these simple language models, we can do some limited analysis of semantics.
So in this lecture, we talked about language model, which is basically a probability distribution over text.
We talked about the simplest language model called unigram language model, which is also just a word distribution.
We talked about the two uses of a language model.
One is we represent the topic in a document, in a collection, or in general.
The other is we discover word associations.
In the next lecture, we're going to talk about how language model can be used to design a retrieval function.
Here are two additional readings.
The first is a textbook on statistical natural language processing.
The second is an article that has a survey of statistical language models with a lot of pointers to research work.




Session 4.3: Query Likelihood Retrieval Function

This lecture is about query likelihood, probabilistic retrieval model.
In this lecture, we continue the discussion of probabilistic retrieval model.
In particular, we're going to talk about the query light holder retrieval function.
In the query light holder retrieval model, our idea is model.
How like their user who likes a document with pose a particular query?
So in this case, you can imagine if a user likes this particular document about a presidential campaign news.
Now we assume, the user would use this a document as a basis to impose a query to try and retrieve this document.
So again, imagine use a process that works as follows.
Where we assume that the query is generated by assembling words from the document.
So for example, a user might pick a word like presidential, from this document and then use this as a query word.
And then the user would pick another word like campaign, and that would be the second query word.
Now this of course is an assumption that we have made about how a user would pose a query.
Whether a user actually followed this process may be a different question, but this assumption has allowed us to formerly characterize this conditional probability.
And this allows us to also not rely on the big table that I showed you earlier to use empirical data to estimate this probability.
And this is why we can use this idea then to further derive retrieval function that we can implement with the program language.
So as you see the assumption that we made here is each query word is independent of the sample.
And also each word is basically obtained from the document.
So now let's see how this works exactly.
Well, since we are completing a query likelihood then the probability here is just the probability of this particular query, which is a sequence of words.
And we make the assumption that each word is generated independently.
So as a result, the probability of the query is just a product of the probability of each query word.
Now how do we compute the probability of each query word?
Well, based on the assumption that a word is picked from the document that the user has in mind.
Now we know the probability of each word is just the relative frequency of each word in the document.
So for example, the probability of presidential given the document.
Would be just the count of presidential document divided by the total number of words in the document or document s.
So with these assumptions we now have actually a simple formula for retrieval.
We can use this to rank our documents.
So does this model work?
Let's take a look.
Here are some example documents that you have seen before.
Suppose now the query is presidential campaign and we see the formula here on the top.
So how do we score this document?
Well, it's very simple.
We just count how many times do we have seen presidential or how many times do we have seen campaigns, etc.
And we see here 44, and we've seen presidential twice.
So that's 2 over the length of document 4 multiplied by 1 over the length of document 4 for the probability of campaign.
And similarly, we can get probabilities for the other two documents.
Now if you look at these numbers or these formulas for scoring all these documents, it seems to make sense.
Because if we assume d3 and d4 have about the same length, then looks like a nominal rank d4 above d3 and which is above d2.
And as we would expect, looks like it did captures a TF query state, and so this seems to work well.
However, if we try a different query like this one, presidential campaign update then we might see a problem.
Well what problem?
Well think about the update.
Now none of these documents has mentioned update.
So according to our assumption that a user would pick a word from a document to generate a query, then the probability of obtaining the word update would be what?
Would be 0.
So that causes a problem, because it would cause all these documents to have zero probability of generating this query.
Now why it's fine to have zero probability for d2, which is non-relevant?
It's not okay to have 0 for d3 and d4 because now we no longer can distinguish them.
What's worse?
We can't even distinguish them from d2.
So that's obviously not desirable.
Now when a  has such result, we should think about what has caused this problem?
So we have to examine what assumptions have been made, as we derive this ranking function.
Now is you examine those assumptions carefully you will realize, what has caused this problem?
So take a moment to think about it.
What do you think is the reason why update has zero probability and how do we fix it?
So if you think about this from the moment you realize that that's because we have made an assumption that every query word must be drawn from the document in the user's mind.
So in order to fix this, we have to assume that the user could have drawn a word not necessarily from the document.
So that's the improved model.
An improvement here is to say that, well instead of drawing a word from the document, let's imagine that the user would actually draw a word from a document model.
And so I show a model here.
And we assume that this document is generated using this unigram language model.
Now, this model doesn't necessarily assign zero probability for update in fact, we can assume this model does not assign zero probability for any word.
Now if we're thinking this way then the generation process is a little bit different.
Now the user has this model in mind instead of this particular document.
Although the model has to be estimated based on the document.
So the user can again generate the query using a singular process.
Namely, pick a word for example, presidential and another word campaign.
Now the difference is that this time we can also pick a word like update, even though update doesn't occur in the document to potentially generate a query word like update.
So that a query was updated 1 times 0 probabilities.
So this would fix our problem.
And it's also reasonable because when our thinking of what the user is looking for in a more general way, that is unique language model instead of fixed document.
So how do we compute this query likelihood?
If we make this sum wide involved two steps.
The first one is compute this model, and we call it document language model here.
For example, I've shown two pulse models here, it's major based on two documents.
And then given a query like a data mining algorithms the thinking is that we'll just compute the likelihood of this query.
And by making independence assumptions we could then have this probability as a product of the probability of each query word.
We do this for both documents, and then we can score these two documents and then rank them.
So that's the basic idea of this query likelihood retrieval function.
So more generally this ranking function would look like in the following.
Here we assume that the query has n words, w1 through wn, and then the scoring function.
The ranking function is the probability that we observe this query, given that the user is thinking of this document.
And this is assume it will be product of probabilities of all individual words.
This is based on independent assumption.
Now we actually often score the document before this query by using log of the query likelihood as shown on the second line.
Now we do this to avoid having a lot of small probabilities, mean multiply together.
And this could cause under flow and we might loose the precision by transforming the value in our algorithm function.
We maintain the order of these documents yet we can avoid the under flow problem.
And so if we take longer than transformation of course, the product would become a sum as you on the second line here.
So the sum of all the query words inside of the sum that is one of the probability of this word given by the document.
And then we can further rewrite the sum to a different form.
So in the first sum here, in this sum, we have it over all the query words and query word.
And in this sum we have a sum of all the possible words.
But we put a counter here of each word in the query.
Essentially we are only considering the words in the query, because if a word is not in the query, the count will be 0.
So we're still considering only these n words.
But we're using a different form as if we were going to take a sample of all the words in the vocabulary.
And of course, a word might occur multiple times in the query.
That's why we have a count here.
And then this part is log of the probability of the word, given by the document language model.
So you can see in this retrieval function, we actually know the count of the word in the query.
So the only thing that we don't know is this document language model.
Therefore, we have converted the retrieval problem include the problem of estimating this document language model.
So that we can compute the probability of each query word given by this document.
And different estimation methods would lead to different ranking functions.
This is just like a different way to place document in the vector space which leads to a different ranking function in the vector space model.
Here different ways to estimate will lead to a different ranking function for query likelihood.




Session 4.4: Statistical Language Model Part 1

This lecture is about smoothing of language models.
In this lecture, we're going to continue talking about the probabilistic retrieval model.
In particular, we're going to talk about the smoothing of language model in the query likelihood retrieval method.
So you have seen this slide from a previous lecture.
This is the ranking function based on the query likelihood.
Here, we assume that the independence of generating each query word And the formula would look like the following where we take a sum of all the query words.
And inside the sum there is a log of probability of a word given by the document or document image model.
So the main task now is to estimate this document language model as we said before different methods for estimating this model would lead to different retrieval functions.
So in this lecture, we're going to be looking to this in more detail.
So how do we estimate this language model?
Well the obvious choice would be the maximum likelihood estimate that we have seen before.
And that is we're going to normalize the word frequencies in the document.
And estimate the probability it would look like this.
This is a step function here.
Which means all of the words that have the same frequency count will have identical problem with it.
This is another freedom to count, that has different probability.
Note that for words that have not occurred in the document here they will have 0 probability.
So we know this is just like the model that we assume earlier in the lecture.
Where we assume that the use of the simple word from the document to a formula to clear it.
And there's no chance of assembling any word that's not in the document and we know that's not good.
So how do we improve this?
Well in order to assign a none 0 probability to words that have not been observed in the document, we would have to take away some probability mass from the words that are observed in the document.
So for example here, we have to take away some probability of the mass because we need some extra probability mass for the words otherwise they won't sum to 1.
So all these probabilities must sum to 1.
So to make this transformation and to improve the maximum likelihood estimated by assigning non zero probabilities to words that are not observed in the data.
We have to do smoothing and smoothing has to do with improving the estimate by considering the possibility that if the author had been asking to write more words for the document, the author might have written other words.
If you think about this factor then the a smoothed language model would be a more accurate than the representation of the actual topic.
Imagine you have seen an abstract of a research article.
Let's say this document is abstract.
If we assume and see words in this abstract that we have a probability of 0.
That would mean there's no chance of sampling a word outside the abstract of the formulated query.
But imagine a user who is interested in the topic of this subject.
The user might actually choose a word that's not in that chapter to use as query.
So obviously, if we has asked this author to write more author would have written a full text of the article.
So smoothing of the language model is an attempt to try to recover the model for the whole article.
And then of course, we don't have knowledge about any words that are not observed in the abstract.
So that's why smoothing is actually a tricky problem.
So let's talk a little more about how to smooth a language model.
The key question here is, what probability should be assigned to those unseen words?
And there are many different ways of doing that.
One idea here, that's very useful for retrieval is let the probability of unseen word be proportional to its probability given by a reference language model.
That means if you don't observe the word in the dataset.
We're going to assume that its probability is kind of governed by another reference language model that we will construct.
It will tell us which unseen words would have a higher probability.
In the case of retrieval, a natural choice would be to take the collection language model as the reference language model.
That is to say, if you don't observe a word in the document, we're going to assume that the probability of this word would be proportional to the probability of word in the whole collection.
So more formally, we'll be estimating the probability of a word key document as follows.
If the word is seen in the document then the probability would be this counted the maximum likelihood estimate P sub c here.
Otherwise, if the word is not seen in the document we're going to let probability be proportional to the probability of the word in the collection.
And here the coefficient that offer is to control the amount of probability mass that we assign to unseen words.
Obviously, all these probabilities must sum to 1, so alpha sub d is constrained in some way.
So what if we plug in this smoothing formula into our query likelihood ranking function?
This is what we will get.
In this formula, we have this as a sum over all the query words and those that we have written here as the sum of all the vocabulary, you see here.
This is the sum of all the words in the vocabulary, but not that we have a count of the word in the query.
So in fact, we are just taking a sample of query words.
This is now a common way that we would use, because of its convenience in some transformations.
So this is as I said, this is sum of all the query words.
In our smoothing method, we assume that the words that are not observed in the method would have a somewhat different form of probability.
Name it's four, this foru.
So we're going to do then, decompose the sum into two parts.
One sum is over all the query words that are matching the document.
That means that in this sum, all the words have a non zero probability in the document.
Sorry, it's the non zero count of the word in the document.
They all occur in the document.
And they also have to of course have a non zero count in the query.
So these are the query words that are matching the document.
On the other hand, in this sum we are taking a sum of all the words that are not all query was not matching the document.
So they occur in the query due to this term, but they don't occur in the document.
In this case, these words have this probability because of our assumption about the smoothing.
That here, these seen words have a different probability.
Now, we can go further by rewriting the second sum as a difference of two other sums.
Basically, the first sum is the sum of all the query words.
Now, we know that the original sum is not over all the query words.
This is over all the query words that are not matched in the document.
So here we pretend that they are actually over all the query words.
So we take a sum over all the query words.
Obviously, this sum has extra terms that are not in this sum.
Because, here we're taking sum over all the query words.
There, it's not matched in the document.
So in order to make them equal, we will have to then subtract another sum here.
And this is the sum over all the query words that are matching in the document.
And this makes sense, because here we are considering all query words.
And then we subtract the query that was matched in the document.
That would give us the query that was not matched in the document.
And this is almost a reverse process of the first step here.
And you might wonder why do we want to do that.
Well, that's because if we do this, then we have different forms of terms inside of these sums.
So now, you can see in this sum we have all the words matched, the query was matching the document with this kind of term.
Here we have another sum over the same set of terms, matched query terms in document.
But inside the sum, it's different.
But these two sums can clearly be merged.
So if we do that, we'll get another form of the formula that looks like before me at the bottom here.
And note that this is a very interesting formula.
Because here we combine these two that all or some of the query words matching in the document in the one sum here.
And the other sum now is decomposing into two parts.
And these two parts look much simpler just, because these are the probabilities of unseen words.
This formula is very interesting because you can see the sum is now over the match the query terms.
And just like in the vector space model, we take a sum of terms that are in the intersection of query vector and the document vector.
So it already looks a little bit like the vector space model.
In fact, there's even more similarity here as we explain on this slide.




Session 4.5: Statistical Language Model Part 2

So I showed you how we rewrite the query like holder which is a function into a form that looks like the formula of this slide after if we make the assumption about the smoothing, the language model based on the collection language model.
Now if you look at this rewriting, it will actually give us two benefits.
The first benefit is it helps us better understand this ranking function.
In particular, we're going to show that from this formula we can see smoothing with the collection language model would give us something like a TF-IDF weighting and length normalization.
The second benefit is that it also allows us to compute the query like holder more efficiently.
In particular we see that the main part of the formula is a sum over the match of the query terms.
So this is much better than if we take a sum over all the words.
After we smooth the document the damage model we essentially have non zero problem for all the words.
So this new form of the formula is much easier to score or to compute.
It's also interesting to note that the last term here is actually independent of the document.
Since our goal is to rank the documents for the same query we can ignore this term for ranking.
Because it's going to be the same for all the documents.
Ignoring it wouldn't affect the order of the documents.
Inside the sum, we also see that each matched query term would contribute a weight.
And this weight actually is very interesting because it looks like a TF-IDF weighting.
First we can already see it has a frequency of the word in a query just like in the vector space model.
When we take a thought product, we see the word frequency in the query to show up in such a sum.
And so naturally this part would correspond between the vector element from the documented vector.
And here indeed we can see it actually encodes a weight that has similar in factor to TF-IDF weight.
I'll let you examine it, can you see it?
Can you see which part is capturing TF?
And which part is a capturing IDF weighting?
So if want you can pause the video to think more about it.
So have you noticed that this P sub seen is related to the term frequency in the sense that if a word occurs very frequently in the document, then the s made through probability here will tend to be larger.
So this means this term is really doing something like a TF weight.
Now have you also noticed that this term in the denominator is actually achieving the factor of IDF?
Why, because this is the popularity of the term in a collection.
But it's in the denominator, so if the probability in the collection is larger then the weight is actually smaller.
And this means a popular term.
We actually have a smaller weight and this is precisely what IDF weighting is doing.
Only that we now have a different form of TF and IDF.
Remember IDF has a logarithm of documented frequency.
But here we have something different.
But intuitively it achieves a similar effect.
Interestingly, we also have something related to the length of libation.
Again, can you see which factor is related to the document length in this formula?
What I just say is that this term is related to IDF weighting.
This collection probability, but it turns out that this term here is actually related to document length normalization.
In particular, F of sub d might be related to document length.
So it encodes how much probability mass we want to give to unseen worlds.
How much smoothing do we want to do?
Intuitively, if a document is long, then we need to do less smoothing because we can assume that data is large enough.
We probably have observed all the words that the author could have written.
But if the document is short then r of sub t could be expected to be large.
We need to do more smoothing.
It's likey there are words that have not been written yet by the author.
So this term appears to paralyze the non document in that other sub D would tend to be longer than or larger than for a long document.
But note that alpha sub d also occurs here and so this may not actually be necessary paralyzing long documents.
The effect is not so clear yet.
But as we will see later, when we consider some specific smoothing methods, it turns out that they do paralyze long documents.
Just like in TF-IDF weighting and document length normalization formula in the vector space model.
So, that's a very interesting observation because it means we don't even have to think about the specific way of doing smoothing.
We just need to assume that if we smooth with this collection memory model, then we would have a formula that looks like TF-IDF weighting and documents length violation.
What's also interesting that we have very fixed form of the ranking function.
And see we have not heuristically put a logarithm here.
In fact, you can think about why we would have a logarithm here.
You look at the assumptions that we have made, it would be clear it's because we have used a logarithm of query like for scoring.
And we turned the product into a sum of logarithm of probability, and that's why we have this logarithm.
Note that if only want to heuristically implement a TF weighting and IDF weighting, we don't necessary have to have a logarithm here.
Imagine if we drop this logarithm, we would still have TF and IDF weighting.
But what's nice with problem risk modeling is that we are automatically given the logarithm function here.
And that's basically a fixed form of the formula that we did not really have to heuristically design, and in this case if you try to drop the logarithm the model probably won't work as well as if you keep the logarithm.
So a nice property of problem risk modeling is that by following some assumptions and the probability rules we'll get a formula automatically.
And the formula would have a particular form like in this case.
And if we heuristically design the formula we may not necessarily end up having such a specific formula.
So to summarize, we talked about the need for smoothing the document imaging model.
Otherwise it would give zero probability for unseen words in the document, and that's not good for storing a query with such an unseen word.
It's also necessary, in general, to improve the accuracy of estimating the model represent the topic of this document.
The general idea of smoothing in retrieval is to use the connecting memory model to, to give us some clue about which unseen words should have a higher probability.
That is, the probability of an unseen word is assumed to be proportional to its probability in the collection.
With this assumption, we've shown that we can derive a general ranking formula for query likelihood that has effect of TF-IDF weighting and document length normalization.
We also see that, through some rewriting, the scoring of such a ranking function is primarily based on sum of weights on matched query terms, just like in the vector space model.
But, the actual ranking function is given us automatically by the probability rules and assumptions that we have made.
And like in the vector space model where we have to heuristically think about the form of the function.
However, we still need to address the question how exactly we should smooth the document and the model.
How exactly we should use the reference and model based on the connection to adjust the probability of the maximum micro is made of and this is the topic of the next batch.




Session 4.6: Smoothing Methods Part 1

This lecture is about the specific smoothing methods for language models used in probabilistic retrieval model.
In this lecture, we will continue the discussion of language models for information retrieval, particularly the query likelihood retrieval method.
And we're going to talk about specifically the smoothing methods used for such a retrieval function.
So this is a slide from a previous lecture where we show that with a query likelihood ranking and smoothing with the collection language model, we add up having a retrieval function that looks like the following.
So this is the retrieval function based on these assumptions that we have discussed.
You can see it's a sum of all the matching query terms, here.
And inside its sum is the count of the term in the query and some weight for the term in the document.
We have t of i, the f weight here, and then we have another constant here in n.
So clearly if we want to implement this function using programming language, we still need to figure out a few variables.
In particular, we're going to need to know how to estimate the probability of a word exactly and how do we set alpha.
So in order to answer this question, we have to think about very specific smoothing methods, and that is main topic of this lecture.
We're going to talk about two smoothing methods.
The first is simple linear interpolation with a fixed coefficient.
And this is also called a Jelinek-Mercer smoothing.
So the idea is actually very simple.
This picture shows how we estimate a document language model by using maximum likelihood estimate.
That gives us word counts normalized by the total number of words in the text.
The idea of using this method is to maximize the probability of the observed text.
As a result, if a word like network is not observed in the text, it's going to get 0 probability, as shown here.
So the idea of smoothing, then, is to rely on collection language model where this word is not going to have a zero probability to help us decide what nonzero probability should be assigned to such a word.
So we can note that network has a nonzero probability here.
So in this approach what we do is we do a linear interpolation between the maximum likelihood placement here and the collection language model, and this is computed by the smoothing parameter lambda, which is between 0 and 1.
So this is a smoothing parameter.
The larger lambda is, the more smoothing we will have.
So by mixing them together, we achieve the goal of assigning nonzero probabilities to a word like network.
So let's see how it works for some of the words here.
For example, if we compute the smooth probability for text.
Now the maximum likelihood estimated gives us 10 over 100, and that's going to be here.
But the collection probability is this.
So we'll just combine them together with this simple formula.
We can also see the word network, which used to have a zero probability, now is getting a non-zero probability of this value.
And that's because the count is going to be zero for network here.
But this part is nonzero, and that's basically how this method works.
Now if you think about this and you can easily see now the alpha sub d in this smoothing method is basically lambda.
Because that's remember the coefficient in front of the probability of the word given by the collection language model here.
Okay, so this is the first smoothing method.
The second one is similar but it has a tie-in into the coefficient for linear interpolation.
It's often called Dirichlet Prior, or Bayesian, Smoothing.
So again here we face problem of zero probability for an unseen word like network.
Again we will use the collection language model, but in this case, we're going to combine them in somewhat different ways.
The formula first can be seen as a interpolation of the maximum likelihood estimate and the collection language model as before, as in the J-M smoothing method.
Only that the coefficient now is not lambda, a fixed number, but a dynamic coefficient in this form, where mu is a parameter, it's a non-negative value.
And you can see if we set mu to a constant, the effect is that a long document would actually get a smaller coefficient here.
Because a long document will have longer lengths, therefore the coefficient is actually smaller.
And so a long document would have less smoothing, as we would expect.
So this seems to make more sense than a fixed coefficient smoothing.
Of course, this part would be of this form so that the two coefficients would sum to 1.
Now this is one way to understand this smoothing.
Basically, it means it's a dynamic coefficient interpolation.
There is another way to understand this formula which is even easier to remember, and that's on this side.
So it's easier to see how we can rewrite the smoothing method in this form.
Now in this form we can easily see what change we have made to the maximum likelihood estimate, which would be this part.
So normalize the count by the document length.
So in this form we can see what we did is we add this to the count of every word.
So what does this mean?
Well, this is basically something related to the probability of the word in the collection.
And we multiply that by the parameter mu.
And when we combine this with the count here, essentially we are adding pseudocounts to the observed text.
We pretend every word has got this many pseudocount.
So the total count would be the sum of these pseudocounts and the actual count of the word in the document.
As a result, in total we would have added this many pseudocounts.
Why?
Because if you take somewhat this one over all the words, then we'll see the probability of the words would sum to 1, and that gives us just mu.
So this is the total number of pseudocounts that we added.
And so these probabilities would still sum to 1.
So in this case, we can easily see the method is essentially to add this as a pseudocount to this data.
Pretend we actually augment the data by including some pseudo data defined by the collection language model.
As a result, we have more counts is that the total counts for a word would be like this.
And as a result, even if a word has zero count here, let's say if we have zero count here, then it would still have nonzero count because of this part.
So this is how this method works.
Let's also take a look at some specific example here.
So for text again we will have 10 as the original count that we actually observe, but we also add some pseudocount.
And so the probability of text would be of this form.
Naturally, the probability of network would be just this part.
And so here you can also see what's alpha sub d here.
Can you see it?
If you want to think about it, you can pause the video.
But you'll notice that this part is basically alpha sub d.
So we can see, in this case, alpha sub d does depend on the document, because this length depends on the document, whereas in the linear interpolation, the J-M smoothing method, this is a constant.




Session 4.7: Smoothing Methods Part 2

So let's plug in these model masses into the ranking function to see what we will get, okay?
This is a general smoothing.
So a general ranking function for smoothing with subtraction and you have seen this before.
And now we have a very specific smoothing method, the JM smoothing method.
So now let's see what what's a value for office of D here.
And what's the value for p sub c here?
Right, so we may need to decide this in order to figure out the exact form of the ranking function.
And we also need to figure out of course alpha.
So let's see.
Well this ratio is basically this, right, so, here, this is the probability of c board on the top, and this is the probability of unseen war or, in other words basically 11 times basically the alpha here, this, so it's easy to see that.
This can be then rewritten as this.
Very simple.
So we can plug this into here.
And then here, what's the value for alpha?
What do you think?
So it would be just lambda, right?
And what would happen if we plug in this value here, if this is lambda.
What can we say about this?
Does it depend on the document?
No, so it can be ignored.
Right?
So we'll end up having this ranking function shown here.
And in this case you can easy to see, this a precisely a vector space model because this part is a sum over all the matched query terms, this is an element of the query map.
What do you think is a element of the document up there?
Well it's this, right.
So that's our document left element.
And let's further examine what's inside of this logarithm.
Well one plus this.
So it's going to be nonnegative, this log of this, it's going to be at least 1, right?
And these, this is a parameter, so lambda is parameter.
And let's look at this.
Now this is a TF.
Now we see very clearly this TF weighting here.
And the larger the count is, the higher the weighting will be.
We also see IDF weighting, which is given by this.
And we see docking the lan's relationship here.
So all these heuristics are captured in this formula.
What's interesting that we kind of have got this weighting function automatically by making various assumptions.
Whereas in the vector space model, we had to go through those heuristic design in order to get this.
And in this case note that there's a specific form.
And when you see whether this form actually makes sense.
All right so what do you think is the denominator here, hm?
This is a math of document.
Total number of words, multiplied by the probability of the word given by the collection, right?
So this actually can be interpreted as expected account over word.
If we're going to draw, a word, from the connection that we model.
And, we're going to draw as many as the number of words in the document.
If you do that, the expected account of a word, w, would be precisely given by this denominator.
So, this ratio basically, is comparing the actual count, here.
The actual count of the word in the document with expected count given by this product if the word is in fact following the distribution in the clutch this.
And if this counter is larger than the expected counter in this part, this ratio would be larger than one.
So that's actually a very interesting interpretation, right?
It's very natural and intuitive, it makes a lot of sense.
And this is one advantage of using this kind of probabilistic reasoning where we have made explicit assumptions.
And, we know precisely why we have a logarithm here.
And, why we have these probabilities here.
And, we also have a formula that intuitively makes a lot of sense and does TF-IDF weighting and documenting and some others.
Let's look at the, the Dirichlet Prior Smoothing.
It's very similar to the case of JM smoothing.
In this case, the smoothing parameter is mu and that's different from lambda that we saw before.
But the format looks very similar.
The form of the function looks very similar.
So we still have linear operation here.
And when we compute this ratio, one will find that is that the ratio is equal to this.
And what's interesting here is that we are doing another comparison here now.
We're comparing the actual count.
Which is the expected account of the world if we sampled meal worlds according to the collection world probability.
So note that it's interesting we don't even see docking the lens here and lighter in the JMs model.
All right so this of course should be plugged into this part.
So you might wonder, so where is docking lens.
Interestingly the docking lens is here in alpha sub d so this would be plugged into this part.
As a result what we get is the following function here and this is again a sum over all the match query words.
And we're against the queer, the query, time frequency here.
And you can interpret this as the element of a document vector, but this is no longer a single dot product, right?
Because we have this part, I know that n is the name of the query, right?
So that just means if we score this function, we have to take a sum over all the query words, and then do some adjustment of the score based on the document.
But it's still, it's still clear that it does documents lens modulation because this lens is in the denominator so a longer document will have a lower weight here.
And we can also see it has tf here and now idf.
Only that this time the form of the formula is different from the previous one in JMs one.
But intuitively it still implements TFIDF waiting and document lens rendition again, the form of the function is dictated by the probabilistic reasoning and assumptions that we have made.
Now there are also disadvantages of this approach.
And that is, there's no guarantee that there's such a form of the formula will actually work well.
So if we look about at this geo function, all those TF-IDF waiting and document lens rendition for example it's unclear whether we have sub-linear transformation.
Unfortunately we can see here there is a logarithm function here.
So we do have also the, so it's here right?
So we do have the sublinear transformation, but we do not intentionally do that.
That means there's no guarantee that we will end up in this, in this way.
Suppose we don't have logarithm, then there's no sub-linear transformation.
As we discussed before, perhaps the formula is not going to work so well.
So that's an example of the gap between a formal model like this and the relevance that we have to model, which is really a subject motion that is tied to users.
So it doesn't mean we cannot fix this.
For example, imagine if we did not have this logarithm, right?
So we can take a risk and we're going to add one, or we can even add double logarithm.
But then, it would mean that the function is no longer a proper risk model.
So the consequence of the modification is no longer as predictable as what we have been doing now.
So, that's also why, for example, PM45 remains very competitive and still, open channel how to use public risk models as they arrive, better model than the PM25.
In particular how do we use query like how to derive a model and that would work consistently better than DM 25.
Currently we still cannot do that.
Still interesting open question.
So to summarize this part, we've talked about the two smoothing methods.
Jelinek-Mercer which is doing the fixed coefficient linear interpolation.
Dirichlet Prior this is what add a pseudo counts to every word and is doing adaptive interpolation in that the coefficient would be larger for shorter documents.
In most cases we can see, by using these smoothing methods, we will be able to reach a retrieval function where the assumptions are clearly articulate.
So they are less heuristic.
Explaining the results also show that these, retrieval functions.
Also are very effective and they are comparable to BM 25 or pm lens adultation.
So this is a major advantage of probably smaller where we don't have to do a lot of heuristic design.
Yet in the end that we naturally implemented TF-IDF weighting and doc length normalization.
Each of these functions also has precise ones smoothing parameter.
In this case of course we still need to set this smoothing parameter.
There are also methods that can be used to estimate these parameters.
So overall, this shows by using a probabilistic model, we follow very different strategies then the vector space model.
Yet, in the end, we end up uh,with some retrievable functions that look very similar to the vector space model.
With some advantages in having assumptions clearly stated.
And then, the form dictated by a probabilistic model.
Now, this also concludes our discussion of the query likelihood probabilistic model.
And let's recall what assumptions we have made in order to derive the functions that we have seen in this lecture.
Well we basically have made four assumptions that I listed here.
The first assumption is that the relevance can be modeled by the query likelihood.
And the second assumption with med is, are query words are generated independently that allows us to decompose the probability of the whole query into a product of probabilities of old words in the query.
And then, the third assumption that we have made is, if a word is not seen, the document or in the late, its probability proportional to its probability in the collection.
That's a smoothing with a collection ama model.
And finally, we made one of these two assumptions about the smoothing.
So we either used JM smoothing or Dirichlet prior smoothing.
If we make these four assumptions then we have no choice but to take the form of the retrieval function that we have seen earlier.
Fortunately the function has a nice property in that it implements TF-IDF weighting and document machine and these functions also work very well.
So in that sense, these functions are less heuristic compared with the vector space model.
And there are many extensions of this, this basic model and you can find the discussion of them in the reference at the end of this lecture.




Session 5.1: Feedback In Text Retrieval

This lecture is about the feedback in text retrieval.
So in this lecture, we will continue with the discussion of text retrieval methods.
In particular, we're going to talk about the feedback in text retrieval.
This is a diagram that shows the retrieval process.
We can see the user would type in a query.
And then, the query would be sent to a retrieval engine or search engine, and the engine would return results.
These results would be issued to the user.
Now, after the user has seen these results, the user can actually make judgements.
So for example, the user says, well, this is good and this document is not very useful and this is good again, etc.
Now, this is called a relevance judgment or relevance feedback because we've got some feedback information from the user based on the judgements.
And this can be very useful to the system, knowing what exactly is interesting to the user.
So the feedback module would then take this as input and also use the document collection to try to improve ranking.
Typically it would involve updating the query so the system can now render the results more accurately for the user.
So this is called relevance feedback.
The feedback is based on relevance judgements made by the users.
Now, these judgements are reliable but the users generally don't want to make extra effort unless they have to.
So the down side is that it involves some extra effort by the user.
There's another form of feedback called pseudo relevance feedback, or blind feedback, also called automatic feedback.
In this case, we can see once the user has gotten  or in fact we don't have to invoke users.
So you can see there's no user involved here.
And we simply assume that the top rank documents to be relevant.
Let's say we have assumed top 10 as relevant.
And then, we will then use this assume the documents to learn and to improve the query.
Now, you might wonder, how could this help if we simply assume the top rank of documents?
Well, you can imagine these top rank of documents are actually similar to relevant documents even if they are not relevant.
They look like relevant documents.
So it's possible to learn some related terms to the query from this set.
In fact, you may recall that we talked about using language model to analyze what association, to learn related words to the word of computer.
And there, what we did is we first use computer to retrieve all the documents that contain computer.
So imagine now the query here is a computer.
And then, the result will be those documents that contain computer.
And what we can do then is to take the top n results.
They can match computer very well.
And we're going to count the terms in this set.
And then, we're going to then use the background language model to choose the terms that are frequent in this set but not frequent in the whole collection.
So if we make a contrast between these two what we can find is that related to terms to the word computer.
As we have seen before.
And these related words can then be added to the original query to expand the query.
And this would help us bring the documents that don't necessarily match computer but match other words like program and software.
So this is very effective for improving the search result.
But of course, pseudo-relevancy values are completely unreliable.
We have to arbitrarily set a cut off.
So there's also something in between called implicit feedback.
In this case, what we do is we do involve users, but we don't have to ask users to make judgments.
Instead, we're going to observe how the user interacts with the search results.
So in this case we'll look at the clickthroughs.
So the user clicked on this one.
And the user viewed this one.
And the user skipped this one.
And the user viewed this one again.
Now, this also is a clue about whether the document is useful to the user.
And we can even assume that we're going to use only the snippet here in this document, the text that's actually seen by the user instead of the actual document of this entry.
The link they are saying web search may be broken but it doesn't matter.
If the user tries to fetch this document because of the displayed text we can assume these displayed text is probably relevant is interesting to you so we can learn from such information.
And this is called interesting feedback.
And we can, again, use the information to update the query.
This is a very important technique used in modern.
Now, think about the Google and Bing and they can collect a lot of user activities while they are serving us.
So they would observe what documents we click on, what documents we skip.
And this information is very valuable.
And they can use this to improve the search engine.
So to summarize, we talked about the three kinds of feedback here.
Relevant feedback where the user makes explicit judgements.
It takes some user effort, but the judgment information is reliable.
We talk about the pseudo feedback where we seem to assume top brand marking will be relevant.
We don't have to involve the user therefore we could do that, actually before we return the results to the user.
And the third is implicit feedback where we use clickthroughs.
Where we involve the users, but the user doesn't have to make it explicitly their fault.
Make judgement.




Session 5.2: Feedback In Vector Space Model Rocchio

This lecture is about the feedback in the vector space model.
In this lecture, we continue talking about the feedback in text retrieval.
Particularly, we're going to talk about feedback in the vector space model.
As we have discussed before, in the case of feedback the task of text retrieval system is removed from examples in improved retrieval accuracy.
We will have positive examples.
Those are the documents that assume would be relevant or be charged with being relevant.
All the documents that are viewed by users.
We also have negative examples.
Those are documents known to be non-relevant.
They can also be the documents that are skipped by users.
The general method in the vector space model for feedback is to modify our query vector.
We want to place the query vector in a better position to make it accurate.
And what does that mean exactly?
Well, if we think about the query vector that would mean we would have to do something to the vector elements.
And in general, that would mean we might add new terms.
Or we might just weight of old terms or assign weights to new terms.
As a result, in general, the query will have more terms.
We often call this query expansion.
The most effective method in the vector space model for feedback is called the Rocchio Feedback, which was actually proposed several decades ago.
So the idea is quite simple.
We illustrate this idea by using a two dimensional display of all the documents in the collection and also the query vector.
So now we can see the query vector is here in the center, and these are all the documents.
So when we use the query back there and use the same narrative function to find the most similar documents, we are basically doing a circle here and that these documents would be basically the top-ranked documents.
And these process are relevant documents, and these are relevant documents, for example, it's relevant, etc.
And then these minuses are negative documents, like these.
So our goal here is trying to move this query back to some position, to improve the retrieval accuracy.
By looking at this diagram, what do you think?
Where should we move the query vector so that we can improve the retrieval accuracy?
Intuitively, where do you want to move query vector?
If you want to think more, you can pause the video.
If you think about this picture, you can realize that in order to work well in this case you want the query vector to be as close to the positive vectors as possible.
That means ideally, you want to place the query vectors somewhere here.
Or we want to move the query vector closer to this point.
Now so what exactly is this point?
Well, if you want these relevant documents to rank on the top, you want this to be in the center of all these relevant documents, right?
Because then if you draw a circle around this one, you'll get all these relevant documents.
So that means we can move the query vector towards the centroid of all the relevant document vectors.
And this is basically the idea of Rocchio.
Of course, you can consider the centroid of negative documents and we want to move away from the negative documents.
Now your match that we're talking about moving vector closer to some other vec and away from other vectors.
It just means that we have this formula.
Here you can see this is original query vector and this average basically is the centroid vector of relevant documents.
When we take the average of these vectors, then were computing the centroid of these vectors.
Similarly, this is the average of non-relevant document like this.
So it's essentially of non-relevant documents.
And we have these three parameters here, alpha, beta, and gamma.
They are controlling the amount of movement.
When we add these two vectors together, we're moving the query vector closer to the centroid.
This is when we add them together.
When we subtracted this part, we kind of move the query vector away from that centroid.
So this is the main idea of Rocchio feedback.
And after we have done this, we will get a new query vector which can be used to score documents.
This new query vector, will then reflect the move of this original query vector toward this relevant centroid vector and away from the non-relevant value.
Okay, so let's take a look at the example.
This is the example that we've seen earlier.
Only that I deemed that display of the actual documents.
I only showed the vector representation of these documents.
We have five documents here and we have to read in the documents here, right.
And they're displayed in red.
And these are the term vectors.
Now I have just assumed some of weights.
A lot of terms, we have zero weights of course.
Now these are negative arguments.
There are two here.
There is another one here.
Now in this Rocchio method, we first compute the centroid of each category.
And so let's see, look at the centroid vector of the positive documents, we simply just, so it's very easy to see.
We just add this with this one the corresponding element.
And then that's down here and take the average.
And then we're going to add the corresponding elements and then just take the average.
And so we do this for all this.
In the end, what we have is this one.
This is the average vector of these two, so it's a centroid of these two.
Let's also look at the centroid of the negative documents.
This is basically the same.
We're going to take the average of the three elements.
And these are the corresponding elements in the three vectors, and so on and so forth.
So in the end, we have this one.
Now in the Rocchio feedback method we're going to combine all these with the original query vector which is this.
So now let's see how we combine them together.
Well, that's basically this.
So we have a parameter alpha controlling the original query times weight that's one.
And now we have beta to control the inference of the positive centroid of the weight, that's 1.5.
That comes from here.
All right, so this goes here.
And we also have this negative weight here gamma here.
And this way, it has come from, of course, the negative centroid here.
And we do exactly the same for other terms, each is for one term.
And this is our new vector.
And we're going to use this new query vector, this one to rank the documents.
You can imagine what would happen, right?
Because of the movement that this one would matches these red documents much better because we moved this vector closer to them.
And it's going to penalize these black documents, these non relevent documents.
So this is precisely what we wanted from feedback.
Now of course if we apply this method in practice we will see one potential problem and that is the original query has only four terms that are now zero.
But after we do query explaining and merging, we'll have many times that would have non zero weights.
So the calculation will have to involve more terms.
In practice, we often truncate this matter and only retain the terms with highest weights.
So let's talk about how we use this method in practice.
I just mentioned that they're often truncated vector.
Consider only a small number of words that have highest weights in the centroid vector.
This is for efficiency concern.
I also said here that negative examples, or non-relevant examples tend not to be very useful, especially compared with positive examples.
Now you can think about why.
One reason is because negative documents tend to distract the query in all directions.
So, when you take the average, it doesn't really tell you where exactly it should be moving to.
Whereas positive documents tend to be clustered together.
And they will point you to a consistent direction.
So that also means that sometimes we don't have to use those negative examples.
But note that in some cases, in difficult queries where most results are negative, negative feedback after is very useful.
Another thing is to avoid over-fitting.
That means we have to keep relatively high weight on the original query terms.
Why?
Because the sample that we see in feedback Is a relatively small sample.
We don't want to overly trust the small sample.
And the original query terms are still very important.
Those terms are heightened by the user and the user has decided that those terms are most important.
So in order to prevent the us from over-fitting or drifting, prevent topic drifting due to the bias toward the feed backing symbols.
We generally would have to keep a pretty high weight on the original terms so it was safe to do that.
And this is especially true for pseudo relevance feedback.
Now, this method can be used for both relevance feedback and pseudo-relevance feedback.
In the case of pseudo-feedback, the prime and the beta should be set to a smaller value because the relevant examples are assumed not to be relevant.
They're not as reliable as the relevance feedback.
In the case of relevance feedback, we obviously could use a larger value.
So those parameters, they have to be set empirically.
And the Rocchio Method is usually robust and effective.
It's still a very popular method for feedback.




Session 5.3: Feedback In Text Retrieval Feedback In Lm

This lecture is about the feedback in the language modeling approach.
In this lecture, we will continue the discussion of feedback in text retrieval.
In particular, we're going to talk about the feedback in language modeling approaches.
So we derive the query likelihood ranking function by making various assumptions.
As a basic retrieval function, all those formulas worked well.
But if we think about the feedback information, it's a little bit awkward to use query likelihood to perform feedback, because a lot of times the feedback information is additional information about the query.
But we assume the query has generated it by assembling words from a language model in the query likelihood method.
It's kind of unnatural to sample words that form feedback documents.
As a result, researchers proposed a way to generalize query likelihood function, and it's called Kullback-Leibler divergence retrieval model.
And this model is actually going to make the query likelihood retrieval function much closer to vector space model.
Yet this form of the language model can be regarded as a generalization of query likelihood, in the sense that it can cover query likelihood as a special case.
And in this case, then feedback can be achieved through simply query model estimation or updating.
This is very similar to Rocchio, which updates the query vector.
So let's see what is this KL-divergence retrieval model.
So on the top, what you see is a query likelihood retrieval function, this one.
And then KL-divergence, or also called cross entropy, retrieval model is basically to generalize the frequency part here into a language model.
So basically it's the difference given by the probabilistic model here to characterize what the user is looking for, versus the count of query words there.
And this difference allows us to plug in various different ways to estimate this.
So this can be estimated in many different ways, including using feedback information.
But this is called a KL-divergence, because this can be interpreted as matching the KL-divergence of two distributions.
One is the query model, denoted by this distribution.
One is the document language model here and smooth them with a collection language model, of course.
And we are not going to talk about the detail of that, and you'll find it in some references.
It's also called cross entropy because, in fact, we ignore some terms in the KL-divergence function and we will end up having actually cross entropy.
And both are terms of information theory.
But anyway, for our purposes here, you can just see the two formulas look almost identical, except that here we have a probability of a word given by a query language model.
And here the sum is over all the words that are in the document and also with the nonzero probability for the query model.
So it's kind of, again, a generalization of sum over all the matching query words.
Now you can also easily see we can recover the query likelihood retrieval function by simply setting this query model to the relative frequency of a word in the query.
This is very easy to see once you plug this into here you can eliminate this query length as a constant.
And then you will get exactly like that.
So you can see the equivalence.
And that's also why this KL-divergence model can be regarded as a generalization of query likelihood, because we can cover query likelihood as a special case.
But it would also allow us to do much more than that.
So this is how we can use the KL-divergence model to then do feedback.
The picture shows that we first estimate a document language model, then we estimate a query language model, and we compute the KL-divergence.
This is often denoted by a D here.
But this basically means this is exactly like the vector space model, because we compute a vector for the document, then compute another vector for the query, and then we compute the distance.
Only that these vectors are of special forms, they are probability distributions.
And then we get the results and we can find some feedback documents.
Let's assume they are mostly positive documents, although we could also consider both kinds of documents.
So what we could do is, like in Rocchio, we're going to compute another language model called the feedback language model here.
Again, this is going to be another vector just like the computing centroid of vector in Rocchio.
And then this model can be combined with the original query model using a linear interpolation, and this would then give us an update model, just like, again, in Rocchio.
So here we can see the parameter alpha can control the amount of feedback.
If it's set to zero, then essentially there is no feedback.
If it's set to one, we get full feedback and we ignore the original query.
And this is generally not desirable, right?
So unless you are absolutely sure you have seen a lot of relevant documents, then the query terms are not important.
So of course, the main question here is, how do you compute this theta F?
This is the big question here, and once you can do that, the rest is easy.
So here we will talk about one of the approaches, and there are many approaches, of course.
This approach is based on generative model, and I'm going to show you how it works.
This will use a generative mixture model.
So this picture shows that we have this model here, the feedback model that we want to estimate.
And the basis is the feedback documents.
Let's say we are observing the positive documents.
These are the clicked documents by users or random documents judged by users, or are simply top ranked documents that we assume to be relevant.
Now imagine how we can compute a centroid for these documents by using language model.
One approach is simply to assume these documents are generated from this language model.
As we did before, what we could do is just normalize the word frequency here to here and then we will get this word distribution.
Now the question is whether this distribution is good for feedback.
Well, you can imagine the top ranked word would be what?
What do you think?
Well, those words would be common words.
As we always see in a language model, the top ranked words are actually common words like the, a, etc.
So it's not very good for feedback, because we would be adding a lot of such words to our query when we interpolate this with the original query model.
So this was not good, so we need to do something.
In particular, we are trying to get rid of those common words.
And we have seen actually one way to do that by using background language model in the case of learning the associations of words, the words that are related to the word computer.
We could do that and that would be another way to do this, but here we are going to talk about another approach which is a more principled approach.
In this case, we're going to say well, you said that there are common words here in these documents that should not belong to this topic model, right?
So now what we can do is to assume that, well, those words are generated from background language model, so they will generate those words like the, for example.
And if we use maximum likelihood estimate, note that if all the words here must be generated from this model, then this model is forced to assign high probabilities to a word like the, because it occurs so frequently here.
Note that in order to reduce its probability in this model, we have to have another model, which is this one, to help explain the word the here.
And in this case, it's not appropriate to use the background language model to achieve this goal because this model would assign high probabilities to these common words.
So in this approach, then, we assume this machine that was generating these words would work as follows.
We have a source control up here.
Imagine we flip a coin here to decide what distribution to use.
With probability of lambda, the coin shows up as head and we're going to use the background language model.
And we're going to do that in sample word from that model.
With probability of 1 minus lambda, we're going to decide to use a known topic model, here, that we would like to estimate.
And we're going to then generate a word here.
If we make this assumption and this whole thing will be just one model, and we call this a mixture model because there are two distributions that are mixed together.
And we actually don't know when each distribution is used.
So again, think of this whole thing as one model, and we can still ask for words and it will still give us a word in a random manner.
And of course, which word will show up will depend on both this distribution and that distribution.
In addition, it would also depend on this lambda, because if you say lambda is very high and it's going to always use the background distribution, you will get different words.
Then if you say, well, lambda is very small, we're going to use this.
So all of these are parameters in this model.
And then if you're thinking this way, basically we can do exactly the same as what we did before.
We're going to use maximum likelihood estimator to adjust this model, to estimate the parameters.
Basically we're going to adjust this parameter so that we can best explain all the data.
The difference now is that we are not asking this model a known to explain this.
But rather we are going to ask this whole model, mixture model, to explain the data.
Because it has got some help from the background model, it doesn't have to assign high probabilities to words like the.
As a result, it will then assign higher probabilities to other words that are common here but not having high probability here.
So those would be common here.
And if they're common, they would have to have high probabilities, according to a maximum likelihood estimate method.
And if they are rare here, then you don't get much help from this background model.
As a result, this topic model must assign high probabilities.
So the high probability words, according to the topic model, would be those that are common here but rare in the background.
So this is basically a little bit like an idea of weighting here.
But this would allow us to achieve the effect of removing these topic words that are meaningless in the feedback.
So mathematically, what we have is to compute the likelihood, again, local likelihood, of the feedback documents.
And note that we also have another parameter, lambda here, but we assume that the lambda denotes the noise in the feedback document.
So we are going to, let's say set this to a parameter.
Let's say 50% of the words are noise or 90% are noise.
And this can then be assumed it will be fixed.
If we assume this is fixed, then we only have these probabilities as parameters, just like in the simple unigram language model.
We have n parameters, n is the number of words.
And then the likelihood function would look like this.
It's very similar to the global likelihood function we see before, except that inside the logarithm there's a sum here.
And this sum is because we consider two distributions.
And which one is used would depend on lambda, and that's why we have this form.
But mathematically, this is the function with theta as unknown variables.
So this is just a function.
All the other values are known except for this guy.
So we can then choose this probability distribution to maximize this log likelihood, the same idea as the maximum likelihood estimate as a mathematical problem.
We just have to solve this optimization problem.
We essentially would try all the theta values until we find one that gives this whole thing the maximum probability.
So it's a well-defined math problem.
Once we have done that, we obtain this theta F that can then be interpolated with original query model to the feedback.
So here are some examples of the feedback model learned from a web document collection.
And we do pseudo-feedback we just use the top ten documents and we use this mixture model.
So the query is airport security.
What we do is we first retrieve ten documents from the web database and this is of course pseudo-feedback.
And then we're going to feed that mixture model to this ten document set.
And these are the words learned using this approach.
This is the probability of a word given by the feedback model in both cases.
So in both cases you can see the highest probability words include the very relevant words to the query.
So airport security, for example, these query words still show up as high probabilities in each case naturally, because they occur frequently in the top ranked documents.
But we also see beverage, alcohol, bomb, terrorist, etc.
So these are relevant to this topic, and they, if combined with original query, can help us much more accurately on documents.
And also they can help us bring up documents that only mention some of these other words, maybe, for example, just airport and then bomb, for example.
So this is how pseudo-feedback works.
It shows that this model really works and picks up some related words to the query.
What's also interesting is that if you look at the two tables here and you compare them, then you'll see, in this case, when lambda is set to a small value, then we'll see some common words here.
And that means, well, we don't use the background model often.
Remember, lambda confuses the probability of using background model to generate the text.
If we don't rely much on background model, we still have to use this topic model to account for the common words.
Whereas if we set lambda to a very high value, we will use the background model very often to explain these words.
Then there's no burden on expanding those common words in the feedback documents by the topic model.
So as a result, the topic model here is very discriminative.
It contains all the relevant words without common words.
So this can be added to the original query to achieve feedback.
So to summarize, in this lecture we have talked about the feedback in language model approach.
In general, feedback is to learn from examples.
These examples can be assumed examples, can be pseudo-examples, like assume the top ten documents that are assumed to be relevant.
They could be based on user interactions, like feedback based on clickthroughs or implicit feedback.
We talked about the three major feedback scenarios, relevance feedback, pseudo feedback, and implicit feedback.
We talked about how to use Rocchio to do feedback in vector space model and how to use query model estimation for feedback in language model.
And we briefly talked about the mixture model and the basic idea.
There are many other methods.
For example, the relevance model is a very effective model for estimating query model.
So you can read more about these methods in the references that are listed at the end of this lecture.
So there are two additional readings here.
The first one is a book that has a systematic review and discussion of language models for information retrieval.
And the second one is a important research paper that's about relevance based language models, and it's a very effective way of computing query model.




Session 5.4: Web Search Introduction Web Crawler

This lecture is about Web Search.
In this lecture, we're going to talk about one of the most important applications of text retrieval, web search engines.
So let's first look at some general challenges and opportunities in web search.
Now, many informational retrieval algorithms had been developed before the web was born.
So when the web was born, it created the best opportunity to apply those algorithms to major application problem that everyone would care about.
So naturally, there have to be some further extensions of the classical search algorithms to address new challenges encountered in web search.
So here are some general challenges.
First, this is a scalability challenge.
How to handle the size of the web and ensure completeness of coverage of all information.
How to serve many users quickly and by answering all their queries.
And so that's one major challenge and before the web was born the scale search was relatively small.
The second problem is that there's no quality information and there are often spams.
The third challenge is Dynamics of the Web.
The new pages are constantly create and some pages may be updated very quickly, so it makes it harder to keep it indexed fresh.
So these are some of the challenges that we have to solve in order to deal with high quality web searching.
On the other hand there are also some interesting opportunities that we can leverage to include the search results.
There are many additional heuristics, for example, using links that we can leverage to improve scoring.
Now everything that we talked about such as the vector space model are general algorithms.
They can be applied to any search applications, so that's the advantage.
On the other hand, they also don't take advantage of special characteristics of pages or documents in the specific applications, such as web search.
Web pages are linked with each other, so obviously, the linking is something that we can also leverage.
So, because of these challenges and opportunities and there are new techniques that have been developed for web search or due to need for web search.
One is parallel indexing and searching and this is to address the issue of scalability.
In particular, Google's imaging of map reduce is very influential and has been very helpful in that aspect.
Second, there are techniques that are developing for addressing the problem of spams, so spam detection.
We'll have to prevent those spam pages from being ranked high.
And there are also techniques to achieve robust ranking.
And we're going to use a lot of signals to rank pages, so that it's not easy to spam the search engine with a particular trick.
And the third line of techniques is link analysis and these are techniques that can allow us to improve such results by leveraging extra information.
And in general in web searching, we're going to use multiple features for ranking not just for link analysis.
But also exploring all kinds of crawls like the layout or anchor text that describes a link to another page.
So, here's a picture showing the basic search engine technologies.
Basically, this is the web on the left and then user on the right side and we're going to help this user to get the access for the web information.
And the first component is a Crawler that would crawl pages and then the second component is Indexer that would take these pages create the inverted index.
The third component there is a Retriever and that would use inverted index to answer user's query by talking to the user's browser.
And then the search results will be given to the user and when the browser would show those results, it allows the user to interact with the web.
So, we're going to talk about each of these components.
First of all, we're going to talk about the crawler, also called a spider or software robot that would do something like crawling pages on the web.
To build a toy crawler is relatively easy, because you just need to start with a set of seed pages.
And then fetch pages from the web and parse these pages and figure out new links.
And then add them to the priority que and then just explore those additional links.
But to be able to real crawler actually is tricky and there are some complicated issues that we have to deal with.
For example robustness, what if the server doesn't respond, what if there's a trap that generates dynamically generated webpages that might attract your crawler to keep crawling on the same side and to fetch dynamic generated pages?
The results of this issue of crawling courtesy and you don't want to overload one particular server with many crawling requests and you have to respect the robot exclusion protocol.
You also need to handle different types of files, there are images, PDF files, all kinds of formats on the web.
And you have to also consider URL extension, so sometimes those are CGI scripts and there are internal references, etc, and sometimes you have JavaScripts on the page and they also create challenges.
And you ideally should also recognize redundant pages because you don't have to duplicate those pages.
And finally, you may be interested in the discover hidden URLs.
Those are URLs that may not be linked to any page, but if you truncate the URL to a shorter path, you might be able to get some additional pages.
So what are the Major Crawling Strategies?
In general, Breadth-First is most common because it naturally balances the sever load.
You would not keep probing a particular server with many requests.
Also parallel crawling is very natural because this task is very easy to parallelize.
And there is some variations of the crawling task, and one interesting variation is called a focused crawling.
In this case, we're going to crawl just some pages about a particular topic.
For example, all pages about automobiles, all right.
And this is typically going to start with a query, and then you can use the query to get some results from a major search engine.
And then you can start it with those results and then gradually crawl more.
The one channel in crawling, is you will find the new channels that people created and people probably are creating new pages all the time.
And this is very challenging if the new pages have not been actually linked to any old pages.
If they are, then you can probably find them by re-crawling the old pages, so these are also some interesting challenges that have to be solved.
And finally, we might face the scenario of incremental crawling or repeated crawling, right.
Let's say, if you want to build a web search engine, and you first crawl a lot of data from the web.
But then, once you have cracked all the data, in the future you just need to crawl the updated pages.
In general, you don't have to re-crawl everything, right?
It's not necessary.
So in this case, your goal is to minimize the resource overhead by using minimum resources to just the update pages.
So, this is actually a very interesting research question here, and this is a open research question, in that there aren't many standard algorithms established yet for doing this task.
But in general, you can imagine, you can learn, from the past experience.
So the two major factors that you have to consider are, first will this page be updated frequently?
And do I have to quote this page again?
If the page is a static page and that hasn't being changed for months, you probably don't have to re-crawl it everyday because it's unlikely that it will changed frequently.
On the other hand, if it's a sports score page that gets updated very frequently and you may need to re-crawl it and maybe even multiple times on the same day.
The other factor to consider is, is this page frequently accessed by users?
If it is, then it means that it is a high utility page and then thus it's more important to ensure such a page to refresh.
Compared with another page that has never been fetched by any users for a year, then even though that page has been changed a lot then.
It's probably not that necessary to crawl that page or at least it's not as urgent as to maintain the freshness of frequently accessed page by users.
So to summarize, web search is one of the most important applications of text retrieval and there are some new challenges particularly scalability, efficiency, quality information.
There are also new opportunities particularly rich link information and layout, etc.
A crawler is an essential component of web search applications and in general, you can find two scenarios.
One is initial crawling and here we want to have complete crawling of the web if you are doing a general search engine or focused crawling if you want to just target as a certain type of pages.
And then, there is another scenario that's incremental updating of the crawl data or incremental crawling.
In this case, you need to optimize the resource, try to use minimum resource to get the  



Session 5.5: Web Indexing

This lecture is about the Web Indexing.
In this lecture, we will continue talking about the Web Search and we're going to talk about how to create a Web Scale Index.
So once we crawl the web, we've got a lot of web pages.
The next step is to use the indexer to create the inverted index.
In general, we can use the same information retrieval techniques for creating an index and that is what we talked about in previous lectures, but there are there are new challenges that we have to solve.
For web scale indexing, and the two main challenges are scalability and efficiency.
The index would be so large, that it cannot actually fit into any single machine or single disk.
So we have to store the data on virtual machines.
Also, because the data is so large, it's beneficial to process the data in parallel, so that we can produce index quickly.
Now to address these challenges, Google has made a number of innovations.
One is the Google File System that's a general File system, that can help programmers manage files stored on a cluster of machines.
The second is MapReduce.
This is a general software framework for supporting parallel computation.
Hadoop is the most well known open source implementation of MapReduce.
Now used in many applications.
So, this is the architecture of the google file system.
It uses a very simple centralized management mechanism to manage all the specific locations of.
Files, so it maintains the file namespace and look up a table to know where exactly each file is stored.
The application client will then talk to this GFS master, and that obtains specific locations of the files they want to process.
And once the GFS file kind obtained the specific location about the files, then the application client can talk to the specific servers whether data actually sits directly, so you can avoid involving other node.
In the network.
So when this file system stores the files on machines, the system also with great fixed sizes of chunks, so the data files are separated into.
Many chunks.
Each chunk is 64 MB, so it's pretty big.
And that's appropriate for large data processing.
These chunks are replicated to ensure reliability.
So this is something that the programmer doesn't have to worry about, and it's all taken care of by this file system.
So from the application perspective, the programmer would see this as if it's a normal file.
And the programmer doesn't have to know where exactly it is stored and can just invoke high level.
Operators to process the file.
And another feature is that the data transfer is directly between application and chunk servers.
So it's efficient in this sense.
On top of the Google file system, Google also proposed MapReduce as a general framework for parallel programming.
Now, this is very useful to support a task like building inverted index.
And so, this framework is, Hiding a lot of low-level features from the program.
As a result, the programmer can make a minimum effort to create an application that can be run a large cluster in parallel.
So some of the low level details are hidden in the framework including the specific and network communications or load balancing or where the task are executed.
All these details are hidden from the programmer.
There is also a nice feature which is the built in fault tolerance.
If one server is broken, the server is down, and then some tasks may not be finished.
Then the MapReduce mapper will know that the task has not been done.
So it automatically dispatches a task on other servers that can do the job.
And therefore, again the program doesn't have to worry about that So here's how MapReduce works.
The input data would be separated into a number of key value pairs.
Now what exactly is in the value would depend on the data and it's actually a fairly general framework to allow you to just partition the data into different parts and each part can be then processed in parallel.
Each key value pair would be and send it to a map function.
The program was right the map function, of course.
And then the map function will process this Key Value pair and then generate a number of other Key Value pairs.
Of course, the new key is usually different from the old key that's given to the map as input.
And these key value pairs are the output of the map function and all the outputs of all the map functions would be then collected, and then there will be for the sorting based on the key.
And the result is that, all the values that are associated with the same key will be then grouped together.
So now we've got a pair of of a key and separate values attached to this key.
So this would then be sent to a reduce function.
Now, of course, each reduce function will handle a different key, so we will send these output values to multiple reduce functions each handling a unique key.
A reduce function would then process the input, which is a key in a set of values to produce another set of key values as the output.
So these output values would be then corrected together to form the final output.
And so, this is the general framework of MapReduce.
Now the programmer only needs to write the Map function and the Reduce function.
Everything else is actually taken care of by the MapReduce framework.
So you can see the program really only needs to do minimum work.
And with such a framework, the input data can be partitioned into multiple parts, which is processing parallel first by map, and then being the process after we reach the reduced stage.
The much more reduced if I'm  can also further process the different keys and their associated values in parallel.
So it achieves some, it achieves the purpose of parallel processing of a large data set.
So let's take a look at a simple example.
And that's Word Counting.
The input is containing words, and the output that we want to generate is the number of occurrences of each word.
So it's the Word Count.
We know this kind of counting would be useful to, for example, assess the popularity of a word in a large collection and this is useful for achieving a factor of IDF wading for search.
So how can we solve this problem?
Well, one natural thought is that, well this task can be done in parallel by simply counting different parts of the file in parallel, and then in the end we just combine all the counts.
And that's precisely the idea of what we can do with MapReduce.
We can parallelize on lines in this input file.
So more specifically, we can assume the input to each map function is a key value pair that represents the line number and the string on that line.
So the first line, for example, has a key of one and that is another word by word and just the four words on that line.
So this key value pair would be sent to a Map Function.
The Map Function then would just count the words in this line.
And in this case, of course there are only four words.
Each world gets a count of one and these are the output that you see here on this slide from this map function.
So the map function is really very simple if you look at what the pseudocode looks like on the right side, you see it simply needs to iterate all the words and this line.
And then just collect the function which means it would then send the word and the count to the collector.
The collector would then try to sort all these key value pairs from different Map Functions, right?
So the function is very simple and the programmer specifies this function as a way to process each part of the data.
Of course, the second line will be handled by a different Map Function which we will produce a single output.
Okay, now the output from the map functions will be then and send it to a collector and the collector would do the internal grouping or sorting.
So at this stage, you can see, we have collected a match for pairs.
Each pair is a word and its count in a line.
So, once we see all these pairs.
Then we can sort them based on the key, which is the word.
So we will collect all the counts of a word, like bye here, together.
And similarly, we do that for other words.
Like Hadoop, Hello, etc.
So each word now is attached to a number of values, a number of counts.
And these counts represent the occurrences to solve this word in different lights.
So now we have got a new pair of a key and a set of values, and this pair will then be fed into reduce function, so the reduce function now would have to finish the job of counting the total occurrences of this word.
Now, it has all ready got all these puzzle accounts, so all it needs to do is simply to add them up.
So the reduce function here is very simple, as well.
You have a counter, and then iterate all the other words.
That you'll see in this array.
And that, you just accumulate accounts, right?
And then finally, you output the P and the proto account.
And that's precisely what we want as the output of this whole program.
So you can see, this is all ready very similar to.
To building an Invert index.
And if you think about it, the output here is index.
And we have already got a dictionary, basically.
We have got the count.
But what's missing is the document the specific frequency counts of words in those documents.
So we can modify this slightly to actually be able to index in parallel, so here's one way to do that.
So in this case, we can assume the input from Map Function is a pair of a key which denotes the document ID, and the value denoting the screen for that document, so it's all the words in that document.
And so, the map function would do something very similar to what we have seen in the word campaign example.
It simply groups all the counts of this word in this document together.
And it would then generate a set of key value pairs.
Each key is a word, and the value is the count of this word in this document plus the document ID.
Now, you can easily see why we need to add document ID here, because later in inverted index, we would like to keep this formation, so the Map Function should keep track of it, and this can then be sent to the reduce function later.
Now similarly another document D2 can be processed in the same way.
So in the end, again, there is a sorting mechanism that would group them together.
And then we will have just a key, like a java, associated with all the documents that match this key.
Or all the documents where java occurred.
And the counts, so the counts of java in those documents.
And this will be collected together.
And this will be, so fed into the reduce function.
So now you can see the reduce function has already got input that looks like an inverted index entry.
So it's just the word and all the documents that contain the word and the frequencies of the word in those documents.
So all you need to do is simply to concatenate them into a continuous chunk of data.
And this can be done written to a file system.
So basically the reduce function is going to do very minimal.
Work.
And so, this is a pseudo-code for  that's construction.
Here we see two functions, procedure Map and procedure Reduce.
And a programmer would specify these two functions to program on top of map reduce.
And you can see basically they are doing what I just described.
In the case of map, it's going to count the occurrences of a word using the AssociativeArray.
And it would output all the counts together with the document ID here.
So, this is the reduce function, on the other hand, simply concatenates all the input that it has been given, and then put them together as one single entry for this key.
So this is a very simple MapReduce function, yet it would allow us to construct an inverted index at very large scale, and the data can be processed by different machines.
And program doesn't have to take care of the details.
So this is how we can do parallel index construction for web search.
So to summarize, web scale indexing requires some new techniques that go beyond the.
Standard traditional indexing techniques.
Mainly, we have to store index on multiple machines.
And this is usually done by using a filing system, like a Google file system.
But this should be through a file system.
And secondly, it requires creating an index an parallel, because it's so large and takes long time to create an index for all the documents.
So if we can do it in parallel, it will be much faster and this is done by using the MapReduce framework.
Note that both the GFS and MapReduce frameworks are very general, so they can also support many other applications.




Session 5.6: Link Analysis Part 1

This lecture is about link analysis for web search.
In this lecture, we're going to talk about the web search and particularly, focusing on how to do link analysis and use the results to improve search.
The main topic of this lecture is to look at the ranking algorithms for Web Search.
In the previous lecture we talked about how to create index.
Now that we have index, we want to see how we can improve ranking of Pages.
The web.
Now standard IR models, can be also applied here.
In fact, they are important building blocks, for, improve, for supporting web search.
But they aren't sufficient.
And mainly for the following reasons.
First, on the web, we tend to have very different information needs, for example, people might search for a webpage, or an entry page.
And this is different from the traditional library search, where people are primarily interested in collecting literature Information.
So this kind of query is often called a navigational queries.
The purpose is to navigate into a particular type of the page.
So for such queries we might benefit from using link information.
Secondly, documents have additional information and on the web pages, are web format, there are a lot of other clues, such as the layout, the title, or link information again.
So this has provided opportunity to use extra context information of the document to improve the scoring.
And finally, information quality varies a lot.
That means we have to consider many factors to improve the range in the algorithm.
This would give us a more robust way to rank pages, making it harder for any spammer to just manipulate the one signal to improve the ranking of a page.
So as a result, people have made a number of major extensions to the ranking algorithms.
One line is to exploit links to improve scoring.
And that's the main topic of this lecture.
People have also proposed algorithms to exploit the loudest, they are implicit.
Feedback information the form of click throughs and that's of course in the category of feedback techniques and machine all is often used there.
In general in web search the ranking algorithms are based on machine learning algorithms to combine all kinds of features.
Many of them are based on the standard of virtual models such as BM25 that we talked about  to score different parts of documents or to provide additional features based on content matching, but link information is also very useful so they provide additional scoring signals.
So let's look at links in more detail on the web.
So this is a snapshot of some part of the web, let's say.
So we can see there are many links that link the different pages together.
And in this case, you can also look at the center here, there is a description of a link that's pointing to the document on the right side.
Now, this description text is called anchor text.
Now if you think about this text, it's actually quite useful because it provides some extra description of that page be points with.
So for example, if someone wants to bookmark Amazon.com front page the person might say the biggest online bookstore and then the link to Amazon, right?
So, the description here after is very similar to what the user would type in the query box when they are looking for or such a page.
And that's why it's very useful for managing pages.
Suppose someone types in the query like online bookstore or biggest online bookstore.
All right the query would match this anchor text in the page here.
And then this actually provides evidence for matching the page that's being pointed to that is the Amazon.
a entry page.
So if you match anchor text that describes an anchor to a page, actually that provides good evidence for the elements of the page being pointed to.
So anchor text is very useful.
If you look at the bottom part of this picture you can also see there are some patterns of some links and these links might indicate the utility of a document.
So for example, on the right side you'll see this page has received the many inlinks.
Now that means many other pages are pointing to this page.
This shows that this page is quite useful.
On the left side you can see this is another page that points to many other pages.
So this is a director page that would allow you to actually see a lot of other pages.
So we can call the first case authority page and the second case half page, but this means the link information can help intuit.
One is to provide extra text for matching.
The other is to provide some additional scores for the webpage to characterize how likely a page is a hub, how likely a page is a authority.
So people then of course and proposed ideas to leverage this link information.
Google's PageRank which was the main technique that they used in early days is a good example and that is an algorithm to capture page and popularity, basically to score authority.
So the intuitions here are links are just like citations in literature.
Now think about one page pointing you to another page, this is very similar to one paper citing another paper.
So, of course then, if a page is cited often, then we can assume this page to be more useful in general.
So that's a very good intuition.
Now PageRank is essentially to take advantage of this Intuition to implement with the principal approach.
Intuitively, it is essentially doing citation counting or in link counting.
It just improves the simple idea in two ways.
One it will consider indirect citations.
So that means you don't just look at how many in links you have.
You also look at what are those pages that are pointing to you.
If those pages themselves have a lot of in-links, that means a lot.
In some sense, you will get some credit from that.
But if those pages that are pointing to you are not being pointed to by other pages they themselves don't have many in-links, then well, you don't get that much.
So that's the idea of getting indirect citation.
All right, so you can also understand this idea by looking at again the research papers.
If you're cited by let's say ten papers, and those ten papers are just workshop papers or some papers that are not very influential, right?
So although you've got ten in-links, and that's not as good as if you are cited by ten papers that themselves have attracted a lot of other citations.
And so in this case where we would like to consider indirect links and page does that.
The other idea is it's good to pseudo citations.
Assume that basically every page is having a number zero pseudo citation count.
Essentially you are trying to imagine there are many virtual links that will link all the pages together so that you actually get the pseudo citations from everyone.
The reason why they want to do that.
Is this will allow them to solve the problem elegantly with linear algebra technique.
So, I think maybe the best way to understand the PageRank is to think of this as through computer probability of random surfer visiting every webpage.




Session 5.7: Link Analysis Part 2

So let's take a look at this in detail.
So in this random surfing model at any page would assume random surfer would choose the next page to visit.
So this is a small graph here.
That's of course, over simplification of the complicated web.
But let's say there are four documents here, d1, d2, d3 and d4.
And let's assume that a random surfer or random walker can be any of these pages.
And then the random surfer could decide to, just randomly jumping to any page or follow a link and then visit the next page.
So if the random surfer is at d1, then there is some probability that random surfer will follow the links.
Now there are two outlinks here, one is pointing to d3, the other is pointing to d4.
So the random surfer could pick any of these two to reach d3 and d4.
But it also assumes that the random so far might get bore sometimes.
So the random surfing which decide to ignore the actual links and simply randomly jump into any page in the web.
So if it does that, it would be able to reach any of the other pages even though there's no link you actually, you want from that page.
So this is to assume that random surfing model.
Imagine a random surfer is really doing surfing like this, then we can ask the question how likely on average the surfer would actually reach a particular page like a d1, a d2, or a d3.
That's the average probability of visiting a particular page and this probability is precisely what a page ranker computes.
So the page rank score of the document is the average probability that the surfer visits a particular page.
Now intuitively, this would basically capture the inlink account, why?
Because if a page has a lot of inlinks, then it would have a higher chance of being visited.
Because there will be more opportunities of having the server to follow a link to come to this page.
And this is why the random surfing model actually captures the ID of counting the inlinks.
Note that it also considers the interacting links, why?
Because if the page is that point then you have themselves a lot of inlinks.
That would mean the random surfer would very likely reach one of them and therefore, it increase the chance of visiting you.
So this is just a nice way to capture both indirect and a direct links.
So mathematically, how can we compute this problem in a day in order to see that, we need to take a look at how this problem there is a computing.
So first of all let's take a look at the transition metrics here.
And this is just metrics with values indicating how likely the random surfer would go from one page to another.
So each rule stands for a starting page.
For example, rule one would indicate the probability of going to any of the other four pages from d1.
And here we see there are only 2 non 0 entries which is 1/2.
So this is because if you look at the graph d1 is pointing to d3 and d4.
There is no link from d1 or d2.
So we've got 0s for the first 2 columns and 0.5 for d3 and d4.
In general, the M in this matrix, M sub ij is the probability of going from di to dj.
And obviously for each rule, the values should sum to 1, because the surfer would have to go to precisely one of these other pages.
So this is a transition metric.
Now how can we compute the probability of a surfer visiting a page?
Well if you look at the surf model then basically, we can compute the probability of reaching a page as follows.
So here on the left hand side, you see it's the probability visiting page dj at time plus 1, so it's the next time point.
On the right hand side, you can see the equation involves the probability of at page di at time t.
So you can see the subscript in that t here, and that indicates that's the probability that the server was at a document at time t.
So the equation basically, captures the two possibilities of reaching at dj at the time t plus 1.
What are these two possibilities?
Well one is through random surfing and one is through following a link, as we just explained.
So the first part captures the probability that the random surfer would reach this page by following a link.
And you can see the random surfer chooses this strategy with probability 1 minus alpha as we assume.
And so there is a factor of 1 minus alpha here.
But the main party is realist sum over all the possible pages that the surfer could have been at time t.
There are n pages so it's a sum over all possible n pages.
Inside the sum is a product of two probabilities.
One is the probability that the surfer was at di at time t, that's p sub t of di.
The other is the transition probability from di to dj.
And so in order to reach this dj page, the surfer must first be at di at time t.
And then also, would also have to follow the link to go from di to dj.
So the probability is the probability of being at di at time t multiplied by the probability of going from that page to the target page, dj here.
The second part is a similar sum, the only difference is that now the transition probability is a uniform transition probability.
1 over n and this part of captures is the probability of reaching this page through random jumping.
So the form is exactly the same and this also allows us to see on why PageRank is essentially assumed a smoothing of the transition matrix.
If you think about this 1 over n as coming from another transition matrix that has all the elements being 1 over n in uniform matrix.
Then you can see very clearly essentially we can merge the two parts, because they are of the same form.
We can imagine there's a different metrics that's combination of this m and that uniform metrics where every m is 1 over n.
And in this sense PageRank uses this idea of smoothing and ensuring that there's no zero entry in such as transition matrix.
Now of course this is the time dependent the calculation of the probabilities.
Now we can imagine, if we'll compute the average of the probabilities, the average of probabilities probably with the sets of file this equation without considering the time index.
So let's drop the time index and just assume that they will be equal.
Now this would give us any equations, because for each page we have such equation.
And if you look at the what variables we have in these equations there are also precisely n variables.
So this basically means, we now have a system of n equations with n variables and these are linear equations.
So basically, now the problem boils down to solve this system of equations.
And here, I also show the equations in the metric form.
It's the vector p here equals a matrix or the transpose of the matrix here and multiplied by the vector again.
Now, if you still remember some knowledge that you've learned from linear algebra and then you will realize, this is precisely the equation for eigenvector.
When multiply the metrics by this vector, you get the same value as this matter and this can be solved by using iterative algorithm.
So because the equations here on the back are basically taken from the previous slide.
So you'll see the relation between the page that ran sports on different pages.
And this iterative approach or power approach, we simply start with s randomly initialized vector p.
And then we repeatedly just update this p by multiplying the metrics here by this p factor.
I also show a concrete example here.
So you can see this now.
If we assume alpha is 0.2, then with the example that we show here on the slide, we have the original transition matrix is here.
That includes the graph, the actual links and we have this smoothing transition metrics, uniform transition metrics representing random jumping.
And we can combine them together with a liner interpolation to form another metric that would be like this.
So essentially, we can imagine now the web looks like this and can be captured like that.
They're all virtual links between all the pages now.
The page we're on now would just initialize the p vector first and then just computed the updating of this p vector by using this metrics multiplication.
Now if you rewrite this metric multiplication in terms of individual equations, you'll see this.
And this is basically, the updating formula for this particular pages and page score.
So you can also see if you want to compute the value of this updated score for d1.
You basically multiply this rule by this column, and we'll take the third product of the two.
And that will give us the value for this value.
So this is how we updated the vector we started with an initial values for these guys for this.
And then we just revise the scores which generate a new set of scores and the updating formula is this one.
So we just repeatedly apply this and here it converges.
And when the matrix is like this, where there's no 0 values and it can be guaranteed to converge.
And at that point the we will just have the PageRank scores for all the pages.
We typically go to sets of initial values just to 1 over n.
So interestingly, this updating formula can be also interpreted as propagating scores on the graph, can you see why?
Or if you look at this formula and then compare that with this graph and can you imagine, how we might be able to interpret this as essentially propagating scores over the graph.
I hope you will see that indeed, we can imagine we have values initialized on each of these pages.
So we can have values here and say, that's a 1 over 4 for each.
And then we're going to use these metrics to update this the scores.
And if you look at the equation here this one, basically we're going to combine the scores of the pages that possibly would lead to reaching this page.
So we'll look at all the pages that are pointing to this page and then combine this score and propagate the sum of the scores to this document, d1.
To look at the scores that we present the probability that the random surfer would be visiting the other pages before it reached d1.
And then just do the propagation to simulate the probability of reaching this page, d1.
So there are two interpretations here.
One is just the matrix multiplication.
We repeat the multiplying that by these metrics.
The other is to just think of it as a propagating these scores repeatedly on the web.
So in practice, the combination of PageRank score is actually efficient.
Because the matrices is fast and there are some, ways we transform the equation.
So that you avoid actually literally computing the values for all those elements.
Sometimes you may also normalize the equation and that will give you a somewhat different form of the equation, but then the ranking of pages will not change.
The results of this potential problem of zero-outlink problem.
In that case, if a page does not have any outlink then the probability of these pages would not sum to 1.
Basically, the probability of reaching the next page from this page would not sum to 1, mainly because we have lost some probability to mass.
One would assume there's some probability that the surfer would try to follow the links, but then there is no link to follow.
And one possible solution is simply to use a page that is specific damping factor, and that could easily fix this.
Basically, that's to say alpha would be 1.0 for a page with no outlink.
In that case, the surfer would just have to randomly jump to another page instead of trying to follow a link.
There are many extensions of PageRank, one extension is to topic-specific PageRank.
Note that PageRank doesn't merely use the query information.
So we can make PageRank specific however.
So for example, at the top of a specific page you rank, we can simply assume when the surfer is bored.
The surfer is not randomly jumping to any page on the web.
Instead, he's going to jump to only those pages that are relevant to our query.
For example, if the query is not sports then we can assume that when it's doing random jumping, it's going to randomly jump to a sports page.
By doing this, then we can buy a PageRank through topic and sports.
And then if you know the current theory is about sports, and then you can use this specialized PageRank score to rank documents.
That would be better than if you use the generic PageRank score.
PageRank is also a channel that can be used in many other applications for network analysis particularly for example, social networks.
You can imagine if you compute the PageRank scores for social network, where a link might indicate a friendship or a relation, you would get some meaningful scores for people 



Session 5.8: Link Analysis Part 3 Optional

So we talked about PageRank as a way to capture the assault.
Now, we also looked at some other examples where a hub might be interesting.
So there is another algorithm called HITS, and that going to compute the scores for authorities and hubs.
The intuitions are pages that are widely cited are good authorities and whereas pages that cite many other pages are good hubs.
I think that the most interesting idea of this algorithm HITS, is it's going to use a reinforcement mechanism to kind of help improve the scoring for hubs and the authorities.
And so here's the idea, it was assumed that good authorities are cited by good hubs.
That means if you are cited by many pages with good hub scores then that inquiry says, you're an authority.
And similarly, good hubs are those that point at good authorities.
So if you pointed to a lot of good authority pages, then your hubs score would be increased.
So then you will have literally reinforced each other, because you have pointed so some good hubs.
And so you have pointed to some good authorities to get a good hubs score, whereas those authority scores would be also improved because they are pointing to by a good hub.
And this is algorithms is also general it can have many applications in graph and network analysis.
So just briefly, here's how it works.
We first also construct a matrix, but this time we're going to construct an adjacent matrix and we're not going to normalize the values.
So if there's a link there's a 1, if there's no link that's 0.
Again, it's the same graph.
And then we're going to define the hubs score of page as the sum of the authority scores of all the pages that it appoints to.
So whether you are hub, really depends on whether you are pointing to a lot of good authority pages.
That's what it says in the first equation.
In the second equation, we define the authorities of a page as a sum of the hub scores of all those pages that appoint to you.
So whether you are good authority would depend on whether those pages that are pointing to you are good hubs.
So you can see this forms iterative reinforcement mechanism.
Now, these three questions can be also written in the metrics format.
So what we get here is then the hub vector is equal to the product of the adjacency matrix and the authority vector, and this is basically the first equation.
And similarly, the second equation can be returned as the authority vector is equal to the product of a transpose multiplied by the hub vector.
Now, these are just different ways of expressing these equations.
But what's interesting is that if you look at the matrix form, you can also plug in the authority equation into the first one.
So if you do that, you have actually eliminated the authority vector completely and you get the equations of only hubs scores.
The hubs score vector is equal to a multiplied by a transpose multiplied by the hub score again.
Similarly, we can do a transformation to have equation for just the authorities also.
So although we frame the problem as computing hubs and authorities, we can actually eliminate one of them to obtain equation just for one of them.
Now, the difference between this and page random is that now the matrix is actually a multiplication of the adjacency matrix and it's transpose.
So this is different from page rank.
But mathematically, then we will be computing the same problem.
So in HITS, we typically would initialize the values.
Let's say, 1 for all these values, and then we would iteratively apply these equations, essentially.
And this is equivalent to multiply that by the metrics a and a transpose.
So the arrows of these is exactly the same in the PageRank.
But here because the adjacency matrix is not normalized.
So what we have to do is after each iteration we're going to normalize, and this would allow us to control the growth of value.
Otherwise they would grow larger and larger.
And if we do that, and that will basically get HITS.
That was the computer, the hubs scores, and authority scores for all the pages.
And these scores can then be used in branching just like the PageRank scores.
So to summarize in this lecture, we have seen that link information's very useful.
In particular, the anchor text is very useful to increase the text representation of a page.
And we also talk about the PageRank and page anchor as two major link analysis algorithms.
Both can generate scores for web pages that can be used in the ranking function.
Note that PageRank and the HITS are also very general algorithms.
So they have many applications in analyzing other graphs or networks.




Session 6.1: Learning To Rank Part 1 Optional

This lecture is about the Learning to Rank.
In this lecture, we are going to continue talking about web search.
In particular we're going to talk about the using machine learning to combine different features to improve the ranking function.
So the question that we address in this lecture is how we can combine many features to generate a single ranking function to optimize search results?
In the previous lectures we have talked about a number of ways to rank documents.
We have talked about some retrieval models like a BM25 or Query Light Code.
They can generate a based this course for matching documents with a query.
And we also talked about the link based approaches like page rank that can give additional scores to help us improve ranking.
Now the question now is, how can we combine all these features and potentially many other features to do ranking?
And this will be very useful for ranking webpages, not only just to improve accuracy, but also to improve the robustness of the ranking function.
So that it's not easy for a spammer to just perturb a one or a few features to promote a page.
So the general idea of learning to rank is to use machine learning to combine this features to optimize the weights on different features to generate the optimal ranking function.
So we will assume that the given a query document pair Q and D, we can define a number of features.
And these features can vary from content based features such as a score of the document with respect to the query according to a retrieval function such as BM25 or Query Light Hold of punitive commands from a machine or PL2 etcetera.
It can also be a link based score like or page rank score like.
It can be also application of retrieval models to the ink text of the page.
Those are the types of descriptions of links that point to this page.
So, these can all the clues whether this document is relevant, or not.
We can even include a feature such as whether the URL has a tilde because this might be indicator of home page or entry page.
So all these features can then be combined together to generate a ranking function.
The question is, of course.
How can we combine them?
In this approach, we simply hypothesize that the probability that this document isn't relevant to this query is a function of all these features.
So we can hypothesize this that the probability of relevance is related to these features through a particular form of the function that has some parameters.
These parameters can control the influence of different features of the final relevance.
Now this is of course just an assumption.
Whether this assumption really makes sense is a big question and that's they have to empirically evaluate the function.
But by hypothesizing that the relevance is related to these features in the particular way, we can then combine these features to generate the potential more powerful ranking function, a more robust ranking function.
Naturally the next question is how do we estimate those parameters?
How do we know which features should have a higher weight, and which features will have lower weight?
So this is the task of training or learning, so in this approach what we will do is use some training data.
Those are the data that have been charted by users so that we already know the relevant judgments.
We already know which documents should be ranked high for which queries.
And this information can be based on real judgments by users or this can also be approximated by just using click through information, where we can assume the clicked documents are better than the skipped documents clicked documents are relevant and the skipped documents are non-relevant.
So in general with the fit such hypothesize ranking function to the training data meaning that we will try to optimize it's retrieval accuracy on the training data.
And we can adjust these parameters to see how we can optimize the performance of the functioning on the training data in terms of some measures such as MAP or NDCG.
So the training date would look like a table of tuples.
Each tuple has three elements, the query, the document, and the judgement.
So it looks very much like our relevance judgement that we talked about in the evaluation of retrieval systems.




Session 6.2: Learning To Rank Part 2 Optional

So now let's take a look at the specific method that's based on regression.
Now, this is one of the many different methods, and in fact, it's one of the simplest methods.
And I choose this to explain the idea because it's simple.
So in this approach, we simply assume that the relevance of document with respect to a query is related to a linear combination of all the features.
Here I used Xi to denote the feature.
So Xi of Q and D is a feature.
And we can have as many features as we would like.
And we assume that these features can be combined in a linear manner.
And each feature is controlled by a parameter here, and this beta i is a parameter.
That's a weighting parameter.
A larger value would mean the feature would have a higher weight, and it would contribute more to the scoring function.
This specific form of the function actually also involves a transformation of the probability of relevance.
So this is the probability of relevance.
And we know that the probability of relevance is within the range from 0 to 1.
And we could have just assumed that the scoring function is related to this linear combination.
So we can do a linear regression.
But then, the value of this linear combination could easily go beyond 1.
So this transformation here would map the 0 to 1 range to the whole range of real values, you can verify it by yourself.
So this allows us then to connect to the probability of variance which is between 0 and 1 to a linear combination of arbitrary features.
And if we rewrite this into a probability function, we would get the next one.
So on this equation, now we'll have the probability of relevance.
And on the right hand side, we'll have this form.
Now, this form is clearly nonnegative, and it still involves a linear combination of features.
And it's also clear that if this value is, this is actually negative of the linear combination in the equation above.
If this value here is large, then it would mean this value is small.
And therefore, this whole probability would be large.
And that's we expect, that basically, it would mean if this combination gives us a high value, then the document's more likely irrelevant.
So this is our hypothesis.
Again, this is not necessarily the best hypothesis, but this is a simple way to connect these features with the probability of relevance.
So now we have this combination function.
The next task is to estimate the parameters so that the function cache will be applied.
But without knowing the beta values, it's harder to apply this function.
So let's see how can estimate our beta values.
All right, let's take a look at a simple example.
In this example, we have three features.
One is the BM25 score of the document and the query.
One is the PageRank score of the document, which might or might not depend on the query.
We might have a topic-sensitive PageRank, that would depend on the query.
Otherwise, the general PageRank doesn't really depend on the query.
And then we have BM25 score on the anchor test of the document.
Now, these are then the feature values for a particular document query pair.
And in this case, the document is D1 and the judgment says that it's relevant.
Here's another training instance and it's these feature values, but in this case, it's not relevant.
This is an oversimplified case where we just have two instances, but it's sufficient to illustrate the point.
So what we can do is we use the maximum likelihood estimator to actually estimate the parameters.
Basically, we're going to predict the relevance status of the document based on the feature values.
That is, given that we observed these feature values here.
Can we predict the relevance here?
Now, of course, the prediction would be using this function that you see here.
And we hypothesize that the probability of relevance is related to features in this way.
So we are going to see, for what values of beta we can predict the relevance well.
What do we mean by predicting the relevance well?
Well, we just mean, in the first case, for D1 this expression right here should give high values.
In fact, we'll hope this to gave a value close to 1.
Why?
Because this is a relevant document.
On the other hand, in the second case, for D2, we hope this value will be small, right.
Why?
Because it's a non-relevant document.
So now let's see how this can be mathematically expressed.
And this is similar to expressing the probability of document, only that we are not talking about the probability of words, but talking about the probability of relevance, 1 or 0.
So what's the probability of this document being relevant if it has these feature values?
Well, this is just this expression.
We just need to plug in the Xi's.
So that's what we will get.
It's exactly like what we have seen above, only that we replaced these Xi's with now specific values.
So for example, this 0.7 goes to here and this 0.11 goes to here.
And these are different feature values, and we combine them in this particular way.
The beta values are still unknown.
But this gives us the probability that this document is relevant, if we assume such a model.
Okay?
And we want to maximize this probability, since this is a relevant document.
What do we do for the second document?
Well, we want to compute the probability that the prediction is non-relevant.
So this would mean we have to compute 1 minus this expression, since this expression is actually the probability of relevance.
So to compute the non-relevance from relevance, we just do 1 minus the probability of relevance.
Okay?
So this whole expression then just is our probability of predicting these two relevance values.
One is 1 here, one is 0.
And this whole equation is our probability of observing a 1 here and observing a 0 here.
Of course, this probability depends on the beta values.
So then our goal is to adjust the beta values to make this whole thing reach its maximum, make it as large as possible.
So that means we're going to compute this.
The beta is just the parameter values that would maximize this whole likelihood expression.
And what it means is, if you look at the function, is, we're going to choose betas to make this as large as possible and make this also as large as possible, which is equivalent to say, make this part as small as possible.
And this is precisely what we want.
So once we do the training, now we will know the beta values.
So then this function would be well-defined.
Once beta values are known, both this and this would be completely specified.
So for any new query and new document, we can simply compute the features for that pair.
And then we just use this formula to generate the ranking score.
And this scoring function can be used to rank documents for a particular query.
So that's the basic idea of learning to rank.




Session 6.3: Learning To Rank Part 3 Optional

There are many more of the Munster learning algorithms than the regression based approaches and they generally attempt to direct the optimizer retrieval method.
Like a MAP or nDCG.
Note that the optimization object or function that we have seen on the previous slide is not directly related to the retrieval measure.
By maximizing the prediction of one or zero, we don't necessarily optimize the ranking of those documents.
One can imagine that our prediction may not be too bad.
And let's say both are around 0.5.
So it's kind of in the middle of zero and one for the two documents.
But the ranking can be wrong, so we might have a larger value for E2 and then E1.
So that won't be good from retrieval perspective, even though function, it's not bad.
In contrast, we might have another case where we predicted the values, or around the 0.9, it said.
And by the objective function, the error would be larger.
But if we didn't get the order of the two documents correct, that's actually a better result.
So these new, more advanced approaches will try to correct that problem.
Of course, then the challenge is that the optimization problem will be harder to solve.
And then, researchers have posed many solutions to the problem, and you can read more of the references at the end, know more about these approaches.
Now, these learning ranked approaches after the general.
So there accounts would be be applied with many other ranking problems, not just the retrieval problem.
So some people will go with recommender systems, computational advertising, or summarization and there are many others that you can probably encounter in your applications..
To summarize this lecture we have talked about using machine learning to combine much more features including ranking results.
Actually the use of machine learning in information retrieval has started since many decades ago.
So for example, the Rocchio feedback approach that we talked about earlier was a machine learning approach prior to relevance feedback.
But the most recent use of machine learning has been driven by some changes in the environment of applications of retrieval systems.
First, it's mostly freedom of availability of a lot of training data in the form of critical, such as they are more available than before.
So the data can provide a lot of useful knowledge about relevance and machine learning methods can be applied into a leverage list.
Secondly, it's also freedom by the need for combining many features, and this is not only just because there are more features available on the web that can be naturally used for improved scoring.
It's also because by combining them, we can improve the robustness of ranking, so this is desired for combating spams.
Modern search engines all use some kind of machine learning techniques to combine many features to optimize ranking and this is a major feature of these commercial engines such a Google or Bing.
The topic of learning to rank is still active research topic in the community, and so we can expect to see new results in development in the next few years, perhaps.
Here are some additional readings that can give you more information about how learning to rank at works and also some advanced methods.




Session 6.4: Future Of Web Search

This lecture is about the future of web search.
In this lecture, we're going to talk about some possible future trends of web search and intelligent information retrieval systems in general.
In order to further improve the accuracy of a search engine, it's important that to consider special cases of information need.
So one particular trend could be to have more and more specialized than customized search engines, and they can be called vertical search engines.
These vertical search engines can be expected to be more effective than the current general search engines because they could assume that users are a special group of users that might have a common information need, and then the search engine can be customized with this ser, so, such users.
And because of the customization, it's also possible to do personalization.
So the search can be personalized, because we have a better understanding of the users.
Because of the restrictions with domain, we also have some advantages in handling the documents, because we can have better understanding of documents.
For example, particular words may not be ambiguous in such a domain.
So we can bypass the problem of ambiguity.
Another trend we can expect to see, is the search engine will be able to learn over time.
It's like a lifetime learning or lifelong learning, and this is, of course, very attractive because that means the search engine will self-improve itself.
As more people are using it, the search engine will become better and better, and this is already happening, because the search engines can learn from the  of feedback.
More users use it, and the quality of the search engine allows for the popular queries that are typed in by many users allow it to become better, so this is sort of another feature that we will see.
The third trend might be to the integration of bottles of information access.
So search, navigation, and recommendation or filtering might be combined to form a full-fledged information management system.
And in the beginning of this course, we talked about push versus pull.
These are different modes of information access, but these modes can be combined.
And similarly, in the pull mode, querying and the browsing could also be combined.
And in fact we're doing that basically, today, is the  search endings.
We are querying, sometimes browsing, clicking on links.
Sometimes we've got some information recommended.
Although most of the cases the information recommended is because of advertising.
But in the future, you can imagine seamlessly integrate the system with multi-mode for information access, and that would be convenient for people.
Another trend is that we might see systems that try to go beyond the searches to support the user tasks.
After all, the reason why people want to search is to solve a problem or to make a decision or perform a task.
For example consumers might search for opinions about products in order to purchase a product, choose a good product by, so in this case it would be beneficial to support the whole workflow of purchasing a product, or choosing a product.
In this era, after the common search engines already provide a good support.
For example, you can sometimes look at the reviews, and then if you want to buy it, you can just click on the button to go the shopping site and directly get it done.
But it does not provide a, a good task support for many other tasks.
For example, for researchers, you might want to find the realm in the literature or site of the literature.
And then, there's no, not much support for finishing a task such as writing a paper.
So, in general, I think, there are many opportunities in the wait.
So in the following few slides, I'll be talking a little bit more about some specific ideas or thoughts that hopefully, can help you in imagining new application possibilities.
Some of them might be already relevant to what you are currently working on.
In general, we can think about any intelligent system, especially intelligent information system, as we specified by these these three nodes.
And so if we connect these three into a triangle, then we'll able to specify an information system.
And I call this Data-User-Service Triangle.
So basically the three questions you ask would be who are you serving and what kind of data are you are managing and what kind of service you provide.
Right there, this would help us basically specify in your system.
And there are many different ways to connect them depending on how you connect them, you will have a different kind of systems.
So let me give you some examples.
On the top, you can see different kinds of users.
On the left side, you can see different types of data or information, and on the bottom, you can see different service functions.
Now imagine you can connect all these in different ways.
So, for example, you can connect everyone with web pages, and the support search and browsing, what do you get?
Well, that's web search, right?
What if we connect UIUC employees with organization documents or enterprise documents to support the search and browsing, but that's enterprise search.
If you connect the scientist with literature information to provide all kinds of service, including search, browsing, or alert of new random documents or mining analyzing research trends, or provide the task with support or decision support.
For example, we might be, might be able to provide a support for automatically generating related work section for a research paper, and this would be closer to task support.
Right?
So then we can imagine this would be a literature assistant.
If we connect the online shoppers with blog articles or product reviews then we can help these people to improve shopping experience.
So we can provide, for example data mining capabilities to analyze the reviews, to compare products, compare sentiment of products and to provide task support or decision support to have them choose what product to buy.
Or we can connect customer service people with emails from the customers, and, and we can imagine a system that can provide a analysis of these emails to find that the major complaints of the customers.
We can imagine a system we could provide task support by automatically generating a response to a customer email.
Maybe intelligently attach also a promotion message if appropriate, if they detect that that's a positive message, not a complaint, and then you might take this opportunity to attach some promotion information.
Whereas if it's a complaint, then you might be able to automatically generate some generic response first and tell the customer that he or she can expect a detailed response later, etc.
All of these are trying to help people to improve the productivity.
So this shows that the opportunities are really a lot.
It's just only restricted by our imagination.
So this picture shows the trend of the technology, and also, it characterizes the, intelligent information system in three angles.
You can see in the center, there's a triangle that connects keyword queries to search a bag of words representation.
That means the current search engines basically provides search support to users and mostly model users based on keyword queries and sees the data through bag of words representation.
So it's a very simple approximation of the actual information in the documents.
But that's what the current system does.
It connects these three nodes in such a simple way, or it only provides a basic search function and doesn't really understand the user, and it doesn't really understand that much information in the documents.
Now, I showed some trends to push each node toward a more advanced function.
So think about the user node here, right?
So we can go beyond the keyword queries, look at the user search history, and then further model the user completely to understand the, the user's task environment, task need context or other information.
Okay, so this is pushing for personalization and complete user model.
And this is a major direction in research in, in order to build intelligent information systems.
On the document side, we can also see, we can go beyond bag of words implementation to have entity relation representation.
This means we'll recognize people's names, their relations, locations, etc.
And this is already feasible with today's natural processing technique.
And Google is the reason the initiative on the knowledge graph.
If you haven't heard of it, it is a good step toward this direction.
And once we can get to that level without initiating robust manner at larger scale, it can enable the search engine to provide a much better service.
In the future we would like to have knowledge representation where we can add perhaps inference rules, and then the search engine would become more intelligent.
So this calls for large-scale semantic analysis, and perhaps this is more feasible for vertical search engines.
It's easier to make progress in the particular domain.
Now on the service side, we see we need to go beyond the search of support information access in general.
So search is only one way to get access to information as well recommender systems and push and pull so different ways to get access to random information.
But going beyond access, we also need to help people digest the information once the information is found, and this step has to do with analysis of information or data mining.
We have to find patterns or convert the text information into real knowledge that can be used in application or actionable knowledge that can be used for decision making.
And furthermore the knowledge will be used to help a user to improve productivity in finishing a task, for example, a decision-making task.
Right, so this is a trend.
And, and, and so basically, in this dimension, we anticipate in the future intelligent information systems will provide intelligent and interactive task support.
Now I should also emphasize interactive here, because it's important to optimize the combined intelligence of the users and the system.
So we, we can get some help from users in some natural way.
And we don't have to assume the system has to do everything when the human, user, and the machine can collaborate in an intelligent way, an efficient way, then the combined intelligence will be high and in general, we can minimize the user's overall effort in solving problem.
So this is the big picture of future intelligent information systems, and this hopefully can provide us with some insights about how to make further innovations on top of what we handled today.




Session 6.5: Recommender Systems Content Based Filtering Part 1

This lecture is about the Recommender Systems.
So far we have talked about a lot of aspects of search engines.
We have talked about the problem of search and ranking problem, different methods for ranking, implementation of search engine and how to evaluate a search engine, etc.
This is important because we know that web search engines are by far the most important applications of text retrieval.
And they are the most useful tools to help people convert big raw text data into a small set of relevant documents.
Another reason why we spend so many lectures on search engines, is because many techniques used in search engines are actually also very useful for Recommender Systems, which is the topic of this lecture.
And so, overall, the two systems are actually well connected.
And there are many techniques that are shared by them.
So this is a slide that you have seen before, when we talked about the two different modes of text access.
Pull and the Push.
And we mentioned that recommender systems are the main systems to serve users in the Push Mode, where the systems will take the initiative to recommend the information to the user or pushes information to the user.
And this often works well when the user has stable information need in the system has a good.
So a Recommender System is sometimes called a filtering system and it's because recommending useful items to people is like discarding or filtering out the the useless articles, and so in this sense they are kind of similar.
And in all the cases the system must make a binary decision and usually there's a dynamic source of information items, and that you have some knowledge about the users' interest.
And then the system would make a decision about whether this item is interesting to the user, and then if it's interesting then the system would recommend the article to the user.
So the basic filtering question here is really will this user like this item?
Will U like item X?
And there are two ways to answer this question, if you think about it.
And one is look at what items U likes and then we can see if X is actually like those items.
The other is to look at who likes X, and we can see if this user looks like a one of those users, or like most of those users.
And these strategies can be combined.
If we follow the first strategy and look at item similarity in the case of recommending text objects, then we're talking about a content-based filtering or content-based recommendation.
If we look at the second strategy, then, it's to compare users and in this case we're user similarity and the technique is often called collaborative filtering.
So, let's first look at the content-based filtering system.
This is what the system would look like.
Inside the system, there will be a Binary Classifier that would have some knowledge about the user's interests, and this is called a User Interest Profile.
It maintains this profile to keep track of all users interests, and then there is a utility function to guide the user to make decision a nice plan utility function in the moment.
It helps the system decide where to set the threshold.
And then the accepted documents will be those that have passed the threshold according to the classified.
There should be also an initialization module that would take a user's input, maybe from a user's specified keywords or chosen category, etc., and this would be to feed into the system with the initiator's profile.
There is also typically a learning module that would learn from users' feedback over time.
Now note that in this case typical users information is stable so the system would have a lot more opportunities to observe the users.
If the user has taken a recommended item, has viewed that, and this a signal to indicate that the recommended item may be relevant.
If the user discarded it, no, it's not relevant.
And so such feedback can be a long term feedback, and can last for a long time.
And the system can collect a lot of information about the user's interest and this then can then be used to improve the classify.
Now what's the criteria for evaluating such a system?
How do we know this filtering system actually performs well?
Now in this case we cannot use the ranking evaluation measures like a map because we can't afford waiting for a lot of documents and then rank the documents to make a decision for the users.
And so the system must make a decision in real time in general to decide whether the item is above the threshold or not.
So in other words, we're trying to decide on absolute relevance.
So in this case, one common user strategy is to use a utility function to evaluate the system.
So here, I show linear utility function.
That's defined as for example three multiplied the number of good items that you delivered, minus two multiplied by the number of bad items that you delivered.
So in other words, we could kind of just treat this as almost in a gambling game.
If you delete one good item, let's say you win three dollars, you gain three dollars but if you deliver a bad one you will lose two dollars.
And this utility function basically kind of measures how much money you are get by doing this kind of game, right?
And so it's clear that if you want to maximize this utility function, this strategy should be delivered as many good articles as possible, and minimize the delivery of bad articles.
That's obvious, right?
Now one interesting question here is how should we set these coefficients?
I just showed a three and negative two as possible coefficients.
But one can ask the question, are they reasonable?
So what do you think?
Do you think that's a reasonable choice?
What about the other choices?
So for example, we can have 10 and minus 1, or 1, minus 10.
What's the difference?
What do you think?
How would this utility function affect the systems' threshold of this issue.
Right, you can think of these two extreme cases.
(10, -1) + (1, -10), which one do you think would encourage this system to over do it and which one would encourage this system to be conservative?
If you think about it you will see that when we get a bigger award for delivering our good document you incur only a small penalty for delivering a bad one.
Intuitively, you would be encouraged to deliver more.
And you can try to deliver more in hope of getting a good one delivered.
And then we'll get a big reward.
So on the other hand, if you choose (1,-10), you really don't get such a big prize if you deliver a good document.
On the other hand, you will have a big loss if you deliver a bad one.
You can imagine that, the system would be very reluctant to deliver a lot of documents.
It has to be absolutely sure that it's not.
So this utility function has to be designed based on a specific application.
The three basic problems in content-based filtering are the following, first, it has to make a filtering decision.
So it has to be a binary decision maker, a binary classifier.
Given a text document and a profile description of the user, it has to say yes or no, whether this document should be deleted or not.
So that's a decision module, and it should be an initialization module as you have seen earlier and this will get the system started.
And we have to initialize the system based on only very limited text exclusion or very few examples from the user.
And the third model is a learning model which you have, has to be able to learn from limited relevance judgements, because we counted them from the user about their preferences on the deliver documents.
If we don't deliver document to the user we'll never be able to know whether the user likes it or not.
And we had accumulate a lot of documents even then from entire history.
All these modules will have to be optimized to maximize the utility.
So how can we deal with such a system?
And there are many different approaches.
Here we're going to talk about how to extend a retrieval system, a search engine for information filtering.
Again, here's why we've spent a lot of time talking about the search engines.
Because it's actually not very hard to extend the search engine for information filtering.
So here's the basic idea for extending a retrieval system for information filtering.
First, we can reuse a lot of retrieval techniques to do scoring.
Right, so we know how to score documents against queries, etc.
We're going to match the similarity between profile text description and a document.
And then we can use a score threshold for the filtering decision.
We do retrieval and then we kind of find the scores of documents and then we'll apply a threshold to see whether the document is passing the threshold or not.
And if it's passing the threshold, we're going to say it's relevant and we're going to deliver it to the user.
Another component that we have to add is, of course, to learn from the history, and we had used is the traditional feedback techniques to learn to improve scoring.
And we know rock hill can be using for scoring improvement.
And, but we have to develop a new approaches to learn how to accept this.
And we need to set it initially and then we have to learn how to update the threshold over time.
So here's what the system might look like if we just generalize the vector-space model for filtering problems, right?
So you can see the document vector could be fed into a scoring module which already exists in a search engine that implements a vector-space model.
And the profile will be treated as a query essentially, and then the profile vector can be matched with the document vector to generate the score.
And then this score would be fed into a thresholding module that would say yes or no, and then the evaluation would be based on the utility for the filtering results.
If it says yes and then the document would be sent to the user.
And then user could give some feedback.
The feedback information would be used to both adjust the threshold and to adjust the vector representation.
So the vector learning is essentially the same as query modification or feedback in the case of search.
The threshold of learning is a new component and that we need to talk a little bit more about.




Session 6.6: Recommender Systems Content Based Filtering Part 2

There are some interesting challenges in threshold for the learning the filtering problem.
So here I show the historical data that you can collect in the filtering system, so you can see the scores and the status of relevance.
So the first one has a score of 36.5 and it's relevant.
The second one is not relevant and it's separate.
Of course, we have a lot of documents for which we don't know the status, because we have never delivered them to the user.
So as you can see here, we only see the judgements of documents delivered to the user.
So this is not a random sample, so it's a sensitive data.
It's kind of biased, so that creates some difficultly for learning.
Secondly, there are in general very little labeled data and very few relevant data, so it's also challenging for machine learning approaches, typically they require more training data.
And in the extreme case at the beginning we don't even have any labeled data as well.
The system there has to make a decision, so that's a very difficult problem at the beginning.
Finally, there is also this issue of exploration versus exploitation tradeoff.
Now, this means we also want to explore the document space a little bit and to see if the user might be interested in documents that we have in data labeled.
So in other words, we're going to explore the space of user interests by testing whether the user might be interested in some other documents that currently are not matching the user's interests so well.
So how do we do that?
Well, we could lower the threshold a little bit until we just deliver some near misses to the user to see what the user would respond, to see how the user would respond to this extra document.
And this is a tradeoff, because on the one hand, you want to explore, but on the other hand, you don't want to really explore too much, because then you will over deliver non-relevant Information.
So exploitation means you would exploit what you learn about the user.
Let's say you know the user is interested in this particular topic, so you don't want to deviate that much, but if you don't deviate at all then you don't exploit so that's also are not good.
You might miss opportunity to learn another interest of the user.
So this is a dilemma.
And that's also a difficulty problem to solve.
Now, how do we solve these problems?
In general, I think one can use the empirical utility optimization strategy.
And this strategy is basically to optimize the threshold based on historical data, just as you have seen on the previous slide.
Right, so you can just compute the utility on the training data for each candidate score threshold.
Pretend that, what if I cut at this point.
What if I cut at the different scoring threshold point, what would happen?
What's utility?
Since these are training data, we can kind of compute the utility, and we know that relevant status, or we assume that we know relevant status based on approximation of click-throughs.
So then we can just choose the threshold that gives the maximum utility on the training data.
But this of course, doesn't account for exploration that we just talked about.
And there is also the difficulty of biased training sample, as we mentioned.
So, in general, we can only get the upper bound for the true optimal threshold, because the threshold might be actually lower than this.
So, it's possible that this could discarded item might be actually interesting to the user.
So how do we solve this problem?
Well, we generally, and as I said we can low with this threshold to explore a little bit.
So here's on particular approach called beta-gamma threshold learning.
So the idea is falling.
So here I show a ranked list of all the training documents that we have seen so far, and they are ranked by their positions.
And on the y axis we show the utility, of course, this function depends on how you specify the coefficients in the utility function, but we can then imagine, that depending on the cutoff position, we will have a utility.
Suppose I cut at this position and that would be a utility.
For example, identify some cutting cutoff point.
The optimal point, theta optimal, is the point when it will achieve the maximum utility if we had chosen this as threshold.
And there is also zero utility threshold.
You can see at this cutoff the utility is zero.
What does that mean?
That means if I lower the threshold a little bit, now I reach this threshold.
The utility would be lower but it's still non-active at least, right?
So it's not as high as the optimal utility.
But it gives us as a safe point to explore the threshold, as I have explained, it's desirable to explore the interest of space.
So it's desirable to lower the threshold based on your training there.
So that means, in general, we want to set the threshold somewhere in this range.
Let's say we can use the alpha to control the deviation from the optimal utility point.
So you can see the formula of the threshold would be just the interpolation of the zero utility threshold and the optimal utility threshold.
Now, the question is, how should we set alpha?
And when should we deviate more from the optimal utility point?
Well, this can depend on multiple factors, and the one way to solve the problem is to encourage this threshold mechanism to explore up to the zero point, and that's a safe point, but we're not going to necessarily reach all the way to the zero point.
Rather, we're going to use other parameters to further define alpha and this specifically is as follows.
So there will be a beta parameter to control the deviation from the optimal threshold and this can be based on can be accounting for the over-fitting to the training data let's say, and so this can be just an adjustment factor.
But what's more interesting is this gamma parameter.
Here, and you can see in this formula, gamma is controlling the inference of the number of examples in training that are set.
So you can see in this formula as N which denotes the number of training examples becomes bigger, then it would actually encourage less exploration.
In other words, when these very small it would try to explore more.
And that just means if we have seen few examples we're not sure whether we have exhausted the space of interest.
So we need to explore but as we have seen many examples from the user many that have we feel that we probably don't have to explore more.
So this gives us a beta gamma for exploration, right.
The more examples we have seen the less exploration we need to do.
So the threshold would be closer to the optimal threshold so that's the basic idea of this approach.
This approach actually has been working well in some evaluation studies, particularly effective.
And also can work on arbitrary utility with the appropriate lower bound.
And explicitly addresses the exploration-exploitation tradeoff and it kind of uses the zero utility threshold point as a safeguard for exploration-exploitation tradeoff.
We're not never going to explore further than the zero utility point.
So if you take the analogy of gambling, and you don't want to risk on losing money.
So it's a safe spend, really conservative strategy for exploration.
And the problem is of course, this approach is purely heuristic and the zero utility lower boundary is also often too conservative, and there are, of course, more advance in machine learning approaches that have been proposed for solving this problems and this is their active research area.
So to summarize, there are two strategies for recommended systems or filtering systems, one is content based, which is looking at the item similarity, and the other is collaborative filtering that was looking at the user similarity.
We've covered content-based filtering approach.
In the next lecture, we will talk about the collaborative filtering.
In content-based filtering system, we generally have to solve several problems relative to filtering decision and learning, etc.
And such a system can actually be built based on a search engine system by adding a threshold mechanism and adding adaptive learning algorithm to allow the system to learn from long term feedback from the user.




Session 6.7: Recommender Systems Collaborative Filtering Part 1

This lecture is about collaborative filtering.
In this lecture we're going to continue the discussion of recommended systems.
In particular, we're going to look at the approach of collaborative filtering.
You have seen this slide before when we talked about the two strategies to answer the basic question, will user U like item X?
In the previous lecture, we looked at the item similarity, that's content-based filtering.
In this lecture, we're going to look at the user similarity.
This is a different strategy, called a collaborative filtering.
So first, what is collaborative filtering?
It is to make filtering decisions for individual user based on the judgements of other uses.
And that is to say we will infer individual's interest or preferences from that of other similar users.
So the general idea is the following.
Given a user u, we're going to first find the similar users, U1 through.
And then we're going to predict the use preferences based on the preferences of these similar users, U1 through.
Now, the user similarity here can be judged based their similarity, the preferences on a common set of items.
Now here you can see the exact content of item doesn't really matter.
We're going to look at the only the relation between the users and the items.
So this means this approach is very general.
It can be applied to any items, not just the text of objects.
So this approach would work well under the following assumptions.
First, users with the same interest will have similar preferences.
Second, the users with similar preferences probably share the same interest.
So for example, if the interest of the user is in information retrieval, then we can infer the user probably favor SIGIR papers.
So those who are interested in information retrieval researching, probably all favor SIGIR papers.
That's an assumption that we make.
And if this assumption is true, then it would help collaborative filtering to work well.
We can also assume that if we see people favor See SIGIR papers, then we can infer their interest is probably information retrieval.
So in these simple examples, it seems to make sense, and in many cases such assumption actually does make sense.
So another assumption we have to make is that there are sufficiently large number of user preferences available to us.
So for example, if you see a lot of ratings of users for movies and those indicate their preferences on movies.
And if you have a lot of such data, then cluttered and filtering can be very effective.
If not, there will be a problem, and that's often called a cold start problem.
That means you don't have many preferences available, so the system could not fully take advantage of collaborative filtering yet.
So let's look at the filtering problem in a more formal way.
So this picture shows that we are, in general, considering a lot of users and we're showing m users here, so U1 through.
And we're also considering a number of objects.
Let's say n objects in order to O1 through On.
And then we will assume that the users will be able to judge those objects and the user could for example give ratings to those items.
For example, those items could be movies, could be products and then the users would give ratings 1 through 5 and see.
So what you see here is that we have shown some ratings available for some combinations.
So some users have watched some movies, they have rated those movies, they obviously won't be able to watch all the movies and some users may actually only watch a few movies.
So this is in general a small symmetrics.
So many items and many entries have unknown values.
And what's interesting here is we could potentially infer the value of an element in this matrix based on other values.
And that's after the essential question in collaborative filtering, and that is, we assume there's an unknown function here, f.
That would map a pair of user and object to a rating.
And we have observed the sum values of this function.
And we want to infer the value of this function for other pairs that don't have that as available here.
So this is very similar to other machinery problems where we'd know the values of the function on some training data set.
And we hope to predict the values of this function on some test data so this is a function approximation.
And how can we pick out the function based on the observed ratings.
So this is the setup.
Now there are many approaches to solving this problem.
In fact, this is a very active research area or reason that there are special conferences dedicated to the problem, major conference devoted to the problem.




Session 6.8: Recommender Systems Collaborative Filtering Part 2

And here we're going to talk about basic strategy.
And that would be based on similarity of users and then predicting the rating of and object by an active user using the ratings of similar users to this active user.
This is called a memory based approach because it's a little bit similar to storing all the user information and when we are considering a particular user we going to try to retrieve the rating users or the similar users to this user case.
And then try to use this information about those users to predict the preference of this user.
So here is the general idea and we use some notations here, so x sub i j denotes the rating of object o j by user u i and n sub i is average rating of object by this user.
So this n i is needed because we would like to normalize the ratings of objects by this user.
So how do you do normalization?
Well, we're going to just subtract the average rating from all the ratings.
Now, this is to normalize these ratings so that the ratings from different users would be comparable.
Because some users might be more generous, and they generally give more high ratings but some others might be more critical so their ratings cannot be directly compared with each other or aggregate them together.
So we need to do this normalization.
Another prediction of the rating on the item by another user or active user, u sub a here can be based on the average ratings of similar users.
So the user u sub a is the user that we are interested in recommending items to.
And we now are interested in recommending this o sub j.
So we're interested in knowing how likely this user will like this object.
How do we know that?
Where the idea here is to look at whether similar users to this user have liked this object.
So mathematically this is to say well the predicted the rating of this user on this app object, user a on object o j is basically combination of the normalized ratings of different users, and in fact here, we're taking a sum over all the users.
But not all users contribute equally to the average, and this is conjured by the weights.
So this weight controls the inference of the user on the prediction.
And of course, naturally this weight should be related to the similarity between ua and this particular user, ui.
The more similar they are, then the more contribution user ui can make in predicting the preference of ua.
So, the formula is extremely simple.
You can see, it's a sum of all the possible users.
And inside the sum we have their ratings, well, their normalized ratings as I just explained.
The ratings need to be normalized in order to be comparable with each other.
And then these ratings are weighted by their similarity.
So you can imagine w of a and i is just a similarity of user a and user i.
Now what's k here?
Well k is simply a normalizer.
It's just one over the sum of all the weights, over all the users.
So this means, basically, if you consider the weight here together with k, and we have coefficients of weight that will sum to one for all the users.
And it's just a normalization strategy so that you get this predictor rating in the same range as these ratings that we used to make the prediction.
Right?
So this is basically the main idea of memory-based approaches for collaborative filtering.
Once we make this prediction, we also would like to map back through the rating that the user would actually make, and this is to further add the mean rating or average rating of this user u sub a to the predicted value.
This would recover a meaningful rating for this user.
So if this user is generous, then the average it would be is somewhat high, and when we add that the rating will be adjusted to our relatively high rate.
Now when you recommend an item to a user this actually doesn't really matter, because you are interested in basically the normalized reading, that's more meaningful.
But when they evaluate these rather than filter approaches, they typically assume that actual ratings of the user on these objects to be unknown and then you do the prediction and then you compare the predicted ratings with their actual ratings.
So, you do have access to the actual ratings.
But, then you pretend that you don't know, and then you compare your systems predictions with the actual ratings.
In that case, obviously, the systems prediction would be adjusted to match the actual ratings of the user and this is what's happening here basically.
Okay so this is the memory based approach.
Now, of course, if you look at the formula, if you want to write the program to implement it, you still face the problem of determining what is this w function?
Once you know the w function, then the formula is very easy to implement.
So, indeed, there are many different ways to compute this function or this weight, w, and specific approaches generally differ in how this is computed.
So here are some possibilities and you can imagine there are many other possibilities.
One popular approach is we use the Pearson correlation coefficient.
This would be a sum over commonly rated items.
And the formula is a standard appears in correlation coefficient formula as shown here.
So this basically measures whether the two users tended to all give higher ratings to similar items or lower ratings to similar items.
Another measure is the cosine measure, and this is going to treat the rating vectors as vectors in the vector space.
And then, we're going to measure the angle and compute the cosine of the angle of the two vectors.
And this measure has been using the vector space model for retrieval, as well.
So as you can imagine there are just as many different ways of doing that.
In all these cases, note that the user's similarity is based on their preferences on items and we did not actually use any content information of these items.
It didn't matter these items are, they can be movies, they can be books, they can be products, they can be text documents which has been cabled the content and so this allows such approach to be applied to a wide range of problems.
Now in some newer approaches of course, we would like to use more information about the user.
Clearly, we know more about the user, not just these preferences on these items.
So in the actual filtering system, is in collaborative filtering, we could also combine that with content based filtering.
We could use more context information, and those are all interesting approaches that people are just starting, and there are new approaches proposed.
But, this memory based approach has been shown to work reasonably well, and it's easy to implement in practical applications this could be a starting point to see if the strategy works well for your application.
So, there are some obvious ways to also improve this approach and mainly we would like to improve the user similarity measure.
And there are some practical issues we deal with here as well.
So for example, there will be a lot of missing values.
What do you do with them?
Well, you can set them to default values or the average ratings of the user.
And that would be a simple solution.
But there are advanced approaches that can actually try to predict those missing values, and then use predictive values to improve the similarity.
So in fact that the memory based apology can predict those missing values, right?
So you get you have iterative approach where you first use some preliminary prediction and then you can use the predictive values to further improve the similarity function.
So this is a heuristic way to solve the problem.
And the strategy obviously would affect the performance of claritative filtering just like any other heuristics would improve these similarity functions.
Another idea which is actually very similar to the idea of IDF that we have seen in text search is called a Inverse User Frequency or IUF.
Now here the idea is to look at where the two users share similar ratings.
If the item is a popular item that has been viewed by many people and seen  to people interested in this item may not be so interesting but if it's a rare item, it has not been viewed by many users.
But these two users deal with this item and they give similar ratings.
And, that says more about their similarity.
It's kind of to emphasize more on similarity on items that are not viewed by many users.




Session 6.9: Recommender Systems Collaborative Filtering Part 3

So to summarize our discussion of recommender systems, in some sense, the filtering task for recommender task is easy, and in some other sense, the task is actually difficult.
So it's easy because the user's expectation is low.
In this case the system takes initiative to push information to the user.
The user doesn't really make any effort, so any recommendation is better than nothing.
All right.
So, unless you recommend the noise items or useless documents.
If you can recommend some useful information users generally will appreciate it, so that's, in that sense that's easy.
However, filtering is actually much harder task than retrieval because you have to make a binary decision and you can't afford waiting for a lot of items and then you're going to see whether one item is better than others.
You have to make a decision when you see this item.
Think about news filtering.
As soon as you see the news enough to decide whether the news would be interesting to the user.
If you wait for a few days, well, even if you can make accurate recommendation of the most relevant news, the utility is going to be significantly decreased.
Another reason why it's hard is because of data sparseness if you think of this as a learning problem.
Collaborative filtering, for example, is purely based on learning from the past ratings.
So if you don't have many ratings there's really not that much you can do, right?
And yeah I just mentioned this cold start problem.
This is actually a very serious, serious problem.
But of course there are strategies that have been proposed for the soft problem, and there are different strategies that you can use to alleviate the problem.
You can use, for example, more user information to asses their similarity, instead of using the preferences of these users on these items give me additional information available about the user, etc.
And we also talk about two strategies for filtering task.
One is content-based where we look at items there is collaborative filtering where we look at Use a similarity.
And they obviously can be combined in a practical system.
You can imagine they generally would have to be combined.
So that would give us a hybrid strategy for filtering.
And we also could recall that we talked about push versus pull as two strategies for getting access to the text data.
And recommender system easy to help users in the push mode, and search engines are serving users in the pull mode.
Obviously the two should be combined, and they can be combined.
The two have a system that can support user with multiple mode information access.
So in the future we could anticipate such a system to be more useful the user.
And either, this is an active research area so there are a lot of new algorithms being proposed all the time.
In particular those new algorithms tend to use a lot of context information.
Now the context here could be the context of the user and could also be the context of the user.
Items.
The items are not the isolated.
They're connected in many ways.
The users might form social network as well, so there's a rich context there that we can leverage in order to really solve the problem well and then that's active research area where also machine learning algorithms have been applied.
Here are some additional readings in the handbook called Recommender Systems and has a collection of a lot of good articles that can give you an overview of a number of specific approaches through recommender systems.




Session 6.10: Summary For Exam 1

This lecture is a summary of this course.
This map shows the major topics we have covered in this course.
And here are some key high-level take-away messages.
First, we talked about natural language content analysis.
Here the main take-away messages is natural language processing is a foundation for text retrieval, but currently the NLP isn't robust enough so the battle of wars is generally the main method used in modern search engines.
And it's often sufficient before most of the search tasks, but obviously for more complex search tasks then we need a deeper natural language processing techniques.
We then talked about the high level strategies for text access and we talked about push versus pull.
In pull we talked about querying versus browsing.
Now in general in future search engines, we should integrate all these techniques to provide a math involved information access.
And now we'll talk about a number of issues related to search engines.
We talked about the search problem.
And we framed that as a ranking problem.
And we talked about a number of retrieval methods.
We start with the overview of vector space model and the probabilistic model and then we talked about the vector space model in depth.
We also later talked about the language modeling approach, and that's probabilistic model.
And here, many take-away message is that the modeling retrieval function tend to look similar, and they generally use various heuristics.
Most important ones are TF-IDF weighting, document length normalization.
And the TF is often transformed through a sub media transformation function.
And then we talked about how to implement a retrieval system, and here, the main techniques that we talked about, how to construct an inverted index so that we can prepare the system to answer a query quickly.
And we talked about how to do a faster search by using the inverted index.
And we then talked about how to evaluate the text retrieval system, mainly introduced to the Cranfield Evaluation Methodology.
This was a very important evaluation methodology that can be applied to many tasks.
We talked about the major evaluation measures.
So, the most important measures for a search engine are MAP, mean average precision, and nDCG Summarize the discount or accumulative gain and also precision and recall are the two basic measures.
And we then talked about feedback techniques.
And we talked about the Rocchio in the Vector Space Model and the mixture model and the language modeling approach.
Feedback is a very important technique especially considering the opportunity of learning from a lot of pixels on the Web.
We then talked about Web search.
And here we talked about how to use parallel in that scene to solve the scalability issue in that scene we're going to use the net reduce.
Then we talked about how to use linking permission model app to improve search.
We talked about page rank and hits as the major hours is to analyzing links on the Web.
We then talked about learning through rank.
This is the use of machine learning to combine multiple features for improvement scoring.
Not only that the effectiveness can be improved in using this approach, but we can also improve the robustness of the.
The ranking function so that it's not easy to expand a search engine.
It just some features to promote the page.
And finally we talked about the future of Web search.
About the some major reactions that we might to see in the future in improving the count of regeneration of such engines.
And then finally we talked about the recommended systems and, these are systems to increment the push mode.
And we'll talk about the two approaches, one is content-based, one is collaborative filtering and they can be combined together.
Now, an obvious missing piece in this picture is the user, so user interface is also an important component in any search engine.
Even though the current search interface is relatively simple they actually have done a lot of studies of user interfaces where we do visualization for example.
And this is the topic to that, you can learn more by reading this book.
It's an excellent book about all kinds of studies of search using the face.
If you want to know more about the topics that we talked about, you can also read some additional readings that are listed here.
In this short course we only manage to cover some basic topics in text retrievals and search engines.
And these resources provide additional information about more advanced topics and they give a more thorough treatment of some of the topics that we talked about.
And a main source is the Synthesis Digital Library that you can see a lot of short to textbook or textbooks, or long tutorials.
They tend to provide a lot of information to explain a topic.
And there a lot of series that are related to this cause.
One is information concepts, retrieval, and services.
One is human langauge technology.
And yet another is artificial intelligence and machine learning.
There are also some major journals and conferences listed here that tend to have a lot of research papers we need to and topic of this course.
And finally, for more information about resources Including readings, tool kits, etc you can check out his URL.
So, if you have not taken the text mining course in this data mining specialization series then naturally the next step is to take that course.
As this picture shows, to mine big text data, we generally need two kinds of techniques.
One is text retrieval, which is covered in this course.
And these techniques will help us convert raw big text data into small relevant text data, which are actually needed in the specific application.
Now human plays important role in mining any text data because text data is written for humans to consume.
So involving humans in the process of data mining is very important and in this course we have covered the various strategies to help users get access to the most relevant data.
These techniques are always so essential in any text mining system to help provide prominence and to help users interpret the inner patterns that the user will define through text data mining.
So, in general, the user would have to go back to the original data to better understand the patterns.
So the text mining cause, or rather, text mining and analytics course will be dealing with what to do once the user has a following information.
So this is a second step in this picture where we would convert the text data into actionable knowledge.
And this has to do with helping users to further digest the found information or to find the patterns and to reveal knowledge.
In text and such knowledge can then be used in application systems to help decision making or to help a user finish a task.
So, if you have not taken that course, the natural step and that natural next step would be to take that course.
Thank you for taking this course.
I hope you had fun and found this course to be useful to you.
And I look forward to interacting with you at a future opportunity.




