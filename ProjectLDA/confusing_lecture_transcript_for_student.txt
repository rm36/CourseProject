And we see the results here show that there is a common  theme that's corresponding to Cluster 1 here in this column.  And there is a common theme indicting that United Nations is involved in both Wars.  It's a common topic covered in both sets of articles.  And that's indicated by the high probability words shown here, united and  nations. 
Now, we don't have time to introduce this model in detail,  but there are references here that you can look into to know more detail.  Here I just want to explain the high level ideas in more detail.  Particularly I want to explain the generation process.  Of text data that has context associated in such a model. 
So because we can infer the reviewers weights on different dimensions,  we can allow a user to actually say what do you care about.  So for example, I have a query here that shows 90% of the weight  should be on value and 10% on others.  So that just means I don't care about other aspect.  I just care about getting a cheaper hotel.  My emphasis is on the value dimension.  Now what we can do with such query is we can use reviewers that we  believe have a similar preference to recommend a hotels for you.  How can we know that?  Well, we can infer the weights of those reviewers on different aspects.  We can find the reviewers whose weights are more precise,  of course inferred rates are similar to yours.  And then use those reviewers to recommend hotels for you and  this is what we call personalized or rather query specific recommendations.  Now the non-personalized recommendations now shown on the top,  and you can see the top results generally have much higher price, than the lower  group and that's because when the reviewer's cared more about the value as  dictated by this query they tended to really favor low price hotels.  So this is yet another application of this technique. 
And we also need to consider the order of those categories.  And we'll talk about ordinal regression for some of these problem.  We have also assume that the generating models are powerful for  mining latent user preferences.  This in particular in the generative model for mining latent regression.  And we embed some interesting preference information and  send the weights of words in the model as a result we can learn most  useful information when fitting the model to the data.  Now most approaches have been proposed and evaluated.  For product reviews, and that was because in such a context, the opinion holder and  the opinion target are clear.  And they are easy to analyze.  And there, of course, also have a lot of practical applications.  But opinion mining from news and social media is also important, but that's  more difficult than analyzing review data, mainly because the opinion holders and  opinion targets are all interested.  So that calls for  natural management processing techniques to uncover them accurately. 
So now you can see the average prices of hotels favored by top ten reviewers  are indeed much cheaper than those that are favored by the bottom ten.  And this provides some indirect way of validating the inferred weights.  It just means the weights are not random.  They are actually meaningful here.  In comparison, the average price in these three cities,  you can actually see the top ten tend to have below average in price,  whereas the bottom half, where they care a lot about other things like a service or  room condition tend to have hotels that have higher prices than average.  So with these results we can build a lot of interesting applications.  For example, a direct application would be to generate the rated aspect, the summary,  and because of the decomposition we have now generated the summaries for  each aspect.  The positive sentences the negative sentences about each aspect.  It's more informative than original review that just has an overall rating and  review text.  Here are some other results  about the aspects that's covered from reviews with no ratings.  These are mp3 reviews,  and these results show that the model can discover some interesting aspects.  Commented on low overall ratings versus those higher overall per ratings.  And they care more about the different aspects. 